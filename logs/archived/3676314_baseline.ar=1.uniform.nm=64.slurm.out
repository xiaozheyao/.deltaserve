/mnt/hwfile/huqinghao/miniconda3/bin/python
2024-05-17 00:51:49,081 - INFO - Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-05-17 00:51:49,081 - INFO - Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-05-17 00:51:49,081 - INFO - NumExpr defaulting to 8 threads.
2024-05-17 00:51:49,974	INFO scripts.py:1182 -- Did not find any active Ray processes.
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc3cabb8070>: Failed to establish a new connection: [Errno 111] Connection refused'))

==========
== CUDA ==
==========

CUDA Version 12.1.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

*************************
** DEPRECATION NOTICE! **
*************************
THIS IMAGE IS DEPRECATED and is scheduled for DELETION.
    https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/support-policy.md

Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc3cabb9450>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc3cabb9d20>: Failed to establish a new connection: [Errno 111] Connection refused'))
INFO 05-17 00:52:11 api_server.py:191] vLLM API server version 0.3.3
INFO 05-17 00:52:11 api_server.py:192] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=[], delta_modules=[], swap_modules=[SwapModule(name='/vllm/.idea/full_models/Llama-2-13b-hf', local_path='/vllm/.idea/full_models/Llama-2-13b-hf'), SwapModule(name='delta-1', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-1'), SwapModule(name='delta-2', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-2'), SwapModule(name='delta-3', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-3'), SwapModule(name='delta-4', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-4'), SwapModule(name='delta-5', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-5'), SwapModule(name='delta-6', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-6'), SwapModule(name='delta-7', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-7'), SwapModule(name='delta-8', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-8'), SwapModule(name='delta-9', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-9'), SwapModule(name='delta-10', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-10'), SwapModule(name='delta-11', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-11'), SwapModule(name='delta-12', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-12'), SwapModule(name='delta-13', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-13'), SwapModule(name='delta-14', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-14'), SwapModule(name='delta-15', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-15'), SwapModule(name='delta-16', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-16'), SwapModule(name='delta-17', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-17'), SwapModule(name='delta-18', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-18'), SwapModule(name='delta-19', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-19'), SwapModule(name='delta-20', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-20'), SwapModule(name='delta-21', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-21'), SwapModule(name='delta-22', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-22'), SwapModule(name='delta-23', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-23'), SwapModule(name='delta-24', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-24'), SwapModule(name='delta-25', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-25'), SwapModule(name='delta-26', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-26'), SwapModule(name='delta-27', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-27'), SwapModule(name='delta-28', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-28'), SwapModule(name='delta-29', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-29'), SwapModule(name='delta-30', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-30'), SwapModule(name='delta-31', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-31'), SwapModule(name='delta-32', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-32'), SwapModule(name='delta-33', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-33'), SwapModule(name='delta-34', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-34'), SwapModule(name='delta-35', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-35'), SwapModule(name='delta-36', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-36'), SwapModule(name='delta-37', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-37'), SwapModule(name='delta-38', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-38'), SwapModule(name='delta-39', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-39'), SwapModule(name='delta-40', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-40'), SwapModule(name='delta-41', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-41'), SwapModule(name='delta-42', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-42'), SwapModule(name='delta-43', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-43'), SwapModule(name='delta-44', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-44'), SwapModule(name='delta-45', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-45'), SwapModule(name='delta-46', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-46'), SwapModule(name='delta-47', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-47'), SwapModule(name='delta-48', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-48'), SwapModule(name='delta-49', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-49'), SwapModule(name='delta-50', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-50'), SwapModule(name='delta-51', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-51'), SwapModule(name='delta-52', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-52'), SwapModule(name='delta-53', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-53'), SwapModule(name='delta-54', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-54'), SwapModule(name='delta-55', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-55'), SwapModule(name='delta-56', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-56'), SwapModule(name='delta-57', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-57'), SwapModule(name='delta-58', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-58'), SwapModule(name='delta-59', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-59'), SwapModule(name='delta-60', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-60'), SwapModule(name='delta-61', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-61'), SwapModule(name='delta-62', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-62'), SwapModule(name='delta-63', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-63'), SwapModule(name='delta-64', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-64')], chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/vllm/.idea/full_models/Llama-2-13b-hf', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, seed=0, swap_space=4, gpu_memory_utilization=0.85, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=True, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=64, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=32, enable_delta=False, max_deltas=0, max_cpu_deltas=32, max_delta_bitwidth=4, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_prefetch=False, scheduler_policy='fcfs', max_swap_slots=8, max_cpu_models=18, enable_swap=True, engine_use_ray=False, disable_log_requests=True, max_log_len=None)
2024-05-17 00:52:14,767	INFO worker.py:1749 -- Started a local Ray instance.
INFO 05-17 00:52:17 llm_engine.py:90] Initializing an LLM engine (v0.3.3) with config: model='/vllm/.idea/full_models/Llama-2-13b-hf', tokenizer='/vllm/.idea/full_models/Llama-2-13b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, lora_config=None, delta_config=None, swap_config=SwapConfig(max_packed_model=8, max_cpu_model=18), max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, seed=0)
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc3cabb9cf0>: Failed to establish a new connection: [Errno 111] Connection refused'))
INFO 05-17 00:52:28 selector.py:16] Using FlashAttention backend.
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc3cabb9420>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc3cabb81f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 00:52:31 selector.py:16] Using FlashAttention backend.
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 00:52:31 selector.py:16] Using FlashAttention backend.
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 00:52:31 selector.py:16] Using FlashAttention backend.
INFO 05-17 00:52:44 model_runner.py:161] Loading model weights took 6.1114 GB
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc3cabba5f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc3cabbaec0>: Failed to establish a new connection: [Errno 111] Connection refused'))
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 00:52:46 model_runner.py:161] Loading model weights took 6.1114 GB
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 00:52:45 model_runner.py:161] Loading model weights took 6.1114 GB
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 00:52:46 model_runner.py:161] Loading model weights took 6.1114 GB
INFO 05-17 00:53:01 ray_gpu_executor.py:275] # GPU blocks: 3146, # CPU blocks: 1310
INFO 05-17 00:53:04 block_manager.py:264] disable automatic prefix caching
WARNING 05-17 00:53:04 serving_chat.py:373] No chat template provided. Chat API will not work.
INFO:     Started server process [101850]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     ::1:53512 - "GET /sysinfo HTTP/1.1" 200 OK
Translating from base-model to /vllm/.idea/full_models/Llama-2-13b-hf
Warming up starts
{'id': 5, 'prompt': 'USER: Count from 1 to 10 with step = 3\nASSISTANT:', 'timestamp': 0, 'model': 'delta-31', 'min_tokens': 67, 'max_tokens': 67}
Prefetching is disabled
INFO 05-17 00:53:12 models.py:164] Disk -> CPU: Loaded in 1.712 seconds
INFO 05-17 00:53:13 worker_manager.py:250] [1715878393.6311119] CPU -> GPU time: 1.3326
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 00:53:16 models.py:164] Disk -> CPU: Loaded in 6.312 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 00:53:16 models.py:164] Disk -> CPU: Loaded in 6.307 seconds
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 00:53:16 models.py:164] Disk -> CPU: Loaded in 6.293 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 00:53:17 worker_manager.py:250] [1715878397.610397] CPU -> GPU time: 0.7151
INFO 05-17 00:53:18 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 00:53:18 worker_manager.py:250] [1715878398.3051767] CPU -> GPU time: 1.4223
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 00:53:18 worker_manager.py:250] [1715878398.4016507] CPU -> GPU time: 1.5026
INFO 05-17 00:53:23 metrics.py:276] Avg prompt throughput: 4.3 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%
INFO 05-17 00:53:28 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%
INFO:     ::1:53514 - "POST /v1/completions HTTP/1.1" 200 OK
Warming up ends
Issuing queries
sending 1 queries at 0.8
sending 1 queries at 2.1
sending 1 queries at 3.0
sending 1 queries at 3.8000000000000003
sending 1 queries at 4.4
sending 1 queries at 5.4
sending 1 queries at 6.0
sending 1 queries at 8.200000000000001
sending 1 queries at 11.5
sending 1 queries at 12.0
sending 1 queries at 13.600000000000001
sending 1 queries at 14.3
sending 1 queries at 15.200000000000001
sending 2 queries at 17.8
sending 2 queries at 17.900000000000002
sending 1 queries at 19.700000000000003
sending 1 queries at 21.200000000000003
sending 1 queries at 23.3
sending 1 queries at 27.1
sending 1 queries at 28.700000000000003
sending 1 queries at 29.3
Prefetching is disabled
Prefetching is disabled
INFO 05-17 00:53:32 models.py:164] Disk -> CPU: Loaded in 1.653 seconds
Prefetching is disabled
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 00:53:37 models.py:164] Disk -> CPU: Loaded in 6.003 seconds
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 00:53:37 models.py:164] Disk -> CPU: Loaded in 5.970 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 00:53:37 models.py:164] Disk -> CPU: Loaded in 6.074 seconds
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
INFO 05-17 00:53:38 metrics.py:276] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 1.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
INFO 05-17 00:53:48 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
INFO 05-17 00:53:58 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%
total threads: 23
Prefetching is disabled
Prefetching is disabled
INFO 05-17 00:54:05 models.py:164] Disk -> CPU: Loaded in 27.502 seconds
INFO 05-17 00:54:08 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 00:54:08 models.py:164] Disk -> CPU: Loaded in 30.026 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 00:54:08 models.py:164] Disk -> CPU: Loaded in 30.637 seconds
INFO 05-17 00:54:09 models.py:164] Disk -> CPU: Loaded in 1.189 seconds
INFO 05-17 00:54:11 models.py:164] Disk -> CPU: Loaded in 1.481 seconds
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 00:54:11 models.py:164] Disk -> CPU: Loaded in 33.565 seconds
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 00:54:14 models.py:164] Disk -> CPU: Loaded in 5.642 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 00:54:16 models.py:164] Disk -> CPU: Loaded in 5.778 seconds
INFO 05-17 00:54:18 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 00:54:19 models.py:164] Disk -> CPU: Loaded in 5.666 seconds
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 00:54:22 models.py:164] Disk -> CPU: Loaded in 6.278 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 00:54:23 models.py:164] Disk -> CPU: Loaded in 6.237 seconds
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 00:54:26 models.py:164] Disk -> CPU: Loaded in 6.050 seconds
INFO 05-17 00:54:28 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%
INFO 05-17 00:54:38 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%
INFO 05-17 00:54:48 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%
INFO 05-17 00:54:48 models.py:164] Disk -> CPU: Loaded in 36.310 seconds
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 00:54:50 models.py:164] Disk -> CPU: Loaded in 27.025 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 00:54:57 models.py:164] Disk -> CPU: Loaded in 32.691 seconds
INFO 05-17 00:54:58 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 00:55:00 models.py:164] Disk -> CPU: Loaded in 32.412 seconds
INFO 05-17 00:55:08 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%
INFO 05-17 00:55:15 models.py:164] Disk -> CPU: Loaded in 26.155 seconds
INFO 05-17 00:55:42 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 00:55:39 models.py:164] Disk -> CPU: Loaded in 48.338 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 00:55:40 models.py:164] Disk -> CPU: Loaded in 42.627 seconds
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 00:55:41 models.py:164] Disk -> CPU: Loaded in 39.917 seconds
INFO 05-17 00:55:44 models.py:164] Disk -> CPU: Loaded in 1.140 seconds
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 00:55:48 models.py:164] Disk -> CPU: Loaded in 5.265 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 00:55:48 models.py:164] Disk -> CPU: Loaded in 5.221 seconds
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 00:55:48 models.py:164] Disk -> CPU: Loaded in 5.265 seconds
INFO 05-17 00:55:50 metrics.py:276] Avg prompt throughput: 33.9 tokens/s, Avg generation throughput: 1.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO 05-17 00:55:55 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO 05-17 00:56:01 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO 05-17 00:56:06 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%
INFO 05-17 00:56:12 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%
INFO 05-17 00:56:17 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO 05-17 00:56:22 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO 05-17 00:56:28 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO 05-17 00:56:33 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO 05-17 00:56:39 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO 05-17 00:56:44 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO 05-17 00:56:50 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO 05-17 00:56:55 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:53741 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 00:57:01 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 13 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:53756 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 00:57:12 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO 05-17 00:57:22 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO 05-17 00:57:29 models.py:164] Disk -> CPU: Loaded in 27.956 seconds
INFO 05-17 00:57:39 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 00:57:33 models.py:164] Disk -> CPU: Loaded in 31.717 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 00:57:36 models.py:164] Disk -> CPU: Loaded in 34.677 seconds
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 00:57:39 models.py:164] Disk -> CPU: Loaded in 37.468 seconds
INFO 05-17 00:57:44 metrics.py:276] Avg prompt throughput: 4.7 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO 05-17 00:57:50 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO 05-17 00:57:56 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 12 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:53932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 00:58:09 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO 05-17 00:58:19 models.py:164] Disk -> CPU: Loaded in 22.616 seconds
INFO 05-17 00:58:19 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO 05-17 00:58:51 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 00:58:44 models.py:164] Disk -> CPU: Loaded in 47.763 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 00:58:47 models.py:164] Disk -> CPU: Loaded in 51.022 seconds
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 00:58:50 models.py:164] Disk -> CPU: Loaded in 53.535 seconds
INFO 05-17 00:58:56 metrics.py:276] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 9.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 11 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:53906 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 00:59:11 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO 05-17 00:59:18 models.py:164] Disk -> CPU: Loaded in 16.158 seconds
INFO 05-17 01:00:01 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 00:59:57 models.py:164] Disk -> CPU: Loaded in 55.268 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 01:00:00 models.py:164] Disk -> CPU: Loaded in 58.409 seconds
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 01:00:00 models.py:164] Disk -> CPU: Loaded in 58.586 seconds
INFO 05-17 01:00:07 metrics.py:276] Avg prompt throughput: 3.1 tokens/s, Avg generation throughput: 3.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO 05-17 01:00:13 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 10 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:53768 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 01:00:21 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO 05-17 01:00:31 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO 05-17 01:00:36 models.py:164] Disk -> CPU: Loaded in 23.340 seconds
INFO 05-17 01:01:11 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 9 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 01:01:05 models.py:164] Disk -> CPU: Loaded in 51.399 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 01:01:08 models.py:164] Disk -> CPU: Loaded in 54.454 seconds
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 01:01:09 models.py:164] Disk -> CPU: Loaded in 56.158 seconds
INFO:     ::1:53926 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 01:01:21 metrics.py:276] Avg prompt throughput: 2.1 tokens/s, Avg generation throughput: 3.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO 05-17 01:01:31 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO 05-17 01:01:41 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO 05-17 01:01:48 models.py:164] Disk -> CPU: Loaded in 32.254 seconds
INFO 05-17 01:02:04 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 01:01:58 models.py:164] Disk -> CPU: Loaded in 42.832 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 01:02:01 models.py:164] Disk -> CPU: Loaded in 45.847 seconds
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 01:02:03 models.py:164] Disk -> CPU: Loaded in 48.220 seconds
INFO 05-17 01:02:14 metrics.py:276] Avg prompt throughput: 1.2 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO:     ::1:53894 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 01:02:20 models.py:164] Disk -> CPU: Loaded in 1.266 seconds
INFO 05-17 01:02:26 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 01:02:24 models.py:164] Disk -> CPU: Loaded in 5.511 seconds
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 01:02:25 models.py:164] Disk -> CPU: Loaded in 5.958 seconds
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 01:02:25 models.py:164] Disk -> CPU: Loaded in 5.979 seconds
INFO 05-17 01:02:36 metrics.py:276] Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO 05-17 01:02:42 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.5 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO 05-17 01:02:48 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO 05-17 01:02:53 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO 05-17 01:02:59 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO 05-17 01:03:05 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 7 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:53684 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 01:03:16 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.4 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO 05-17 01:03:26 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO 05-17 01:03:36 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO 05-17 01:03:40 models.py:164] Disk -> CPU: Loaded in 31.116 seconds
INFO 05-17 01:04:01 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 6 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 01:03:46 models.py:164] Disk -> CPU: Loaded in 36.691 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 01:03:50 models.py:164] Disk -> CPU: Loaded in 41.165 seconds
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 01:03:59 models.py:164] Disk -> CPU: Loaded in 49.940 seconds
INFO:     ::1:53938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 01:04:03 models.py:164] Disk -> CPU: Loaded in 1.622 seconds
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 01:04:07 models.py:164] Disk -> CPU: Loaded in 6.199 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 01:04:07 models.py:164] Disk -> CPU: Loaded in 6.155 seconds
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 01:04:07 models.py:164] Disk -> CPU: Loaded in 6.248 seconds
INFO 05-17 01:04:09 metrics.py:276] Avg prompt throughput: 5.1 tokens/s, Avg generation throughput: 0.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO 05-17 01:04:21 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO 05-17 01:04:31 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 5 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:53934 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 01:04:41 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.8 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO 05-17 01:04:51 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO 05-17 01:05:01 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO 05-17 01:05:06 models.py:164] Disk -> CPU: Loaded in 33.230 seconds
INFO 05-17 01:05:18 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 01:05:08 models.py:164] Disk -> CPU: Loaded in 35.182 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 01:05:13 models.py:164] Disk -> CPU: Loaded in 40.736 seconds
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 01:05:17 models.py:164] Disk -> CPU: Loaded in 44.498 seconds
INFO 05-17 01:05:28 metrics.py:276] Avg prompt throughput: 5.3 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO 05-17 01:05:35 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO 05-17 01:05:40 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO 05-17 01:05:46 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO 05-17 01:05:52 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO 05-17 01:05:58 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%
INFO 05-17 01:06:03 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:53660 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 01:06:04 models.py:164] Disk -> CPU: Loaded in 1.428 seconds
INFO 05-17 01:06:10 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 01:06:09 models.py:164] Disk -> CPU: Loaded in 5.975 seconds
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 01:06:09 models.py:164] Disk -> CPU: Loaded in 6.086 seconds
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 01:06:09 models.py:164] Disk -> CPU: Loaded in 6.205 seconds
INFO 05-17 01:06:15 metrics.py:276] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 1.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO 05-17 01:06:21 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO 05-17 01:06:27 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%
INFO 05-17 01:06:33 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO 05-17 01:06:38 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO 05-17 01:06:44 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%
INFO 05-17 01:06:50 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 3 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%
INFO:     ::1:53916 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 01:06:53 models.py:164] Disk -> CPU: Loaded in 1.521 seconds
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 01:06:57 models.py:164] Disk -> CPU: Loaded in 5.484 seconds
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 01:06:57 models.py:164] Disk -> CPU: Loaded in 6.206 seconds
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 01:06:58 models.py:164] Disk -> CPU: Loaded in 6.455 seconds
INFO 05-17 01:06:59 metrics.py:276] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 2.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%
INFO 05-17 01:07:10 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 2 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%
INFO:     ::1:53710 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 01:07:20 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO 05-17 01:07:30 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO 05-17 01:07:40 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO 05-17 01:07:45 models.py:164] Disk -> CPU: Loaded in 32.056 seconds
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 01:07:48 models.py:164] Disk -> CPU: Loaded in 34.480 seconds
INFO 05-17 01:07:55 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 01:07:52 models.py:164] Disk -> CPU: Loaded in 38.553 seconds
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 01:07:54 models.py:164] Disk -> CPU: Loaded in 41.346 seconds
INFO:     ::1:53936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 01:08:01 models.py:164] Disk -> CPU: Loaded in 1.132 seconds
INFO 05-17 01:08:07 metrics.py:276] Avg prompt throughput: 1.7 tokens/s, Avg generation throughput: 1.6 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=110063)[0m INFO 05-17 01:08:05 models.py:164] Disk -> CPU: Loaded in 5.341 seconds
[36m(RayWorkerVllm pid=109893)[0m INFO 05-17 01:08:05 models.py:164] Disk -> CPU: Loaded in 6.037 seconds
[36m(RayWorkerVllm pid=110214)[0m INFO 05-17 01:08:06 models.py:164] Disk -> CPU: Loaded in 6.050 seconds
INFO 05-17 01:08:16 metrics.py:276] Avg prompt throughput: 5.3 tokens/s, Avg generation throughput: 1.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO 05-17 01:08:22 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.2 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO 05-17 01:08:28 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%
INFO 05-17 01:08:34 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO 05-17 01:08:39 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:53946 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 01:08:44 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO:     ::1:53956 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 01:08:50 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.2 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO:     ::1:53952 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:53830 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 01:08:55 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.2 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO:     ::1:53948 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 01:09:01 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.5 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO 05-17 01:09:06 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.6 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO 05-17 01:09:11 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.6 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO 05-17 01:09:16 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO 05-17 01:09:21 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.4 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:53724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 01:09:26 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO 05-17 01:09:31 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%
INFO 05-17 01:09:36 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO 05-17 01:09:41 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO 05-17 01:09:46 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO 05-17 01:09:51 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO:     ::1:53954 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 01:09:57 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.8 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO 05-17 01:10:02 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%
INFO:     ::1:53942 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 01:10:07 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%
INFO 05-17 01:10:12 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%
INFO 05-17 01:10:17 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%
INFO:     ::1:53940 - "POST /v1/completions HTTP/1.1" 200 OK
Results written to .artifact/benchmarks/results/fed811b7-f4c3-4d3c-9fc6-5945380c7348.jsonl
Killing process 101850...
/var/spool/slurmd/job3676314/slurm_script: line 42: 101816 Killed                  apptainer run --nv --home /mnt/petrelfs/huqinghao/xzyao/:/home/xiayao --bind /mnt/petrelfs/huqinghao/xzyao/code/vllm:/vllm --bind /mnt/petrelfs/huqinghao/xzyao/code/triteia:/triteia --env PYTHONPATH=/vllm:/triteia --env CUDA_LAUNCH_BLOCKING=1 --env TVM_TARGET=nvidia/nvidia-a100 --env BITBLAS_TARGET=nvidia/nvidia-a100 --env RAY_DEDUP_LOGS=0 --workdir /vllm /mnt/petrelfs/huqinghao/xzyao/images/deltaserve.sif python3 /vllm/vllm/entrypoints/openai/api_server.py --model /vllm/.idea/full_models/Llama-2-13b-hf --disable-log-requests --gpu-memory-utilization 0.85 --swap-modules /vllm/.idea/full_models/Llama-2-13b-hf=/vllm/.idea/full_models/Llama-2-13b-hf delta-1=/vllm/.idea/full_models/Llama-2-13b-hf-1 delta-2=/vllm/.idea/full_models/Llama-2-13b-hf-2 delta-3=/vllm/.idea/full_models/Llama-2-13b-hf-3 delta-4=/vllm/.idea/full_models/Llama-2-13b-hf-4 delta-5=/vllm/.idea/full_models/Llama-2-13b-hf-5 delta-6=/vllm/.idea/full_models/Llama-2-13b-hf-6 delta-7=/vllm/.idea/full_models/Llama-2-13b-hf-7 delta-8=/vllm/.idea/full_models/Llama-2-13b-hf-8 delta-9=/vllm/.idea/full_models/Llama-2-13b-hf-9 delta-10=/vllm/.idea/full_models/Llama-2-13b-hf-10 delta-11=/vllm/.idea/full_models/Llama-2-13b-hf-11 delta-12=/vllm/.idea/full_models/Llama-2-13b-hf-12 delta-13=/vllm/.idea/full_models/Llama-2-13b-hf-13 delta-14=/vllm/.idea/full_models/Llama-2-13b-hf-14 delta-15=/vllm/.idea/full_models/Llama-2-13b-hf-15 delta-16=/vllm/.idea/full_models/Llama-2-13b-hf-16 delta-17=/vllm/.idea/full_models/Llama-2-13b-hf-17 delta-18=/vllm/.idea/full_models/Llama-2-13b-hf-18 delta-19=/vllm/.idea/full_models/Llama-2-13b-hf-19 delta-20=/vllm/.idea/full_models/Llama-2-13b-hf-20 delta-21=/vllm/.idea/full_models/Llama-2-13b-hf-21 delta-22=/vllm/.idea/full_models/Llama-2-13b-hf-22 delta-23=/vllm/.idea/full_models/Llama-2-13b-hf-23 delta-24=/vllm/.idea/full_models/Llama-2-13b-hf-24 delta-25=/vllm/.idea/full_models/Llama-2-13b-hf-25 delta-26=/vllm/.idea/full_models/Llama-2-13b-hf-26 delta-27=/vllm/.idea/full_models/Llama-2-13b-hf-27 delta-28=/vllm/.idea/full_models/Llama-2-13b-hf-28 delta-29=/vllm/.idea/full_models/Llama-2-13b-hf-29 delta-30=/vllm/.idea/full_models/Llama-2-13b-hf-30 delta-31=/vllm/.idea/full_models/Llama-2-13b-hf-31 delta-32=/vllm/.idea/full_models/Llama-2-13b-hf-32 delta-33=/vllm/.idea/full_models/Llama-2-13b-hf-33 delta-34=/vllm/.idea/full_models/Llama-2-13b-hf-34 delta-35=/vllm/.idea/full_models/Llama-2-13b-hf-35 delta-36=/vllm/.idea/full_models/Llama-2-13b-hf-36 delta-37=/vllm/.idea/full_models/Llama-2-13b-hf-37 delta-38=/vllm/.idea/full_models/Llama-2-13b-hf-38 delta-39=/vllm/.idea/full_models/Llama-2-13b-hf-39 delta-40=/vllm/.idea/full_models/Llama-2-13b-hf-40 delta-41=/vllm/.idea/full_models/Llama-2-13b-hf-41 delta-42=/vllm/.idea/full_models/Llama-2-13b-hf-42 delta-43=/vllm/.idea/full_models/Llama-2-13b-hf-43 delta-44=/vllm/.idea/full_models/Llama-2-13b-hf-44 delta-45=/vllm/.idea/full_models/Llama-2-13b-hf-45 delta-46=/vllm/.idea/full_models/Llama-2-13b-hf-46 delta-47=/vllm/.idea/full_models/Llama-2-13b-hf-47 delta-48=/vllm/.idea/full_models/Llama-2-13b-hf-48 delta-49=/vllm/.idea/full_models/Llama-2-13b-hf-49 delta-50=/vllm/.idea/full_models/Llama-2-13b-hf-50 delta-51=/vllm/.idea/full_models/Llama-2-13b-hf-51 delta-52=/vllm/.idea/full_models/Llama-2-13b-hf-52 delta-53=/vllm/.idea/full_models/Llama-2-13b-hf-53 delta-54=/vllm/.idea/full_models/Llama-2-13b-hf-54 delta-55=/vllm/.idea/full_models/Llama-2-13b-hf-55 delta-56=/vllm/.idea/full_models/Llama-2-13b-hf-56 delta-57=/vllm/.idea/full_models/Llama-2-13b-hf-57 delta-58=/vllm/.idea/full_models/Llama-2-13b-hf-58 delta-59=/vllm/.idea/full_models/Llama-2-13b-hf-59 delta-60=/vllm/.idea/full_models/Llama-2-13b-hf-60 delta-61=/vllm/.idea/full_models/Llama-2-13b-hf-61 delta-62=/vllm/.idea/full_models/Llama-2-13b-hf-62 delta-63=/vllm/.idea/full_models/Llama-2-13b-hf-63 delta-64=/vllm/.idea/full_models/Llama-2-13b-hf-64 --tensor-parallel-size 4 --enforce-eager --max-deltas 0 --enable-swap --max-swap-slots 8 --max-cpu-models 18
