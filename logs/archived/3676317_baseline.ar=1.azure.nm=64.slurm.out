/mnt/hwfile/huqinghao/miniconda3/bin/python
2024-05-17 03:01:01,738 - INFO - Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-05-17 03:01:01,739 - INFO - Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-05-17 03:01:01,739 - INFO - NumExpr defaulting to 8 threads.
2024-05-17 03:01:02,605	INFO scripts.py:1182 -- Did not find any active Ray processes.
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f963db84070>: Failed to establish a new connection: [Errno 111] Connection refused'))

==========
== CUDA ==
==========

CUDA Version 12.1.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

*************************
** DEPRECATION NOTICE! **
*************************
THIS IMAGE IS DEPRECATED and is scheduled for DELETION.
    https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/support-policy.md

Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f963db85450>: Failed to establish a new connection: [Errno 111] Connection refused'))
INFO 05-17 03:01:21 api_server.py:191] vLLM API server version 0.3.3
INFO 05-17 03:01:21 api_server.py:192] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=[], delta_modules=[], swap_modules=[SwapModule(name='/vllm/.idea/full_models/Llama-2-13b-hf', local_path='/vllm/.idea/full_models/Llama-2-13b-hf'), SwapModule(name='delta-1', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-1'), SwapModule(name='delta-2', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-2'), SwapModule(name='delta-3', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-3'), SwapModule(name='delta-4', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-4'), SwapModule(name='delta-5', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-5'), SwapModule(name='delta-6', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-6'), SwapModule(name='delta-7', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-7'), SwapModule(name='delta-8', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-8'), SwapModule(name='delta-9', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-9'), SwapModule(name='delta-10', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-10'), SwapModule(name='delta-11', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-11'), SwapModule(name='delta-12', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-12'), SwapModule(name='delta-13', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-13'), SwapModule(name='delta-14', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-14'), SwapModule(name='delta-15', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-15'), SwapModule(name='delta-16', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-16'), SwapModule(name='delta-17', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-17'), SwapModule(name='delta-18', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-18'), SwapModule(name='delta-19', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-19'), SwapModule(name='delta-20', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-20'), SwapModule(name='delta-21', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-21'), SwapModule(name='delta-22', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-22'), SwapModule(name='delta-23', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-23'), SwapModule(name='delta-24', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-24'), SwapModule(name='delta-25', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-25'), SwapModule(name='delta-26', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-26'), SwapModule(name='delta-27', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-27'), SwapModule(name='delta-28', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-28'), SwapModule(name='delta-29', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-29'), SwapModule(name='delta-30', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-30'), SwapModule(name='delta-31', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-31'), SwapModule(name='delta-32', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-32'), SwapModule(name='delta-33', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-33'), SwapModule(name='delta-34', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-34'), SwapModule(name='delta-35', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-35'), SwapModule(name='delta-36', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-36'), SwapModule(name='delta-37', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-37'), SwapModule(name='delta-38', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-38'), SwapModule(name='delta-39', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-39'), SwapModule(name='delta-40', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-40'), SwapModule(name='delta-41', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-41'), SwapModule(name='delta-42', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-42'), SwapModule(name='delta-43', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-43'), SwapModule(name='delta-44', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-44'), SwapModule(name='delta-45', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-45'), SwapModule(name='delta-46', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-46'), SwapModule(name='delta-47', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-47'), SwapModule(name='delta-48', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-48'), SwapModule(name='delta-49', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-49'), SwapModule(name='delta-50', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-50'), SwapModule(name='delta-51', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-51'), SwapModule(name='delta-52', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-52'), SwapModule(name='delta-53', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-53'), SwapModule(name='delta-54', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-54'), SwapModule(name='delta-55', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-55'), SwapModule(name='delta-56', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-56'), SwapModule(name='delta-57', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-57'), SwapModule(name='delta-58', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-58'), SwapModule(name='delta-59', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-59'), SwapModule(name='delta-60', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-60'), SwapModule(name='delta-61', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-61'), SwapModule(name='delta-62', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-62'), SwapModule(name='delta-63', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-63'), SwapModule(name='delta-64', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-64')], chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/vllm/.idea/full_models/Llama-2-13b-hf', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, seed=0, swap_space=4, gpu_memory_utilization=0.85, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=True, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=64, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=32, enable_delta=False, max_deltas=0, max_cpu_deltas=32, max_delta_bitwidth=4, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_prefetch=False, scheduler_policy='fcfs', max_swap_slots=8, max_cpu_models=18, enable_swap=True, engine_use_ray=False, disable_log_requests=True, max_log_len=None)
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f963db85d20>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-05-17 03:01:25,648	INFO worker.py:1749 -- Started a local Ray instance.
INFO 05-17 03:01:28 llm_engine.py:90] Initializing an LLM engine (v0.3.3) with config: model='/vllm/.idea/full_models/Llama-2-13b-hf', tokenizer='/vllm/.idea/full_models/Llama-2-13b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, lora_config=None, delta_config=None, swap_config=SwapConfig(max_packed_model=8, max_cpu_model=18), max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, seed=0)
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f963db85cf0>: Failed to establish a new connection: [Errno 111] Connection refused'))
INFO 05-17 03:01:39 selector.py:16] Using FlashAttention backend.
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f963db85420>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f963db841f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
[36m(RayWorkerVllm pid=88714)[0m INFO 05-17 03:01:42 selector.py:16] Using FlashAttention backend.
[36m(RayWorkerVllm pid=88920)[0m INFO 05-17 03:01:42 selector.py:16] Using FlashAttention backend.
[36m(RayWorkerVllm pid=89189)[0m INFO 05-17 03:01:42 selector.py:16] Using FlashAttention backend.
INFO 05-17 03:01:54 model_runner.py:161] Loading model weights took 6.1114 GB
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f963db865f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
[36m(RayWorkerVllm pid=89189)[0m INFO 05-17 03:01:55 model_runner.py:161] Loading model weights took 6.1114 GB
[36m(RayWorkerVllm pid=88714)[0m INFO 05-17 03:01:56 model_runner.py:161] Loading model weights took 6.1114 GB
[36m(RayWorkerVllm pid=88920)[0m INFO 05-17 03:01:56 model_runner.py:161] Loading model weights took 6.1114 GB
INFO 05-17 03:02:08 ray_gpu_executor.py:275] # GPU blocks: 3146, # CPU blocks: 1310
INFO 05-17 03:02:11 block_manager.py:264] disable automatic prefix caching
WARNING 05-17 03:02:11 serving_chat.py:373] No chat template provided. Chat API will not work.
INFO:     Started server process [80577]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     ::1:56060 - "GET /sysinfo HTTP/1.1" 200 OK
Translating from base-model to /vllm/.idea/full_models/Llama-2-13b-hf
Warming up starts
{'id': 22, 'prompt': 'USER: Write a python one line lambda function that calculates mean of two lists, without using any imported libraries. The entire function should fit on a single line, start with this. mean = lambda A:\nASSISTANT:', 'timestamp': 0, 'model': 'delta-2', 'min_tokens': 32, 'max_tokens': 32}
Prefetching is disabled
INFO 05-17 03:02:14 models.py:164] Disk -> CPU: Loaded in 1.448 seconds
INFO 05-17 03:02:15 worker_manager.py:250] [1715886135.6891904] CPU -> GPU time: 1.0636
[36m(RayWorkerVllm pid=88714)[0m INFO 05-17 03:02:19 models.py:164] Disk -> CPU: Loaded in 6.058 seconds
[36m(RayWorkerVllm pid=88920)[0m INFO 05-17 03:02:19 models.py:164] Disk -> CPU: Loaded in 6.086 seconds
[36m(RayWorkerVllm pid=89189)[0m INFO 05-17 03:02:19 models.py:164] Disk -> CPU: Loaded in 6.057 seconds
[36m(RayWorkerVllm pid=88714)[0m INFO 05-17 03:02:19 worker_manager.py:250] [1715886139.9220607] CPU -> GPU time: 0.6855
[36m(RayWorkerVllm pid=89189)[0m INFO 05-17 03:02:20 worker_manager.py:250] [1715886140.113022] CPU -> GPU time: 0.8762
[36m(RayWorkerVllm pid=88920)[0m INFO 05-17 03:02:20 worker_manager.py:250] [1715886140.8469694] CPU -> GPU time: 1.5813
INFO 05-17 03:02:21 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%
INFO 05-17 03:02:26 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%
INFO:     ::1:56063 - "POST /v1/completions HTTP/1.1" 200 OK
Warming up ends
Issuing queries
sending 1 queries at 0.8
sending 1 queries at 2.1
sending 1 queries at 3.0
sending 1 queries at 3.8000000000000003
sending 1 queries at 4.4
sending 1 queries at 5.4
sending 1 queries at 6.0
sending 1 queries at 8.200000000000001
sending 1 queries at 11.5
sending 1 queries at 12.0
sending 1 queries at 13.600000000000001
sending 1 queries at 14.3
sending 1 queries at 15.200000000000001
sending 2 queries at 17.8
sending 2 queries at 17.900000000000002
sending 1 queries at 19.700000000000003
sending 1 queries at 21.200000000000003
sending 1 queries at 23.3
sending 1 queries at 27.1
sending 1 queries at 28.700000000000003
sending 1 queries at 29.3
Prefetching is disabled
Prefetching is disabled
INFO 05-17 03:02:28 models.py:164] Disk -> CPU: Loaded in 1.443 seconds
Prefetching is disabled
[36m(RayWorkerVllm pid=88714)[0m INFO 05-17 03:02:33 models.py:164] Disk -> CPU: Loaded in 5.925 seconds
INFO 05-17 03:02:34 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=88920)[0m INFO 05-17 03:02:33 models.py:164] Disk -> CPU: Loaded in 5.953 seconds
[36m(RayWorkerVllm pid=89189)[0m INFO 05-17 03:02:33 models.py:164] Disk -> CPU: Loaded in 5.872 seconds
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
INFO 05-17 03:02:36 models.py:164] Disk -> CPU: Loaded in 1.612 seconds
Prefetching is disabled
Prefetching is disabled
INFO 05-17 03:02:39 models.py:164] Disk -> CPU: Loaded in 1.623 seconds
Prefetching is disabled
Prefetching is disabled
[36m(RayWorkerVllm pid=89189)[0m INFO 05-17 03:02:40 models.py:164] Disk -> CPU: Loaded in 5.940 seconds
[36m(RayWorkerVllm pid=88714)[0m INFO 05-17 03:02:41 models.py:164] Disk -> CPU: Loaded in 6.061 seconds
[36m(RayWorkerVllm pid=88920)[0m INFO 05-17 03:02:41 models.py:164] Disk -> CPU: Loaded in 6.117 seconds
INFO 05-17 03:02:41 models.py:164] Disk -> CPU: Loaded in 1.685 seconds
Prefetching is disabled
INFO 05-17 03:02:55 metrics.py:276] Avg prompt throughput: 1.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%
total threads: 23
[36m(RayWorkerVllm pid=88714)[0m INFO 05-17 03:02:47 models.py:164] Disk -> CPU: Loaded in 5.722 seconds
[36m(RayWorkerVllm pid=89189)[0m INFO 05-17 03:02:47 models.py:164] Disk -> CPU: Loaded in 5.802 seconds
[36m(RayWorkerVllm pid=88920)[0m INFO 05-17 03:02:47 models.py:164] Disk -> CPU: Loaded in 5.793 seconds
[36m(RayWorkerVllm pid=89189)[0m INFO 05-17 03:02:53 models.py:164] Disk -> CPU: Loaded in 5.507 seconds
[36m(RayWorkerVllm pid=88714)[0m INFO 05-17 03:02:53 models.py:164] Disk -> CPU: Loaded in 5.540 seconds
[36m(RayWorkerVllm pid=88920)[0m INFO 05-17 03:02:54 models.py:164] Disk -> CPU: Loaded in 5.527 seconds
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
Prefetching is disabled
INFO 05-17 03:02:57 models.py:164] Disk -> CPU: Loaded in 1.212 seconds
INFO 05-17 03:02:59 models.py:164] Disk -> CPU: Loaded in 1.219 seconds
[36m(RayWorkerVllm pid=89189)[0m INFO 05-17 03:03:01 models.py:164] Disk -> CPU: Loaded in 5.387 seconds
INFO 05-17 03:03:08 metrics.py:276] Avg prompt throughput: 9.9 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 21 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
[36m(RayWorkerVllm pid=88714)[0m INFO 05-17 03:03:01 models.py:164] Disk -> CPU: Loaded in 5.668 seconds
[36m(RayWorkerVllm pid=88920)[0m INFO 05-17 03:03:01 models.py:164] Disk -> CPU: Loaded in 6.077 seconds
[36m(RayWorkerVllm pid=89189)[0m INFO 05-17 03:03:06 models.py:164] Disk -> CPU: Loaded in 4.932 seconds
[36m(RayWorkerVllm pid=88714)[0m INFO 05-17 03:03:07 models.py:164] Disk -> CPU: Loaded in 5.049 seconds
[36m(RayWorkerVllm pid=88920)[0m INFO 05-17 03:03:07 models.py:164] Disk -> CPU: Loaded in 4.986 seconds
INFO:     ::1:56434 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:03:13 metrics.py:276] Avg prompt throughput: 78.6 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:56376 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:03:19 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%
INFO:     ::1:56360 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:03:25 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.5 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO 05-17 03:03:31 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.2 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO 05-17 03:03:36 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.9 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:56656 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:03:41 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.8 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO:     ::1:56428 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:03:48 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.0 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO 05-17 03:03:58 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO 05-17 03:04:02 models.py:164] Disk -> CPU: Loaded in 18.514 seconds
[36m(RayWorkerVllm pid=88714)[0m INFO 05-17 03:04:02 models.py:164] Disk -> CPU: Loaded in 18.487 seconds
[36m(RayWorkerVllm pid=88920)[0m INFO 05-17 03:04:02 models.py:164] Disk -> CPU: Loaded in 18.495 seconds
[36m(RayWorkerVllm pid=89189)[0m INFO 05-17 03:04:02 models.py:164] Disk -> CPU: Loaded in 18.499 seconds
INFO 05-17 03:04:04 metrics.py:276] Avg prompt throughput: 9.9 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO 05-17 03:04:09 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO:     ::1:56356 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:04:15 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.3 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%
INFO:     ::1:56640 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:04:20 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO 05-17 03:04:25 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.3 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:56346 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:04:31 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%
INFO:     ::1:56348 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:04:36 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO 05-17 03:04:41 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%
INFO 05-17 03:04:47 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.3%, CPU KV cache usage: 0.0%
INFO:     ::1:56384 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:04:52 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 16.9 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%
INFO:     ::1:56582 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:04:58 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 16.2 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%
INFO:     ::1:56550 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:56350 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:05:03 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%
INFO:     ::1:56430 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:05:08 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.7 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO:     ::1:56368 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:05:13 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.4 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO 05-17 03:05:19 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO 05-17 03:05:24 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.2 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO:     ::1:56340 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:05:29 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.4 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO 05-17 03:05:35 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO 05-17 03:05:40 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.8 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%
INFO 05-17 03:05:45 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.7 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:56338 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:05:50 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.9 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO:     ::1:56652 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:05:56 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.6 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO 05-17 03:06:01 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.0 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO 05-17 03:06:07 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:56342 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:06:12 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO 05-17 03:06:17 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO 05-17 03:06:22 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.3 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO 05-17 03:06:27 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 9.4 tokens/s, Running: 4 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:56354 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:06:32 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO 05-17 03:06:38 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO 05-17 03:06:43 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO:     ::1:56504 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:06:48 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:56344 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-17 03:06:53 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO 05-17 03:06:58 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO 05-17 03:07:03 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO 05-17 03:07:09 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO 05-17 03:07:14 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO 05-17 03:07:19 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%
INFO 05-17 03:07:24 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%
INFO 05-17 03:07:29 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%
INFO:     ::1:56436 - "POST /v1/completions HTTP/1.1" 200 OK
Results written to .artifact/benchmarks/results/9bc32cc6-2f7d-482e-8930-9cdde9561b56.jsonl
Killing process 80577...
/var/spool/slurmd/job3676317/slurm_script: line 42: 80544 Killed                  apptainer run --nv --home /mnt/petrelfs/huqinghao/xzyao/:/home/xiayao --bind /mnt/petrelfs/huqinghao/xzyao/code/vllm:/vllm --bind /mnt/petrelfs/huqinghao/xzyao/code/triteia:/triteia --env PYTHONPATH=/vllm:/triteia --env CUDA_LAUNCH_BLOCKING=1 --env TVM_TARGET=nvidia/nvidia-a100 --env BITBLAS_TARGET=nvidia/nvidia-a100 --env RAY_DEDUP_LOGS=0 --workdir /vllm /mnt/petrelfs/huqinghao/xzyao/images/deltaserve.sif python3 /vllm/vllm/entrypoints/openai/api_server.py --model /vllm/.idea/full_models/Llama-2-13b-hf --disable-log-requests --gpu-memory-utilization 0.85 --swap-modules /vllm/.idea/full_models/Llama-2-13b-hf=/vllm/.idea/full_models/Llama-2-13b-hf delta-1=/vllm/.idea/full_models/Llama-2-13b-hf-1 delta-2=/vllm/.idea/full_models/Llama-2-13b-hf-2 delta-3=/vllm/.idea/full_models/Llama-2-13b-hf-3 delta-4=/vllm/.idea/full_models/Llama-2-13b-hf-4 delta-5=/vllm/.idea/full_models/Llama-2-13b-hf-5 delta-6=/vllm/.idea/full_models/Llama-2-13b-hf-6 delta-7=/vllm/.idea/full_models/Llama-2-13b-hf-7 delta-8=/vllm/.idea/full_models/Llama-2-13b-hf-8 delta-9=/vllm/.idea/full_models/Llama-2-13b-hf-9 delta-10=/vllm/.idea/full_models/Llama-2-13b-hf-10 delta-11=/vllm/.idea/full_models/Llama-2-13b-hf-11 delta-12=/vllm/.idea/full_models/Llama-2-13b-hf-12 delta-13=/vllm/.idea/full_models/Llama-2-13b-hf-13 delta-14=/vllm/.idea/full_models/Llama-2-13b-hf-14 delta-15=/vllm/.idea/full_models/Llama-2-13b-hf-15 delta-16=/vllm/.idea/full_models/Llama-2-13b-hf-16 delta-17=/vllm/.idea/full_models/Llama-2-13b-hf-17 delta-18=/vllm/.idea/full_models/Llama-2-13b-hf-18 delta-19=/vllm/.idea/full_models/Llama-2-13b-hf-19 delta-20=/vllm/.idea/full_models/Llama-2-13b-hf-20 delta-21=/vllm/.idea/full_models/Llama-2-13b-hf-21 delta-22=/vllm/.idea/full_models/Llama-2-13b-hf-22 delta-23=/vllm/.idea/full_models/Llama-2-13b-hf-23 delta-24=/vllm/.idea/full_models/Llama-2-13b-hf-24 delta-25=/vllm/.idea/full_models/Llama-2-13b-hf-25 delta-26=/vllm/.idea/full_models/Llama-2-13b-hf-26 delta-27=/vllm/.idea/full_models/Llama-2-13b-hf-27 delta-28=/vllm/.idea/full_models/Llama-2-13b-hf-28 delta-29=/vllm/.idea/full_models/Llama-2-13b-hf-29 delta-30=/vllm/.idea/full_models/Llama-2-13b-hf-30 delta-31=/vllm/.idea/full_models/Llama-2-13b-hf-31 delta-32=/vllm/.idea/full_models/Llama-2-13b-hf-32 delta-33=/vllm/.idea/full_models/Llama-2-13b-hf-33 delta-34=/vllm/.idea/full_models/Llama-2-13b-hf-34 delta-35=/vllm/.idea/full_models/Llama-2-13b-hf-35 delta-36=/vllm/.idea/full_models/Llama-2-13b-hf-36 delta-37=/vllm/.idea/full_models/Llama-2-13b-hf-37 delta-38=/vllm/.idea/full_models/Llama-2-13b-hf-38 delta-39=/vllm/.idea/full_models/Llama-2-13b-hf-39 delta-40=/vllm/.idea/full_models/Llama-2-13b-hf-40 delta-41=/vllm/.idea/full_models/Llama-2-13b-hf-41 delta-42=/vllm/.idea/full_models/Llama-2-13b-hf-42 delta-43=/vllm/.idea/full_models/Llama-2-13b-hf-43 delta-44=/vllm/.idea/full_models/Llama-2-13b-hf-44 delta-45=/vllm/.idea/full_models/Llama-2-13b-hf-45 delta-46=/vllm/.idea/full_models/Llama-2-13b-hf-46 delta-47=/vllm/.idea/full_models/Llama-2-13b-hf-47 delta-48=/vllm/.idea/full_models/Llama-2-13b-hf-48 delta-49=/vllm/.idea/full_models/Llama-2-13b-hf-49 delta-50=/vllm/.idea/full_models/Llama-2-13b-hf-50 delta-51=/vllm/.idea/full_models/Llama-2-13b-hf-51 delta-52=/vllm/.idea/full_models/Llama-2-13b-hf-52 delta-53=/vllm/.idea/full_models/Llama-2-13b-hf-53 delta-54=/vllm/.idea/full_models/Llama-2-13b-hf-54 delta-55=/vllm/.idea/full_models/Llama-2-13b-hf-55 delta-56=/vllm/.idea/full_models/Llama-2-13b-hf-56 delta-57=/vllm/.idea/full_models/Llama-2-13b-hf-57 delta-58=/vllm/.idea/full_models/Llama-2-13b-hf-58 delta-59=/vllm/.idea/full_models/Llama-2-13b-hf-59 delta-60=/vllm/.idea/full_models/Llama-2-13b-hf-60 delta-61=/vllm/.idea/full_models/Llama-2-13b-hf-61 delta-62=/vllm/.idea/full_models/Llama-2-13b-hf-62 delta-63=/vllm/.idea/full_models/Llama-2-13b-hf-63 delta-64=/vllm/.idea/full_models/Llama-2-13b-hf-64 --tensor-parallel-size 4 --enforce-eager --max-deltas 0 --enable-swap --max-swap-slots 8 --max-cpu-models 18
