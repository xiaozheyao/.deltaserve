/mnt/hwfile/huqinghao/miniconda3/bin/python
2024-05-12 12:32:23,012 - INFO - Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-05-12 12:32:23,013 - INFO - Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-05-12 12:32:23,013 - INFO - NumExpr defaulting to 8 threads.
2024-05-12 12:32:23,935	INFO scripts.py:1182 -- Did not find any active Ray processes.

==========
== CUDA ==
==========

CUDA Version 12.1.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

*************************
** DEPRECATION NOTICE! **
*************************
THIS IMAGE IS DEPRECATED and is scheduled for DELETION.
    https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/support-policy.md

Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d588e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:160: UserWarning: Field "model_name_or_path" has conflict with protected namespace "model_".

You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.
  warnings.warn(
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58e80>: Failed to establish a new connection: [Errno 111] Connection refused'))
INFO 05-12 12:32:36 api_server.py:219] vLLM API server version 0.3.3
INFO 05-12 12:32:36 api_server.py:220] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=[], delta_modules=[], swap_modules=[SwapModule(name='delta-1', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-1'), SwapModule(name='delta-2', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-2'), SwapModule(name='delta-3', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-3'), SwapModule(name='delta-4', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-4'), SwapModule(name='delta-5', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-5'), SwapModule(name='delta-6', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-6'), SwapModule(name='delta-7', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-7'), SwapModule(name='delta-8', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-8')], chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/vllm/.idea/full_models/Llama-2-13b-hf', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, seed=0, swap_space=4, gpu_memory_utilization=0.85, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=True, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=64, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=32, enable_delta=False, max_deltas=0, max_cpu_deltas=32, max_delta_bitwidth=4, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_prefetch=False, scheduler_policy='fcfs', engine_use_ray=False, disable_log_requests=True, max_log_len=None)
2024-05-12 12:32:39,214	INFO worker.py:1749 -- Started a local Ray instance.
INFO 05-12 12:32:41 llm_engine.py:87] Initializing an LLM engine (v0.3.3) with config: model='/vllm/.idea/full_models/Llama-2-13b-hf', tokenizer='/vllm/.idea/full_models/Llama-2-13b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, lora_config=None, delta_config=None, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, seed=0)
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d59270>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d59b40>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5a410>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5ace0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5a2f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d59a20>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d59150>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58dc0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b5b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b550>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58a90>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d59060>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d59930>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5a290>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4562153be0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d59ff0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d59720>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d580d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b010>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5ab30>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b130>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58190>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d594e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d59db0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5a260>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5a710>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d59ab0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d591e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58c40>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b3a0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5ac20>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4562153d60>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b2e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d59030>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d59900>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5a1d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5be80>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d59db0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d594e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58190>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b880>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5a650>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5a6e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5ab30>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b010>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d580d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58040>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5bee0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5af80>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b8b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b430>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5a740>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f45621539d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5a3b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5bbb0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b610>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58100>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58eb0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d987c0>: Failed to establish a new connection: [Errno 111] Connection refused'))
INFO 05-12 12:42:26 selector.py:16] Using FlashAttention backend.
[36m(RayWorkerVllm pid=116680)[0m Exception ignored in: <function TempDirectory.__del__ at 0x7f41665f0e50>
[36m(RayWorkerVllm pid=116680)[0m Traceback (most recent call last):
[36m(RayWorkerVllm pid=116680)[0m   File "/usr/local/lib/python3.10/dist-packages/bitblas/3rdparty/tvm/python/tvm/contrib/utils.py", line 145, in __del__
[36m(RayWorkerVllm pid=116680)[0m     self.remove()
[36m(RayWorkerVllm pid=116680)[0m   File "/usr/local/lib/python3.10/dist-packages/bitblas/3rdparty/tvm/python/tvm/contrib/utils.py", line 121, in remove
[36m(RayWorkerVllm pid=116680)[0m     if self.temp_dir:
[36m(RayWorkerVllm pid=116680)[0m AttributeError: 'TempDirectory' object has no attribute 'temp_dir'
[36m(RayWorkerVllm pid=117328)[0m Exception ignored in: <function TempDirectory.__del__ at 0x7f6efdd0ce50>
[36m(RayWorkerVllm pid=117328)[0m Traceback (most recent call last):
[36m(RayWorkerVllm pid=117328)[0m   File "/usr/local/lib/python3.10/dist-packages/bitblas/3rdparty/tvm/python/tvm/contrib/utils.py", line 145, in __del__
[36m(RayWorkerVllm pid=117328)[0m     self.remove()
[36m(RayWorkerVllm pid=117328)[0m   File "/usr/local/lib/python3.10/dist-packages/bitblas/3rdparty/tvm/python/tvm/contrib/utils.py", line 121, in remove
[36m(RayWorkerVllm pid=117328)[0m     if self.temp_dir:
[36m(RayWorkerVllm pid=117328)[0m AttributeError: 'TempDirectory' object has no attribute 'temp_dir'
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58700>: Failed to establish a new connection: [Errno 111] Connection refused'))
[36m(RayWorkerVllm pid=116680)[0m Exception ignored in: <function TempDirectory.__del__ at 0x7f41665f0e50>
[36m(RayWorkerVllm pid=116680)[0m Traceback (most recent call last):
[36m(RayWorkerVllm pid=116680)[0m   File "/usr/local/lib/python3.10/dist-packages/bitblas/3rdparty/tvm/python/tvm/contrib/utils.py", line 145, in __del__
[36m(RayWorkerVllm pid=116680)[0m     self.remove()
[36m(RayWorkerVllm pid=116680)[0m   File "/usr/local/lib/python3.10/dist-packages/bitblas/3rdparty/tvm/python/tvm/contrib/utils.py", line 121, in remove
[36m(RayWorkerVllm pid=116680)[0m     if self.temp_dir:
[36m(RayWorkerVllm pid=116680)[0m AttributeError: 'TempDirectory' object has no attribute 'temp_dir'
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b070>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5abc0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5bac0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d98340>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5bc70>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b220>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58160>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d596f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b190>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d983a0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d598a0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58790>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b0d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5aaa0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d98a00>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5ac80>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5abc0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5af50>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58f10>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d59f90>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d990c0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58eb0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58100>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b610>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b550>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d985b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5ad10>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5aaa0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5aad0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5aec0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58a00>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d98880>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b670>: Failed to establish a new connection: [Errno 111] Connection refused'))
[36m(RayWorkerVllm pid=117328)[0m Exception ignored in: <function TempDirectory.__del__ at 0x7f6efdd0ce50>
[36m(RayWorkerVllm pid=117328)[0m Traceback (most recent call last):
[36m(RayWorkerVllm pid=117328)[0m   File "/usr/local/lib/python3.10/dist-packages/bitblas/3rdparty/tvm/python/tvm/contrib/utils.py", line 145, in __del__
[36m(RayWorkerVllm pid=117328)[0m     self.remove()
[36m(RayWorkerVllm pid=117328)[0m   File "/usr/local/lib/python3.10/dist-packages/bitblas/3rdparty/tvm/python/tvm/contrib/utils.py", line 121, in remove
[36m(RayWorkerVllm pid=117328)[0m     if self.temp_dir:
[36m(RayWorkerVllm pid=117328)[0m AttributeError: 'TempDirectory' object has no attribute 'temp_dir'
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b640>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b940>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5ba30>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d59db0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4562153b80>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5ada0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5acb0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d587c0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d595a0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d59660>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58790>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b0d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5aa40>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d597b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4562153a00>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4562153d60>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5bb50>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5afe0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58730>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d59600>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d988b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5aad0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5aaa0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58b80>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5a260>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d993c0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4562153d60>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b910>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5ab30>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b010>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5af50>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d99990>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58790>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b0d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5aa40>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d597b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d983a0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5bd00>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5bdc0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b1f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b9d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b340>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d99120>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5aad0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5aaa0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58b80>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5a260>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d99c00>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4562153dc0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d590f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b550>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5b610>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d5af50>: Failed to establish a new connection: [Errno 111] Connection refused'))
[36m(RayWorkerVllm pid=117328)[0m BitBLAS Operator not loaded, retrying in 5 seconds...
[36m(RayWorkerVllm pid=117328)[0m Error: [Errno 17] File exists: '/home/xiayao/.cache/bitblas/nvidia/nvidia-a100/c96df0f34eb23be516ef0126651d4e78b151337f28ebc87cc5492d79d135855f/tvm_rt_mod'
[36m(RayWorkerVllm pid=116680)[0m BitBLAS Operator not loaded, retrying in 5 seconds...
[36m(RayWorkerVllm pid=116680)[0m Error: [Errno 17] File exists: '/home/xiayao/.cache/bitblas/nvidia/nvidia-a100/c96df0f34eb23be516ef0126651d4e78b151337f28ebc87cc5492d79d135855f/tvm_rt_mod'
[36m(RayWorkerVllm pid=116680)[0m BitBLAS Operator not loaded, retrying in 5 seconds...
[36m(RayWorkerVllm pid=116680)[0m Error: [Errno 17] File exists: '/home/xiayao/.cache/bitblas/nvidia/nvidia-a100/c96df0f34eb23be516ef0126651d4e78b151337f28ebc87cc5492d79d135855f/tvm_rt_mod'
[36m(RayWorkerVllm pid=117328)[0m BitBLAS Operator not loaded, retrying in 5 seconds...
[36m(RayWorkerVllm pid=117328)[0m Error: [Errno 17] File exists: '/home/xiayao/.cache/bitblas/nvidia/nvidia-a100/349fa12c234404a571f1253940ed6924c7588a9dfaff45397b4918254a8f8de0/tvm_rt_mod'
[36m(RayWorkerVllm pid=118135)[0m INFO 05-12 12:51:14 selector.py:16] Using FlashAttention backend.
[36m(RayWorkerVllm pid=116680)[0m INFO 05-12 12:51:21 selector.py:16] Using FlashAttention backend.
[36m(RayWorkerVllm pid=117328)[0m INFO 05-12 12:56:48 selector.py:16] Using FlashAttention backend.
INFO 05-12 12:57:01 model_runner.py:155] Loading model weights took 6.1114 GB
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d9a290>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f4561d58790>: Failed to establish a new connection: [Errno 111] Connection refused'))
[36m(RayWorkerVllm pid=117328)[0m INFO 05-12 12:57:03 model_runner.py:155] Loading model weights took 6.1114 GB
[36m(RayWorkerVllm pid=118135)[0m INFO 05-12 12:57:03 model_runner.py:155] Loading model weights took 6.1114 GB
[36m(RayWorkerVllm pid=116680)[0m INFO 05-12 12:57:03 model_runner.py:155] Loading model weights took 6.1114 GB
INFO 05-12 12:57:16 ray_gpu_executor.py:266] # GPU blocks: 19075, # CPU blocks: 1310
INFO 05-12 12:57:18 block_manager.py:264] disable automatic prefix caching
WARNING 05-12 12:57:19 serving_chat.py:373] No chat template provided. Chat API will not work.
INFO:     Started server process [108936]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     ::1:53836 - "GET /sysinfo HTTP/1.1" 200 OK
Translating from base-model to /vllm/.idea/full_models/Llama-2-13b-hf
Warming up starts
{'id': 163, 'prompt': "USER: Please reproduce this report verbatim, but do correct where items have been spelled out verbally (e.g. Cal gary C A L G A R Y  -> Calgary), or where punctuation has been dictated replace it with the appropriate symbol. Retain all the other instructions provided in the dictation.\nBEGIN:\nThis is a dictation of a medical report. Please put that medical report in bold at the top of the document.\n\nSolicitor's reference in bold. Capitals A C C slash one two three slash nine eight seven.\n\nNext line in bold again. Instructions from Joe Bloggs Solicitor's.\n\nNext line. Client's name in bold. Then Chris Smith.\n\nAddress in bold on the next line one light foot streets at LIGHT F O O T street in Hull\n\nH O O L E in Chester. Date of birth in bold 1st of March 1965. Date of accident in bold\n\non the next line 10th of February 2010. Date of report on the next line in bold 15th of\n\nApril 2010 next line in bold review of notes none next line in bold\n\nidentification none next line in bold occupation unemployed first heading is\n\naccident in bold underlined on the afternoon of the 10th of February\n\nChris Smith was traveling from Chester to Birmingham in a Ford Fiesta fitted\n\nwith seatbelts and headrests he was the front seat passenger full stop\n\nwithout warning, while stood suddenly in traffic, another vehicle impacted with the rear of\n\nthe vehicle he was travelling in and shunted him into the vehicle in front, full stop.\n\nNext paragraph and heading is in bold and underlined again, movement of client on impact,\n\nhe suffered a force fl\nASSISTANT:", 'timestamp': 0, 'model': 'delta-8', 'min_tokens': 285, 'max_tokens': 285}
INFO 05-12 12:57:25 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:25 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 12:57:30 api_server.py:155] waiting for model...
INFO 05-12 12:57:30 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:30 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%
INFO 05-12 12:57:35 metrics.py:276] Avg prompt throughput: 84.4 tokens/s, Avg generation throughput: 26.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%
INFO 05-12 12:57:40 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%
INFO:     ::1:53838 - "POST /v1/completions HTTP/1.1" 200 OK
Warming up ends
Issuing queries
sending 1 queries at 0.1
sending 1 queries at 0.2
sending 1 queries at 0.30000000000000004
sending 2 queries at 0.4
sending 2 queries at 0.5
sending 1 queries at 0.7000000000000001
sending 2 queries at 1.0
sending 2 queries at 1.2000000000000002
sending 1 queries at 1.3
sending 4 queries at 1.5
sending 1 queries at 1.7000000000000002
sending 1 queries at 1.8
sending 1 queries at 2.0
sending 1 queries at 2.3000000000000003
sending 1 queries at 2.4000000000000004
sending 1 queries at 2.5
sending 2 queries at 2.6
sending 2 queries at 2.7
sending 2 queries at 3.0
sending 2 queries at 3.1
sending 1 queries at 3.2
sending 3 queries at 3.3000000000000003
sending 1 queries at 3.4000000000000004
sending 1 queries at 3.5
sending 1 queries at 3.6
sending 1 queries at 3.8000000000000003
sending 1 queries at 3.9000000000000004
sending 2 queries at 4.0
sending 2 queries at 4.1000000000000005
sending 1 queries at 4.2
sending 3 queries at 4.3
sending 2 queries at 4.4
sending 2 queries at 4.5
sending 4 queries at 4.9
sending 2 queries at 5.0
sending 4 queries at 5.1000000000000005
sending 2 queries at 5.2
sending 2 queries at 5.300000000000001
sending 2 queries at 5.4
sending 2 queries at 5.6000000000000005
sending 1 queries at 5.9
sending 1 queries at 6.0
sending 1 queries at 6.300000000000001
sending 1 queries at 6.4
sending 5 queries at 6.5
sending 4 queries at 6.6000000000000005
sending 1 queries at 6.7
sending 2 queries at 6.800000000000001
sending 2 queries at 6.9
sending 1 queries at 7.0
sending 2 queries at 7.2
sending 2 queries at 7.300000000000001
sending 1 queries at 7.4
sending 2 queries at 7.5
sending 2 queries at 7.6000000000000005
sending 2 queries at 7.7
sending 2 queries at 7.800000000000001
sending 1 queries at 7.9
sending 2 queries at 8.200000000000001
sending 1 queries at 8.3
sending 1 queries at 8.4
sending 2 queries at 8.5
sending 1 queries at 8.700000000000001
sending 1 queries at 8.8
sending 1 queries at 8.9
sending 1 queries at 9.0
sending 1 queries at 9.1
sending 2 queries at 9.200000000000001
sending 1 queries at 9.4
sending 1 queries at 9.5
sending 1 queries at 9.700000000000001
sending 1 queries at 9.8
sending 2 queries at 9.9
sending 1 queries at 10.200000000000001
sending 2 queries at 10.3
sending 3 queries at 10.4
sending 1 queries at 10.5
sending 2 queries at 10.600000000000001
sending 3 queries at 10.700000000000001
sending 1 queries at 10.8
sending 2 queries at 10.9
sending 1 queries at 11.0
sending 1 queries at 11.100000000000001
sending 1 queries at 11.200000000000001
sending 3 queries at 11.4
sending 1 queries at 11.600000000000001
sending 1 queries at 11.8
sending 2 queries at 11.9
sending 1 queries at 12.100000000000001
sending 1 queries at 12.200000000000001
sending 1 queries at 12.700000000000001
sending 1 queries at 12.8
sending 2 queries at 12.9
sending 2 queries at 13.0
sending 1 queries at 13.200000000000001
sending 1 queries at 13.3
sending 2 queries at 13.4
sending 2 queries at 13.5
sending 1 queries at 13.600000000000001
sending 1 queries at 13.700000000000001
sending 1 queries at 13.9
sending 1 queries at 14.200000000000001
sending 2 queries at 14.3
sending 1 queries at 14.4
sending 2 queries at 14.5
sending 4 queries at 14.600000000000001
sending 2 queries at 14.700000000000001
sending 1 queries at 14.8
sending 1 queries at 15.0
sending 3 queries at 15.100000000000001
sending 1 queries at 15.200000000000001
sending 2 queries at 15.3
sending 1 queries at 15.5
sending 1 queries at 15.600000000000001
sending 2 queries at 15.700000000000001
sending 1 queries at 15.8
sending 3 queries at 15.9
sending 1 queries at 16.0
sending 1 queries at 16.2
sending 1 queries at 16.3
sending 4 queries at 16.400000000000002
sending 2 queries at 16.5
sending 2 queries at 16.6
sending 3 queries at 16.7
sending 2 queries at 16.8
sending 1 queries at 16.900000000000002
sending 1 queries at 17.1
sending 1 queries at 17.400000000000002
sending 1 queries at 17.5
sending 3 queries at 17.6
sending 2 queries at 17.7
sending 2 queries at 17.8
sending 1 queries at 18.0
sending 1 queries at 18.1
sending 2 queries at 18.3
sending 2 queries at 18.400000000000002
sending 1 queries at 18.7
sending 2 queries at 18.8
sending 1 queries at 19.0
sending 3 queries at 19.200000000000003
sending 4 queries at 19.3
sending 3 queries at 19.400000000000002
sending 1 queries at 19.5
sending 2 queries at 19.700000000000003
sending 2 queries at 19.8
sending 2 queries at 19.900000000000002
sending 3 queries at 20.0
sending 2 queries at 20.1
sending 1 queries at 20.3
sending 1 queries at 20.5
sending 1 queries at 20.6
sending 3 queries at 20.8
sending 1 queries at 20.900000000000002
sending 3 queries at 21.200000000000003
sending 2 queries at 21.3
sending 1 queries at 21.5
sending 1 queries at 21.6
sending 2 queries at 21.700000000000003
sending 2 queries at 21.8
sending 1 queries at 21.900000000000002
sending 1 queries at 22.200000000000003
sending 2 queries at 22.400000000000002
sending 2 queries at 22.700000000000003
sending 1 queries at 22.900000000000002
sending 1 queries at 23.200000000000003
sending 1 queries at 23.3
sending 1 queries at 23.400000000000002
sending 2 queries at 23.6
sending 1 queries at 23.700000000000003
sending 2 queries at 23.8
sending 2 queries at 23.900000000000002
sending 1 queries at 24.200000000000003
sending 2 queries at 24.3
sending 2 queries at 24.400000000000002
sending 1 queries at 24.5
sending 2 queries at 24.6
sending 3 queries at 24.700000000000003
sending 1 queries at 24.900000000000002
sending 1 queries at 25.200000000000003
sending 1 queries at 25.400000000000002
sending 1 queries at 25.6
sending 4 queries at 25.8
sending 3 queries at 25.900000000000002
sending 2 queries at 26.0
sending 4 queries at 26.1
sending 1 queries at 26.200000000000003
sending 2 queries at 26.3
sending 1 queries at 26.400000000000002
sending 1 queries at 26.5
sending 1 queries at 26.6
sending 5 queries at 26.700000000000003
sending 1 queries at 26.8
sending 1 queries at 26.900000000000002
sending 2 queries at 27.0
sending 2 queries at 27.1
sending 2 queries at 27.200000000000003
sending 1 queries at 27.3
sending 1 queries at 27.400000000000002
sending 3 queries at 27.6
sending 2 queries at 27.700000000000003
sending 4 queries at 27.8
sending 3 queries at 28.0
sending 2 queries at 28.1
sending 1 queries at 28.400000000000002
sending 3 queries at 28.5
sending 1 queries at 28.6
sending 1 queries at 28.700000000000003
sending 4 queries at 28.8
sending 2 queries at 29.1
sending 1 queries at 29.3
sending 2 queries at 29.400000000000002
sending 1 queries at 29.6
sending 1 queries at 29.700000000000003
sending 2 queries at 29.8
sending 1 queries at 30.0
INFO 05-12 12:57:41 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:41 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 metrics.py:276] Avg prompt throughput: 3.7 tokens/s, Avg generation throughput: 2.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:46 api_server.py:155] waiting for model...
INFO 05-12 12:57:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:53946 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:57:46 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 12:57:51 api_server.py:155] waiting for model...
INFO 05-12 12:57:51 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:155] waiting for model...
INFO 05-12 12:57:51 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:155] waiting for model...
INFO 05-12 12:57:51 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:155] waiting for model...
INFO 05-12 12:57:51 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:155] waiting for model...
INFO 05-12 12:57:51 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:155] waiting for model...
INFO 05-12 12:57:51 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:51 api_server.py:155] waiting for model...
INFO 05-12 12:57:51 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:155] waiting for model...
INFO 05-12 12:57:51 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:155] waiting for model...
INFO 05-12 12:57:51 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:155] waiting for model...
INFO 05-12 12:57:51 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:51 metrics.py:276] Avg prompt throughput: 235.3 tokens/s, Avg generation throughput: 15.9 tokens/s, Running: 33 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:51 api_server.py:134] Waiting for reload lock...
INFO:     ::1:53940 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:53908 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:57:51 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 12:57:56 api_server.py:155] waiting for model...
INFO 05-12 12:57:56 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:56 api_server.py:155] waiting for model...
INFO 05-12 12:57:56 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:56 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:56 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:56 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:56 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:56 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:56 api_server.py:155] waiting for model...
INFO 05-12 12:57:56 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:57:56 api_server.py:134] Waiting for reload lock...
INFO:     ::1:53958 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:57:56 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:57:56 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 12:58:01 api_server.py:155] waiting for model...
INFO 05-12 12:58:01 api_server.py:163] requested model is loaded --> continuing...
INFO 05-12 12:58:01 api_server.py:155] waiting for model...
INFO 05-12 12:58:01 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
Prefetching is disabled
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 metrics.py:276] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 10.3 tokens/s, Running: 33 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:155] waiting for model...
INFO 05-12 12:58:01 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:155] waiting for model...
INFO 05-12 12:58:01 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:155] waiting for model...
INFO 05-12 12:58:01 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:155] waiting for model...
INFO 05-12 12:58:01 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:155] waiting for model...
INFO 05-12 12:58:01 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:155] waiting for model...
INFO 05-12 12:58:01 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:155] waiting for model...
INFO 05-12 12:58:01 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:01 api_server.py:134] Waiting for reload lock...
INFO:     ::1:53952 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:58:01 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 12:58:06 api_server.py:155] waiting for model...
INFO 05-12 12:58:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:155] waiting for model...
INFO 05-12 12:58:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:155] waiting for model...
INFO 05-12 12:58:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:06 api_server.py:155] waiting for model...
INFO 05-12 12:58:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:155] waiting for model...
INFO 05-12 12:58:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:155] waiting for model...
INFO 05-12 12:58:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:155] waiting for model...
INFO 05-12 12:58:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:06 api_server.py:155] waiting for model...
INFO 05-12 12:58:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:155] waiting for model...
INFO 05-12 12:58:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:06 metrics.py:276] Avg prompt throughput: 112.7 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 43 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO 05-12 12:58:06 api_server.py:155] waiting for model...
INFO 05-12 12:58:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:53960 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:58:06 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 12:58:11 api_server.py:155] waiting for model...
INFO 05-12 12:58:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:155] waiting for model...
INFO 05-12 12:58:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
total threads: 369
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:155] waiting for model...
INFO 05-12 12:58:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:155] waiting for model...
INFO 05-12 12:58:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 12:58:11 metrics.py:276] Avg prompt throughput: 76.8 tokens/s, Avg generation throughput: 43.8 tokens/s, Running: 55 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54422 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:58:11 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 12:58:16 api_server.py:155] waiting for model...
INFO 05-12 12:58:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54296 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54470 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:58:16 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 12:58:21 api_server.py:155] waiting for model...
INFO 05-12 12:58:21 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:21 metrics.py:276] Avg prompt throughput: 1.7 tokens/s, Avg generation throughput: 16.7 tokens/s, Running: 53 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54464 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:58:21 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 12:58:26 api_server.py:155] waiting for model...
INFO 05-12 12:58:26 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:26 metrics.py:276] Avg prompt throughput: 4.5 tokens/s, Avg generation throughput: 111.8 tokens/s, Running: 52 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%
INFO:     ::1:53904 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:58:26 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 12:58:31 api_server.py:155] waiting for model...
INFO 05-12 12:58:31 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:31 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 53 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54072 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:58:32 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 12:58:37 api_server.py:155] waiting for model...
INFO 05-12 12:58:37 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:37 metrics.py:276] Avg prompt throughput: 5.4 tokens/s, Avg generation throughput: 116.3 tokens/s, Running: 53 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:53918 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:58:37 api_server.py:155] waiting for model...
INFO 05-12 12:58:37 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:53992 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:58:37 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 12:58:42 api_server.py:155] waiting for model...
INFO 05-12 12:58:42 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:42 metrics.py:276] Avg prompt throughput: 15.5 tokens/s, Avg generation throughput: 31.1 tokens/s, Running: 53 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:53902 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:58:42 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 12:58:47 api_server.py:155] waiting for model...
INFO 05-12 12:58:47 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:47 metrics.py:276] Avg prompt throughput: 8.1 tokens/s, Avg generation throughput: 42.9 tokens/s, Running: 54 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:53930 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:58:47 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 12:58:52 api_server.py:155] waiting for model...
INFO 05-12 12:58:52 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54018 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:58:52 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 12:58:57 api_server.py:155] waiting for model...
INFO 05-12 12:58:57 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:58:57 metrics.py:276] Avg prompt throughput: 5.0 tokens/s, Avg generation throughput: 11.0 tokens/s, Running: 53 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:53914 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:58:57 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 12:59:02 api_server.py:155] waiting for model...
INFO 05-12 12:59:02 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:59:02 metrics.py:276] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 135.2 tokens/s, Running: 53 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%
INFO:     ::1:53886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:59:02 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 12:59:07 api_server.py:155] waiting for model...
INFO 05-12 12:59:07 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:59:07 metrics.py:276] Avg prompt throughput: 5.6 tokens/s, Avg generation throughput: 72.2 tokens/s, Running: 51 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%
INFO:     ::1:53892 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54546 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:59:07 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 12:59:12 api_server.py:155] waiting for model...
INFO 05-12 12:59:12 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:59:12 metrics.py:276] Avg prompt throughput: 12.2 tokens/s, Avg generation throughput: 21.5 tokens/s, Running: 52 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%
INFO:     ::1:54122 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:59:12 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 12:59:17 api_server.py:155] waiting for model...
INFO 05-12 12:59:17 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:59:17 metrics.py:276] Avg prompt throughput: 7.6 tokens/s, Avg generation throughput: 10.6 tokens/s, Running: 53 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%
INFO:     ::1:53954 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:59:17 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 12:59:22 api_server.py:155] waiting for model...
INFO 05-12 12:59:22 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:59:22 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 21.0 tokens/s, Running: 52 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%
INFO:     ::1:54434 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:59:22 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 12:59:27 api_server.py:155] waiting for model...
INFO 05-12 12:59:27 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:53956 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:59:27 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 12:59:32 api_server.py:155] waiting for model...
INFO 05-12 12:59:32 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:59:32 metrics.py:276] Avg prompt throughput: 5.2 tokens/s, Avg generation throughput: 21.6 tokens/s, Running: 52 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%
INFO:     ::1:53934 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:59:32 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 12:59:37 api_server.py:155] waiting for model...
INFO 05-12 12:59:37 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:59:37 metrics.py:276] Avg prompt throughput: 3.7 tokens/s, Avg generation throughput: 21.1 tokens/s, Running: 52 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54344 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:59:37 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 12:59:42 api_server.py:155] waiting for model...
INFO 05-12 12:59:42 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:59:42 metrics.py:276] Avg prompt throughput: 10.5 tokens/s, Avg generation throughput: 31.8 tokens/s, Running: 52 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:53973 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:59:42 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 12:59:47 api_server.py:155] waiting for model...
INFO 05-12 12:59:47 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54516 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:59:47 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 12:59:52 api_server.py:155] waiting for model...
INFO 05-12 12:59:52 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:59:52 metrics.py:276] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 52 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:53910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54026 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:59:52 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 12:59:57 api_server.py:155] waiting for model...
INFO 05-12 12:59:57 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 12:59:57 metrics.py:276] Avg prompt throughput: 7.2 tokens/s, Avg generation throughput: 62.6 tokens/s, Running: 51 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54560 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 12:59:57 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:00:02 api_server.py:155] waiting for model...
INFO 05-12 13:00:02 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54162 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:00:02 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:00:07 api_server.py:155] waiting for model...
INFO 05-12 13:00:07 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:00:07 metrics.py:276] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 31.4 tokens/s, Running: 51 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54124 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:00:07 api_server.py:155] waiting for model...
INFO 05-12 13:00:07 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:53924 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:53996 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:00:07 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:00:12 api_server.py:155] waiting for model...
INFO 05-12 13:00:12 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:00:12 metrics.py:276] Avg prompt throughput: 13.0 tokens/s, Avg generation throughput: 83.0 tokens/s, Running: 49 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:53962 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:00:12 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:00:17 api_server.py:155] waiting for model...
INFO 05-12 13:00:17 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:00:17 metrics.py:276] Avg prompt throughput: 22.1 tokens/s, Avg generation throughput: 10.5 tokens/s, Running: 51 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:53975 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:00:17 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:00:22 api_server.py:155] waiting for model...
INFO 05-12 13:00:22 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:00:22 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 10.1 tokens/s, Running: 50 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:53916 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:00:22 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:00:27 api_server.py:155] waiting for model...
INFO 05-12 13:00:27 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:00:27 metrics.py:276] Avg prompt throughput: 19.3 tokens/s, Avg generation throughput: 9.6 tokens/s, Running: 50 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:53985 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:00:28 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:00:32 api_server.py:155] waiting for model...
INFO 05-12 13:00:32 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:00:32 metrics.py:276] Avg prompt throughput: 4.1 tokens/s, Avg generation throughput: 30.1 tokens/s, Running: 50 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54130 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:00:33 api_server.py:155] waiting for model...
INFO 05-12 13:00:33 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:53890 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:00:33 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:00:38 api_server.py:155] waiting for model...
INFO 05-12 13:00:38 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:00:38 metrics.py:276] Avg prompt throughput: 8.1 tokens/s, Avg generation throughput: 65.6 tokens/s, Running: 50 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54310 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:00:38 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:00:43 api_server.py:155] waiting for model...
INFO 05-12 13:00:43 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:00:43 metrics.py:276] Avg prompt throughput: 9.9 tokens/s, Avg generation throughput: 70.6 tokens/s, Running: 50 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54188 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:00:43 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:00:48 api_server.py:155] waiting for model...
INFO 05-12 13:00:48 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:00:48 metrics.py:276] Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 50.7 tokens/s, Running: 51 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:53906 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:53926 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:00:49 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:00:53 api_server.py:155] waiting for model...
INFO 05-12 13:00:53 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:00:53 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 126.1 tokens/s, Running: 49 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:53882 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:00:53 api_server.py:155] waiting for model...
INFO 05-12 13:00:53 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:53979 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:00:53 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:00:59 api_server.py:155] waiting for model...
INFO 05-12 13:00:59 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:00:59 metrics.py:276] Avg prompt throughput: 9.0 tokens/s, Avg generation throughput: 46.0 tokens/s, Running: 49 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54006 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:00:59 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:01:04 api_server.py:155] waiting for model...
INFO 05-12 13:01:04 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:01:04 metrics.py:276] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 79.2 tokens/s, Running: 49 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:53938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:01:04 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:01:09 api_server.py:155] waiting for model...
INFO 05-12 13:01:09 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:01:09 metrics.py:276] Avg prompt throughput: 8.0 tokens/s, Avg generation throughput: 45.4 tokens/s, Running: 49 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:53989 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:01:10 api_server.py:155] waiting for model...
INFO 05-12 13:01:10 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:53878 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:01:10 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:01:15 api_server.py:155] waiting for model...
INFO 05-12 13:01:15 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:01:15 metrics.py:276] Avg prompt throughput: 12.4 tokens/s, Avg generation throughput: 122.0 tokens/s, Running: 49 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO:     ::1:54012 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:01:15 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:01:20 api_server.py:155] waiting for model...
INFO 05-12 13:01:20 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:01:20 metrics.py:276] Avg prompt throughput: 61.3 tokens/s, Avg generation throughput: 39.5 tokens/s, Running: 50 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%
INFO:     ::1:54000 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:01:20 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:01:25 api_server.py:155] waiting for model...
INFO 05-12 13:01:25 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:01:25 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 69.3 tokens/s, Running: 49 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:53932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:01:25 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:01:31 api_server.py:155] waiting for model...
INFO 05-12 13:01:31 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:01:31 metrics.py:276] Avg prompt throughput: 7.3 tokens/s, Avg generation throughput: 9.6 tokens/s, Running: 49 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%
INFO:     ::1:54052 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:01:31 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:01:37 api_server.py:155] waiting for model...
INFO 05-12 13:01:37 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:01:37 metrics.py:276] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 65.9 tokens/s, Running: 49 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:53994 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:01:37 api_server.py:155] waiting for model...
INFO 05-12 13:01:37 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54056 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:01:37 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:01:42 api_server.py:155] waiting for model...
INFO 05-12 13:01:42 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:01:42 metrics.py:276] Avg prompt throughput: 7.2 tokens/s, Avg generation throughput: 95.1 tokens/s, Running: 49 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54032 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:01:42 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:01:47 api_server.py:155] waiting for model...
INFO 05-12 13:01:47 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:01:47 metrics.py:276] Avg prompt throughput: 3.3 tokens/s, Avg generation throughput: 29.0 tokens/s, Running: 49 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54170 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54068 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:01:47 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:01:53 api_server.py:155] waiting for model...
INFO 05-12 13:01:53 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:01:53 metrics.py:276] Avg prompt throughput: 65.4 tokens/s, Avg generation throughput: 27.0 tokens/s, Running: 47 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:53983 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:01:53 api_server.py:155] waiting for model...
INFO 05-12 13:01:53 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54518 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:01:53 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:01:58 api_server.py:155] waiting for model...
INFO 05-12 13:01:58 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:01:58 metrics.py:276] Avg prompt throughput: 9.2 tokens/s, Avg generation throughput: 19.8 tokens/s, Running: 48 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54050 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:01:58 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:02:03 api_server.py:155] waiting for model...
INFO 05-12 13:02:03 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:02:03 metrics.py:276] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 46.1 tokens/s, Running: 48 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:53948 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:02:03 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:02:09 api_server.py:155] waiting for model...
INFO 05-12 13:02:09 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:02:09 metrics.py:276] Avg prompt throughput: 2.9 tokens/s, Avg generation throughput: 8.6 tokens/s, Running: 48 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%
INFO:     ::1:53884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:02:09 api_server.py:155] waiting for model...
INFO 05-12 13:02:09 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54480 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:02:09 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:02:14 api_server.py:155] waiting for model...
INFO 05-12 13:02:14 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:02:14 metrics.py:276] Avg prompt throughput: 6.8 tokens/s, Avg generation throughput: 46.1 tokens/s, Running: 48 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54076 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:02:14 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:02:19 api_server.py:155] waiting for model...
INFO 05-12 13:02:19 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:02:19 metrics.py:276] Avg prompt throughput: 7.6 tokens/s, Avg generation throughput: 49.0 tokens/s, Running: 49 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54537 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:02:19 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:02:25 api_server.py:155] waiting for model...
INFO 05-12 13:02:25 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:02:25 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 35.0 tokens/s, Running: 48 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54004 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:02:25 api_server.py:155] waiting for model...
INFO 05-12 13:02:25 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:53987 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:02:25 api_server.py:155] waiting for model...
INFO 05-12 13:02:25 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54082 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:02:26 api_server.py:155] waiting for model...
INFO 05-12 13:02:26 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:53964 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54022 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54078 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:02:26 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:02:30 api_server.py:155] waiting for model...
INFO 05-12 13:02:30 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:02:30 metrics.py:276] Avg prompt throughput: 19.9 tokens/s, Avg generation throughput: 156.8 tokens/s, Running: 46 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54062 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:02:30 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:02:35 api_server.py:155] waiting for model...
INFO 05-12 13:02:35 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:02:35 metrics.py:276] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 27.9 tokens/s, Running: 46 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54074 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:02:36 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:02:42 api_server.py:155] waiting for model...
INFO 05-12 13:02:42 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:02:42 metrics.py:276] Avg prompt throughput: 5.4 tokens/s, Avg generation throughput: 112.1 tokens/s, Running: 46 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%
INFO:     ::1:53896 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:02:42 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:02:47 api_server.py:155] waiting for model...
INFO 05-12 13:02:47 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54586 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:02:47 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:02:52 api_server.py:155] waiting for model...
INFO 05-12 13:02:52 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:02:52 metrics.py:276] Avg prompt throughput: 1.5 tokens/s, Avg generation throughput: 31.5 tokens/s, Running: 46 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54058 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:02:53 api_server.py:155] waiting for model...
INFO 05-12 13:02:53 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54064 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:02:53 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:02:58 api_server.py:155] waiting for model...
INFO 05-12 13:02:58 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:02:58 metrics.py:276] Avg prompt throughput: 11.9 tokens/s, Avg generation throughput: 120.8 tokens/s, Running: 46 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54146 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:02:58 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:03:03 api_server.py:155] waiting for model...
INFO 05-12 13:03:03 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:03:03 metrics.py:276] Avg prompt throughput: 3.7 tokens/s, Avg generation throughput: 16.7 tokens/s, Running: 46 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%
INFO:     ::1:53950 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:03:04 api_server.py:155] waiting for model...
INFO 05-12 13:03:04 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54638 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:03:04 api_server.py:155] waiting for model...
INFO 05-12 13:03:04 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54112 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:03:04 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:03:09 api_server.py:155] waiting for model...
INFO 05-12 13:03:09 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:03:09 metrics.py:276] Avg prompt throughput: 24.2 tokens/s, Avg generation throughput: 103.4 tokens/s, Running: 46 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54030 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:03:09 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:03:14 api_server.py:155] waiting for model...
INFO 05-12 13:03:14 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:03:14 metrics.py:276] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 70.5 tokens/s, Running: 46 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54084 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:03:14 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:03:20 api_server.py:155] waiting for model...
INFO 05-12 13:03:20 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:03:20 metrics.py:276] Avg prompt throughput: 3.7 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 46 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%
INFO:     ::1:53936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:03:20 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:03:25 api_server.py:155] waiting for model...
INFO 05-12 13:03:25 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:03:25 metrics.py:276] Avg prompt throughput: 8.9 tokens/s, Avg generation throughput: 56.2 tokens/s, Running: 46 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54020 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:03:25 api_server.py:155] waiting for model...
INFO 05-12 13:03:25 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54378 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:03:25 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:03:30 api_server.py:155] waiting for model...
INFO 05-12 13:03:30 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:03:30 metrics.py:276] Avg prompt throughput: 4.3 tokens/s, Avg generation throughput: 45.6 tokens/s, Running: 46 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54268 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:03:30 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:03:35 api_server.py:155] waiting for model...
INFO 05-12 13:03:35 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:03:35 metrics.py:276] Avg prompt throughput: 2.7 tokens/s, Avg generation throughput: 8.5 tokens/s, Running: 46 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%
INFO:     ::1:53888 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:03:36 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:03:41 api_server.py:155] waiting for model...
INFO 05-12 13:03:41 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:03:41 metrics.py:276] Avg prompt throughput: 5.6 tokens/s, Avg generation throughput: 36.1 tokens/s, Running: 46 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%
INFO:     ::1:53998 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:03:41 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:03:46 api_server.py:155] waiting for model...
INFO 05-12 13:03:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54092 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:03:46 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:03:51 api_server.py:155] waiting for model...
INFO 05-12 13:03:51 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:03:51 metrics.py:276] Avg prompt throughput: 3.3 tokens/s, Avg generation throughput: 31.6 tokens/s, Running: 46 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%
INFO:     ::1:53990 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54510 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:03:51 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:03:56 api_server.py:155] waiting for model...
INFO 05-12 13:03:56 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54142 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54016 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:03:56 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:04:02 api_server.py:155] waiting for model...
INFO 05-12 13:04:02 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:04:02 metrics.py:276] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 9.0 tokens/s, Running: 44 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%
INFO:     ::1:54104 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:53922 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:04:02 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:04:06 api_server.py:155] waiting for model...
INFO 05-12 13:04:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:04:07 metrics.py:276] Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 61.9 tokens/s, Running: 43 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO:     ::1:54040 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:04:07 api_server.py:155] waiting for model...
INFO 05-12 13:04:07 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54144 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:04:07 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:04:12 api_server.py:155] waiting for model...
INFO 05-12 13:04:12 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:04:12 metrics.py:276] Avg prompt throughput: 39.7 tokens/s, Avg generation throughput: 54.4 tokens/s, Running: 43 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54088 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:04:13 api_server.py:155] waiting for model...
INFO 05-12 13:04:13 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54034 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:04:13 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:04:18 api_server.py:155] waiting for model...
INFO 05-12 13:04:18 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:04:18 metrics.py:276] Avg prompt throughput: 41.0 tokens/s, Avg generation throughput: 125.1 tokens/s, Running: 43 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54148 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54024 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:04:18 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:04:23 api_server.py:155] waiting for model...
INFO 05-12 13:04:23 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:04:23 metrics.py:276] Avg prompt throughput: 10.4 tokens/s, Avg generation throughput: 32.3 tokens/s, Running: 42 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54098 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:04:24 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:04:29 api_server.py:155] waiting for model...
INFO 05-12 13:04:29 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:04:29 metrics.py:276] Avg prompt throughput: 12.5 tokens/s, Avg generation throughput: 186.7 tokens/s, Running: 42 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54038 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:04:29 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:04:35 api_server.py:155] waiting for model...
INFO 05-12 13:04:35 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:04:35 metrics.py:276] Avg prompt throughput: 4.3 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 42 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54150 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:04:35 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:04:40 api_server.py:155] waiting for model...
INFO 05-12 13:04:40 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:04:40 metrics.py:276] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 8.4 tokens/s, Running: 42 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54046 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:53968 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:04:40 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:04:45 api_server.py:155] waiting for model...
INFO 05-12 13:04:45 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:04:45 metrics.py:276] Avg prompt throughput: 3.1 tokens/s, Avg generation throughput: 41.6 tokens/s, Running: 40 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54120 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:04:45 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:04:50 api_server.py:155] waiting for model...
INFO 05-12 13:04:50 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:04:50 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 41 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:53981 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:04:50 api_server.py:155] waiting for model...
INFO 05-12 13:04:50 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54048 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:04:51 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:04:56 api_server.py:155] waiting for model...
INFO 05-12 13:04:56 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:04:56 metrics.py:276] Avg prompt throughput: 24.2 tokens/s, Avg generation throughput: 103.9 tokens/s, Running: 41 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54002 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:04:56 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:05:01 api_server.py:155] waiting for model...
INFO 05-12 13:05:01 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:05:01 metrics.py:276] Avg prompt throughput: 3.3 tokens/s, Avg generation throughput: 24.4 tokens/s, Running: 41 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54192 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:05:01 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:05:06 api_server.py:155] waiting for model...
INFO 05-12 13:05:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:05:06 metrics.py:276] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 56.4 tokens/s, Running: 40 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54184 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:05:06 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:05:11 api_server.py:155] waiting for model...
INFO 05-12 13:05:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:05:11 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 41 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54156 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:05:12 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:05:17 api_server.py:155] waiting for model...
INFO 05-12 13:05:17 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:05:17 metrics.py:276] Avg prompt throughput: 6.8 tokens/s, Avg generation throughput: 24.8 tokens/s, Running: 40 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54160 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:05:17 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:05:22 api_server.py:155] waiting for model...
INFO 05-12 13:05:22 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:05:22 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 41 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54060 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:05:22 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:05:27 api_server.py:155] waiting for model...
INFO 05-12 13:05:27 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54010 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:05:27 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:05:32 api_server.py:155] waiting for model...
INFO 05-12 13:05:32 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:05:32 metrics.py:276] Avg prompt throughput: 5.8 tokens/s, Avg generation throughput: 8.3 tokens/s, Running: 41 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO:     ::1:54118 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:05:32 api_server.py:155] waiting for model...
INFO 05-12 13:05:32 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54194 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:05:32 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:05:37 api_server.py:155] waiting for model...
INFO 05-12 13:05:37 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:05:37 metrics.py:276] Avg prompt throughput: 25.0 tokens/s, Avg generation throughput: 42.6 tokens/s, Running: 42 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO:     ::1:54116 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:05:37 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:05:42 api_server.py:155] waiting for model...
INFO 05-12 13:05:42 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54158 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:05:42 api_server.py:155] waiting for model...
INFO 05-12 13:05:42 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:05:42 metrics.py:276] Avg prompt throughput: 7.8 tokens/s, Avg generation throughput: 25.3 tokens/s, Running: 41 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO:     ::1:54214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:05:42 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:05:48 api_server.py:155] waiting for model...
INFO 05-12 13:05:48 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:05:48 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 41 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO:     ::1:54110 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:05:48 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:05:53 api_server.py:155] waiting for model...
INFO 05-12 13:05:53 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:05:53 metrics.py:276] Avg prompt throughput: 12.5 tokens/s, Avg generation throughput: 50.1 tokens/s, Running: 42 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO:     ::1:53976 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:05:53 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:05:57 api_server.py:155] waiting for model...
INFO 05-12 13:05:57 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:05:58 metrics.py:276] Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 66.9 tokens/s, Running: 42 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO:     ::1:54166 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54182 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:05:58 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:06:03 api_server.py:155] waiting for model...
INFO 05-12 13:06:03 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:06:03 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.4 tokens/s, Running: 40 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:54132 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:06:03 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:06:08 api_server.py:155] waiting for model...
INFO 05-12 13:06:08 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54212 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:06:08 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:06:13 api_server.py:155] waiting for model...
INFO 05-12 13:06:13 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:06:13 metrics.py:276] Avg prompt throughput: 1.5 tokens/s, Avg generation throughput: 11.9 tokens/s, Running: 40 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:54126 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:06:13 api_server.py:155] waiting for model...
INFO 05-12 13:06:13 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54070 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:06:13 api_server.py:155] waiting for model...
INFO 05-12 13:06:13 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54234 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54106 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:06:14 api_server.py:155] waiting for model...
INFO 05-12 13:06:14 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54178 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:06:14 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:06:19 api_server.py:155] waiting for model...
INFO 05-12 13:06:19 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:06:19 metrics.py:276] Avg prompt throughput: 24.8 tokens/s, Avg generation throughput: 69.7 tokens/s, Running: 39 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:54230 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:06:19 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:06:24 api_server.py:155] waiting for model...
INFO 05-12 13:06:24 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54224 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:06:24 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:06:29 api_server.py:155] waiting for model...
INFO 05-12 13:06:29 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:06:29 metrics.py:276] Avg prompt throughput: 5.1 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 39 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:54236 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54164 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:06:29 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:06:34 api_server.py:155] waiting for model...
INFO 05-12 13:06:34 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:06:34 metrics.py:276] Avg prompt throughput: 9.7 tokens/s, Avg generation throughput: 15.9 tokens/s, Running: 39 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:54180 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:06:34 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:06:38 api_server.py:155] waiting for model...
INFO 05-12 13:06:38 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54094 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:06:38 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:06:44 api_server.py:155] waiting for model...
INFO 05-12 13:06:44 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:06:44 metrics.py:276] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 15.3 tokens/s, Running: 38 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54028 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:06:44 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:06:49 api_server.py:155] waiting for model...
INFO 05-12 13:06:49 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:06:49 metrics.py:276] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 38 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54472 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:06:49 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:06:55 api_server.py:155] waiting for model...
INFO 05-12 13:06:55 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:06:55 metrics.py:276] Avg prompt throughput: 75.3 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 38 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:53942 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54246 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:06:55 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:06:59 api_server.py:155] waiting for model...
INFO 05-12 13:06:59 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54200 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:07:00 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:07:05 api_server.py:155] waiting for model...
INFO 05-12 13:07:05 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:07:05 metrics.py:276] Avg prompt throughput: 8.5 tokens/s, Avg generation throughput: 18.4 tokens/s, Running: 37 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54222 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:07:05 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:07:10 api_server.py:155] waiting for model...
INFO 05-12 13:07:10 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:07:10 metrics.py:276] Avg prompt throughput: 6.4 tokens/s, Avg generation throughput: 35.9 tokens/s, Running: 37 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54128 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:07:11 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:07:16 api_server.py:155] waiting for model...
INFO 05-12 13:07:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:07:16 metrics.py:276] Avg prompt throughput: 6.1 tokens/s, Avg generation throughput: 64.7 tokens/s, Running: 37 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54244 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:07:16 api_server.py:155] waiting for model...
INFO 05-12 13:07:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54134 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:07:17 api_server.py:155] waiting for model...
INFO 05-12 13:07:17 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54266 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:07:17 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:07:22 api_server.py:155] waiting for model...
INFO 05-12 13:07:22 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:07:22 metrics.py:276] Avg prompt throughput: 19.2 tokens/s, Avg generation throughput: 133.1 tokens/s, Running: 37 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:54208 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:07:22 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:07:27 api_server.py:155] waiting for model...
INFO 05-12 13:07:27 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:07:27 metrics.py:276] Avg prompt throughput: 138.6 tokens/s, Avg generation throughput: 30.0 tokens/s, Running: 38 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO:     ::1:54136 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:07:27 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:07:33 api_server.py:155] waiting for model...
INFO 05-12 13:07:33 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:07:33 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 52.4 tokens/s, Running: 37 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO:     ::1:53920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:07:33 api_server.py:155] waiting for model...
INFO 05-12 13:07:33 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54154 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:07:33 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:07:38 api_server.py:155] waiting for model...
INFO 05-12 13:07:38 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:07:38 metrics.py:276] Avg prompt throughput: 7.5 tokens/s, Avg generation throughput: 80.5 tokens/s, Running: 37 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO:     ::1:54226 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:07:38 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:07:43 api_server.py:155] waiting for model...
INFO 05-12 13:07:43 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:07:43 metrics.py:276] Avg prompt throughput: 2.1 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 37 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO:     ::1:54186 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:07:43 api_server.py:155] waiting for model...
INFO 05-12 13:07:43 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54086 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54278 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:07:44 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:07:49 api_server.py:155] waiting for model...
INFO 05-12 13:07:49 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:07:49 metrics.py:276] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 68.6 tokens/s, Running: 36 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:54240 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:07:49 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:07:54 api_server.py:155] waiting for model...
INFO 05-12 13:07:54 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:07:54 metrics.py:276] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 103.8 tokens/s, Running: 36 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO:     ::1:54238 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:07:54 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:08:00 api_server.py:155] waiting for model...
INFO 05-12 13:08:00 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:08:00 metrics.py:276] Avg prompt throughput: 5.4 tokens/s, Avg generation throughput: 33.0 tokens/s, Running: 34 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:54190 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54054 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:08:00 api_server.py:155] waiting for model...
INFO 05-12 13:08:00 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54172 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:08:00 api_server.py:155] waiting for model...
INFO 05-12 13:08:00 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54210 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:08:00 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:08:05 api_server.py:155] waiting for model...
INFO 05-12 13:08:05 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:08:05 metrics.py:276] Avg prompt throughput: 34.9 tokens/s, Avg generation throughput: 28.6 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:54284 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:08:05 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:08:10 api_server.py:155] waiting for model...
INFO 05-12 13:08:10 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:08:10 metrics.py:276] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 101.6 tokens/s, Running: 34 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO:     ::1:54258 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:08:10 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:08:15 api_server.py:155] waiting for model...
INFO 05-12 13:08:15 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:08:15 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO:     ::1:54302 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:08:15 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:08:21 api_server.py:155] waiting for model...
INFO 05-12 13:08:21 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:08:21 metrics.py:276] Avg prompt throughput: 7.4 tokens/s, Avg generation throughput: 31.3 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO:     ::1:54196 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:08:21 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:08:26 api_server.py:155] waiting for model...
INFO 05-12 13:08:26 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:08:26 metrics.py:276] Avg prompt throughput: 42.2 tokens/s, Avg generation throughput: 54.4 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%
INFO:     ::1:54254 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:08:27 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:08:32 api_server.py:155] waiting for model...
INFO 05-12 13:08:32 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:08:32 metrics.py:276] Avg prompt throughput: 101.3 tokens/s, Avg generation throughput: 60.3 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54270 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:08:32 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:08:37 api_server.py:155] waiting for model...
INFO 05-12 13:08:37 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:08:37 metrics.py:276] Avg prompt throughput: 3.1 tokens/s, Avg generation throughput: 53.5 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54252 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:08:37 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:08:42 api_server.py:155] waiting for model...
INFO 05-12 13:08:42 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:08:42 metrics.py:276] Avg prompt throughput: 9.0 tokens/s, Avg generation throughput: 35.1 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54256 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:08:43 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:08:48 api_server.py:155] waiting for model...
INFO 05-12 13:08:48 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:08:48 metrics.py:276] Avg prompt throughput: 3.7 tokens/s, Avg generation throughput: 26.8 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54312 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:08:48 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:08:53 api_server.py:155] waiting for model...
INFO 05-12 13:08:53 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:08:53 metrics.py:276] Avg prompt throughput: 21.7 tokens/s, Avg generation throughput: 26.1 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54066 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:08:54 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:08:58 api_server.py:155] waiting for model...
INFO 05-12 13:08:58 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:08:58 metrics.py:276] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 75.3 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54242 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:08:58 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:09:04 api_server.py:155] waiting for model...
INFO 05-12 13:09:04 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:09:04 metrics.py:276] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 18.7 tokens/s, Running: 34 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54096 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:09:04 api_server.py:155] waiting for model...
INFO 05-12 13:09:04 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54218 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:09:04 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:09:09 api_server.py:155] waiting for model...
INFO 05-12 13:09:09 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:09:09 metrics.py:276] Avg prompt throughput: 11.5 tokens/s, Avg generation throughput: 21.8 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54090 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:09:09 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:09:14 api_server.py:155] waiting for model...
INFO 05-12 13:09:14 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:09:14 metrics.py:276] Avg prompt throughput: 6.1 tokens/s, Avg generation throughput: 19.8 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%
INFO:     ::1:54282 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:09:15 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:09:19 api_server.py:155] waiting for model...
INFO 05-12 13:09:19 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54288 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:09:19 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:09:24 api_server.py:155] waiting for model...
INFO 05-12 13:09:24 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:09:24 metrics.py:276] Avg prompt throughput: 3.7 tokens/s, Avg generation throughput: 7.6 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO:     ::1:54080 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:09:25 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:09:30 api_server.py:155] waiting for model...
INFO 05-12 13:09:30 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:09:30 metrics.py:276] Avg prompt throughput: 2.9 tokens/s, Avg generation throughput: 133.4 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54292 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:09:30 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:09:36 api_server.py:155] waiting for model...
INFO 05-12 13:09:36 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:09:36 metrics.py:276] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 19.0 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54198 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:09:36 api_server.py:155] waiting for model...
INFO 05-12 13:09:36 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54298 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:09:37 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:09:42 api_server.py:155] waiting for model...
INFO 05-12 13:09:42 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:09:42 metrics.py:276] Avg prompt throughput: 87.6 tokens/s, Avg generation throughput: 84.2 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54280 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:09:42 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:09:47 api_server.py:155] waiting for model...
INFO 05-12 13:09:47 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:09:47 metrics.py:276] Avg prompt throughput: 3.7 tokens/s, Avg generation throughput: 7.1 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54100 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:09:47 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:09:52 api_server.py:155] waiting for model...
INFO 05-12 13:09:52 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:09:52 metrics.py:276] Avg prompt throughput: 5.4 tokens/s, Avg generation throughput: 34.7 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54260 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:09:52 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:09:57 api_server.py:155] waiting for model...
INFO 05-12 13:09:57 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:09:57 metrics.py:276] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 33.7 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54274 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:09:57 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:10:02 api_server.py:155] waiting for model...
INFO 05-12 13:10:02 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54360 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:10:02 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:10:07 api_server.py:155] waiting for model...
INFO 05-12 13:10:07 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:10:07 metrics.py:276] Avg prompt throughput: 4.5 tokens/s, Avg generation throughput: 25.6 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54362 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:10:07 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:10:13 api_server.py:155] waiting for model...
INFO 05-12 13:10:13 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:10:13 metrics.py:276] Avg prompt throughput: 2.7 tokens/s, Avg generation throughput: 38.7 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54248 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:10:13 api_server.py:155] waiting for model...
INFO 05-12 13:10:13 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54321 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:10:14 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:10:18 api_server.py:155] waiting for model...
INFO 05-12 13:10:18 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:10:18 metrics.py:276] Avg prompt throughput: 7.2 tokens/s, Avg generation throughput: 141.7 tokens/s, Running: 34 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54333 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:10:18 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:10:23 api_server.py:155] waiting for model...
INFO 05-12 13:10:23 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:10:23 metrics.py:276] Avg prompt throughput: 8.6 tokens/s, Avg generation throughput: 36.4 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54204 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:10:23 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:10:28 api_server.py:155] waiting for model...
INFO 05-12 13:10:28 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54326 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:10:28 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:10:34 api_server.py:155] waiting for model...
INFO 05-12 13:10:34 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:10:34 metrics.py:276] Avg prompt throughput: 29.4 tokens/s, Avg generation throughput: 24.3 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54342 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:10:34 api_server.py:155] waiting for model...
INFO 05-12 13:10:34 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54250 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:10:34 api_server.py:155] waiting for model...
INFO 05-12 13:10:34 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54370 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:10:35 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:10:40 api_server.py:155] waiting for model...
INFO 05-12 13:10:40 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:10:40 metrics.py:276] Avg prompt throughput: 16.5 tokens/s, Avg generation throughput: 143.6 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54206 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:10:40 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:10:45 api_server.py:155] waiting for model...
INFO 05-12 13:10:45 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:10:46 metrics.py:276] Avg prompt throughput: 6.3 tokens/s, Avg generation throughput: 25.5 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54202 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:10:46 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:10:51 api_server.py:155] waiting for model...
INFO 05-12 13:10:51 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:10:51 metrics.py:276] Avg prompt throughput: 2.7 tokens/s, Avg generation throughput: 70.4 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54364 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:10:51 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:10:56 api_server.py:155] waiting for model...
INFO 05-12 13:10:56 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:10:56 metrics.py:276] Avg prompt throughput: 41.6 tokens/s, Avg generation throughput: 13.4 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54317 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:10:56 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:11:01 api_server.py:155] waiting for model...
INFO 05-12 13:11:01 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:11:01 metrics.py:276] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 33.4 tokens/s, Running: 34 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54322 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:11:01 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:11:06 api_server.py:155] waiting for model...
INFO 05-12 13:11:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:11:06 metrics.py:276] Avg prompt throughput: 8.8 tokens/s, Avg generation throughput: 36.4 tokens/s, Running: 36 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54174 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:11:07 api_server.py:155] waiting for model...
INFO 05-12 13:11:07 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54372 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:11:07 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:11:12 api_server.py:155] waiting for model...
INFO 05-12 13:11:12 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:11:12 metrics.py:276] Avg prompt throughput: 6.7 tokens/s, Avg generation throughput: 46.5 tokens/s, Running: 34 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54376 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:11:12 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:11:17 api_server.py:155] waiting for model...
INFO 05-12 13:11:17 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:11:17 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54286 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54338 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:11:18 api_server.py:155] waiting for model...
INFO 05-12 13:11:18 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54382 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:11:18 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:11:23 api_server.py:155] waiting for model...
INFO 05-12 13:11:23 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:11:23 metrics.py:276] Avg prompt throughput: 15.6 tokens/s, Avg generation throughput: 80.7 tokens/s, Running: 33 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%
INFO:     ::1:54220 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:11:23 api_server.py:155] waiting for model...
INFO 05-12 13:11:23 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54300 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:11:23 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:11:29 api_server.py:155] waiting for model...
INFO 05-12 13:11:29 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:11:29 metrics.py:276] Avg prompt throughput: 7.8 tokens/s, Avg generation throughput: 75.8 tokens/s, Running: 34 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54308 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:11:29 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:11:34 api_server.py:155] waiting for model...
INFO 05-12 13:11:34 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:11:34 metrics.py:276] Avg prompt throughput: 5.4 tokens/s, Avg generation throughput: 34.5 tokens/s, Running: 34 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO:     ::1:54319 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:11:34 api_server.py:155] waiting for model...
INFO 05-12 13:11:34 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54374 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:11:34 api_server.py:155] waiting for model...
INFO 05-12 13:11:34 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54416 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54324 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:11:34 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:11:39 api_server.py:155] waiting for model...
INFO 05-12 13:11:39 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:11:39 metrics.py:276] Avg prompt throughput: 14.2 tokens/s, Avg generation throughput: 41.7 tokens/s, Running: 33 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:54386 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54394 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:11:39 api_server.py:155] waiting for model...
INFO 05-12 13:11:39 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54262 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:11:39 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:11:45 api_server.py:155] waiting for model...
INFO 05-12 13:11:45 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:11:45 metrics.py:276] Avg prompt throughput: 33.7 tokens/s, Avg generation throughput: 44.9 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:54406 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:11:45 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:11:50 api_server.py:155] waiting for model...
INFO 05-12 13:11:50 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:11:50 metrics.py:276] Avg prompt throughput: 6.6 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 33 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:54414 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:11:50 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:11:55 api_server.py:155] waiting for model...
INFO 05-12 13:11:55 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:11:55 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.4 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:54412 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:11:55 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:12:00 api_server.py:155] waiting for model...
INFO 05-12 13:12:00 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:12:00 metrics.py:276] Avg prompt throughput: 21.5 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 33 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:54426 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:12:00 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:12:05 api_server.py:155] waiting for model...
INFO 05-12 13:12:05 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54380 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:12:05 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:12:10 api_server.py:155] waiting for model...
INFO 05-12 13:12:10 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:12:10 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.0 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:54314 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:12:10 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:12:15 api_server.py:155] waiting for model...
INFO 05-12 13:12:15 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:12:15 metrics.py:276] Avg prompt throughput: 25.9 tokens/s, Avg generation throughput: 32.6 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:54337 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:12:15 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:12:21 api_server.py:155] waiting for model...
INFO 05-12 13:12:21 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:12:21 metrics.py:276] Avg prompt throughput: 3.5 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:54328 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:12:21 api_server.py:155] waiting for model...
INFO 05-12 13:12:21 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54404 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:12:21 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:12:26 api_server.py:155] waiting for model...
INFO 05-12 13:12:26 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:12:26 metrics.py:276] Avg prompt throughput: 18.2 tokens/s, Avg generation throughput: 32.7 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:54176 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:12:26 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:12:31 api_server.py:155] waiting for model...
INFO 05-12 13:12:31 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:12:31 metrics.py:276] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 12.3 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54388 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:12:31 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:12:36 api_server.py:155] waiting for model...
INFO 05-12 13:12:36 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:12:36 metrics.py:276] Avg prompt throughput: 20.3 tokens/s, Avg generation throughput: 39.5 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54367 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:12:36 api_server.py:155] waiting for model...
INFO 05-12 13:12:36 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54440 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:12:36 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:12:41 api_server.py:155] waiting for model...
INFO 05-12 13:12:41 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:12:41 metrics.py:276] Avg prompt throughput: 17.1 tokens/s, Avg generation throughput: 39.4 tokens/s, Running: 33 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54424 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:12:41 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:12:46 api_server.py:155] waiting for model...
INFO 05-12 13:12:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:12:46 metrics.py:276] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 19.8 tokens/s, Running: 33 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54398 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:12:46 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:12:52 api_server.py:155] waiting for model...
INFO 05-12 13:12:52 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:12:52 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.9 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54410 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:12:52 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:12:57 api_server.py:155] waiting for model...
INFO 05-12 13:12:57 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:12:57 metrics.py:276] Avg prompt throughput: 4.9 tokens/s, Avg generation throughput: 30.8 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54460 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:12:57 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:13:03 api_server.py:155] waiting for model...
INFO 05-12 13:13:03 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:13:03 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54348 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:03 api_server.py:155] waiting for model...
INFO 05-12 13:13:03 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54466 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:03 api_server.py:155] waiting for model...
INFO 05-12 13:13:03 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54402 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:03 api_server.py:155] waiting for model...
INFO 05-12 13:13:03 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54474 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:04 api_server.py:155] waiting for model...
INFO 05-12 13:13:04 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54331 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:04 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:13:09 api_server.py:155] waiting for model...
INFO 05-12 13:13:09 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:13:09 metrics.py:276] Avg prompt throughput: 47.9 tokens/s, Avg generation throughput: 140.0 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:54408 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:10 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:13:15 api_server.py:155] waiting for model...
INFO 05-12 13:13:15 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:13:15 metrics.py:276] Avg prompt throughput: 3.5 tokens/s, Avg generation throughput: 129.9 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO:     ::1:54418 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:15 api_server.py:155] waiting for model...
INFO 05-12 13:13:15 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54486 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:16 api_server.py:155] waiting for model...
INFO 05-12 13:13:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54436 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54484 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:16 api_server.py:155] waiting for model...
INFO 05-12 13:13:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54446 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:17 api_server.py:155] waiting for model...
INFO 05-12 13:13:17 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54276 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:17 api_server.py:155] waiting for model...
INFO 05-12 13:13:17 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54368 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:17 api_server.py:155] waiting for model...
INFO 05-12 13:13:17 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54457 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:18 api_server.py:155] waiting for model...
INFO 05-12 13:13:18 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54478 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:18 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:13:23 api_server.py:155] waiting for model...
INFO 05-12 13:13:23 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:13:23 metrics.py:276] Avg prompt throughput: 29.6 tokens/s, Avg generation throughput: 219.6 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%
INFO:     ::1:54494 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:23 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:13:28 api_server.py:155] waiting for model...
INFO 05-12 13:13:28 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:13:28 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.6%, CPU KV cache usage: 0.0%
INFO:     ::1:54335 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:29 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:13:34 api_server.py:155] waiting for model...
INFO 05-12 13:13:34 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:13:34 metrics.py:276] Avg prompt throughput: 6.8 tokens/s, Avg generation throughput: 25.2 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.5%, CPU KV cache usage: 0.0%
INFO:     ::1:54306 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:34 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:13:39 api_server.py:155] waiting for model...
INFO 05-12 13:13:39 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:13:39 metrics.py:276] Avg prompt throughput: 4.2 tokens/s, Avg generation throughput: 61.0 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO:     ::1:54272 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:39 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:13:44 api_server.py:155] waiting for model...
INFO 05-12 13:13:44 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:13:44 metrics.py:276] Avg prompt throughput: 3.3 tokens/s, Avg generation throughput: 23.3 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54442 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:44 api_server.py:155] waiting for model...
INFO 05-12 13:13:44 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54468 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:45 api_server.py:155] waiting for model...
INFO 05-12 13:13:45 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54500 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:45 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:13:50 api_server.py:155] waiting for model...
INFO 05-12 13:13:50 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:13:50 metrics.py:276] Avg prompt throughput: 12.3 tokens/s, Avg generation throughput: 107.2 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:54504 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:50 api_server.py:155] waiting for model...
INFO 05-12 13:13:50 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54482 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:50 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:13:56 api_server.py:155] waiting for model...
INFO 05-12 13:13:56 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:13:56 metrics.py:276] Avg prompt throughput: 12.1 tokens/s, Avg generation throughput: 49.4 tokens/s, Running: 29 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54490 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54384 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:13:56 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:14:01 api_server.py:155] waiting for model...
INFO 05-12 13:14:01 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:14:01 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54291 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:14:01 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:14:06 api_server.py:155] waiting for model...
INFO 05-12 13:14:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:14:06 metrics.py:276] Avg prompt throughput: 7.0 tokens/s, Avg generation throughput: 24.3 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54228 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:14:06 api_server.py:155] waiting for model...
INFO 05-12 13:14:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54526 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54508 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:14:06 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:14:11 api_server.py:155] waiting for model...
INFO 05-12 13:14:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:14:11 metrics.py:276] Avg prompt throughput: 13.0 tokens/s, Avg generation throughput: 29.3 tokens/s, Running: 29 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54524 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:14:11 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:14:16 api_server.py:155] waiting for model...
INFO 05-12 13:14:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54358 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:14:16 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:14:21 api_server.py:155] waiting for model...
INFO 05-12 13:14:21 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:14:21 metrics.py:276] Avg prompt throughput: 4.9 tokens/s, Avg generation throughput: 18.5 tokens/s, Running: 29 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54458 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:14:21 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:14:27 api_server.py:155] waiting for model...
INFO 05-12 13:14:27 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:14:27 metrics.py:276] Avg prompt throughput: 6.2 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 29 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54432 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54530 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:14:27 api_server.py:155] waiting for model...
INFO 05-12 13:14:27 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54340 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:14:27 api_server.py:155] waiting for model...
INFO 05-12 13:14:27 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:14:28 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:14:32 api_server.py:155] waiting for model...
INFO 05-12 13:14:32 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:14:32 metrics.py:276] Avg prompt throughput: 7.9 tokens/s, Avg generation throughput: 122.9 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%
INFO:     ::1:54492 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:14:33 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:14:39 api_server.py:155] waiting for model...
INFO 05-12 13:14:39 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:14:39 metrics.py:276] Avg prompt throughput: 5.1 tokens/s, Avg generation throughput: 83.7 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54550 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:14:39 api_server.py:155] waiting for model...
INFO 05-12 13:14:39 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54400 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:14:39 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:14:44 api_server.py:155] waiting for model...
INFO 05-12 13:14:44 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:14:44 metrics.py:276] Avg prompt throughput: 12.5 tokens/s, Avg generation throughput: 55.7 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54544 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:14:44 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:14:49 api_server.py:155] waiting for model...
INFO 05-12 13:14:49 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:14:49 metrics.py:276] Avg prompt throughput: 13.0 tokens/s, Avg generation throughput: 10.6 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54552 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:14:50 api_server.py:155] waiting for model...
INFO 05-12 13:14:50 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54390 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:14:50 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:14:55 api_server.py:155] waiting for model...
INFO 05-12 13:14:55 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:14:55 metrics.py:276] Avg prompt throughput: 13.5 tokens/s, Avg generation throughput: 57.4 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54528 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:14:55 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:15:00 api_server.py:155] waiting for model...
INFO 05-12 13:15:00 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:15:00 metrics.py:276] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 36.2 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54512 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:15:01 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:15:06 api_server.py:155] waiting for model...
INFO 05-12 13:15:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:15:06 metrics.py:276] Avg prompt throughput: 2.9 tokens/s, Avg generation throughput: 62.0 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54488 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:15:06 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:15:11 api_server.py:155] waiting for model...
INFO 05-12 13:15:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:15:11 metrics.py:276] Avg prompt throughput: 3.9 tokens/s, Avg generation throughput: 46.1 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54571 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:15:12 api_server.py:155] waiting for model...
INFO 05-12 13:15:12 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54538 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:15:12 api_server.py:155] waiting for model...
INFO 05-12 13:15:12 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54462 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:15:12 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:15:17 api_server.py:155] waiting for model...
INFO 05-12 13:15:17 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:15:17 metrics.py:276] Avg prompt throughput: 20.2 tokens/s, Avg generation throughput: 131.7 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54568 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:15:18 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:15:23 api_server.py:155] waiting for model...
INFO 05-12 13:15:23 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:15:23 metrics.py:276] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 68.0 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54454 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:15:23 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:15:28 api_server.py:155] waiting for model...
INFO 05-12 13:15:28 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54534 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:15:28 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:15:33 api_server.py:155] waiting for model...
INFO 05-12 13:15:33 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:15:33 metrics.py:276] Avg prompt throughput: 5.4 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54514 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:15:33 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:15:38 api_server.py:155] waiting for model...
INFO 05-12 13:15:38 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:15:38 metrics.py:276] Avg prompt throughput: 19.2 tokens/s, Avg generation throughput: 23.2 tokens/s, Running: 29 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54496 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:15:38 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:15:43 api_server.py:155] waiting for model...
INFO 05-12 13:15:43 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:15:43 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 21.9 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54572 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:15:44 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:15:49 api_server.py:155] waiting for model...
INFO 05-12 13:15:49 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:15:49 metrics.py:276] Avg prompt throughput: 3.5 tokens/s, Avg generation throughput: 64.8 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54421 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:15:49 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:15:54 api_server.py:155] waiting for model...
INFO 05-12 13:15:54 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:15:54 metrics.py:276] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 16.9 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54556 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:15:55 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:16:00 api_server.py:155] waiting for model...
INFO 05-12 13:16:00 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:16:00 metrics.py:276] Avg prompt throughput: 4.1 tokens/s, Avg generation throughput: 156.8 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54596 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:16:00 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:16:06 api_server.py:155] waiting for model...
INFO 05-12 13:16:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:16:06 metrics.py:276] Avg prompt throughput: 3.1 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:54604 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:16:06 api_server.py:155] waiting for model...
INFO 05-12 13:16:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54502 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:16:06 api_server.py:155] waiting for model...
INFO 05-12 13:16:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54584 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:16:07 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:16:12 api_server.py:155] waiting for model...
INFO 05-12 13:16:12 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:16:12 metrics.py:276] Avg prompt throughput: 16.3 tokens/s, Avg generation throughput: 104.5 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:54558 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:16:12 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:16:17 api_server.py:155] waiting for model...
INFO 05-12 13:16:17 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:16:17 metrics.py:276] Avg prompt throughput: 2.9 tokens/s, Avg generation throughput: 21.4 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:54564 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:16:17 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:16:22 api_server.py:155] waiting for model...
INFO 05-12 13:16:22 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54393 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:16:22 api_server.py:155] waiting for model...
INFO 05-12 13:16:22 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54607 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:16:22 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:16:27 api_server.py:155] waiting for model...
INFO 05-12 13:16:27 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:16:27 metrics.py:276] Avg prompt throughput: 3.8 tokens/s, Avg generation throughput: 19.7 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54618 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54602 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:16:28 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:16:32 api_server.py:155] waiting for model...
INFO 05-12 13:16:32 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:16:32 metrics.py:276] Avg prompt throughput: 7.7 tokens/s, Avg generation throughput: 28.6 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54598 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:16:33 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:16:38 api_server.py:155] waiting for model...
INFO 05-12 13:16:38 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:16:38 metrics.py:276] Avg prompt throughput: 8.5 tokens/s, Avg generation throughput: 11.0 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54499 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:16:38 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:16:43 api_server.py:155] waiting for model...
INFO 05-12 13:16:43 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:16:43 metrics.py:276] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 5.3 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54616 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:16:43 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:16:49 api_server.py:155] waiting for model...
INFO 05-12 13:16:49 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:16:49 metrics.py:276] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 5.2 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54632 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:16:49 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:16:54 api_server.py:155] waiting for model...
INFO 05-12 13:16:54 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:16:54 metrics.py:276] Avg prompt throughput: 4.2 tokens/s, Avg generation throughput: 16.4 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:54628 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:16:54 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:16:59 api_server.py:155] waiting for model...
INFO 05-12 13:16:59 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:16:59 metrics.py:276] Avg prompt throughput: 87.6 tokens/s, Avg generation throughput: 22.5 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:54476 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:16:59 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:17:04 api_server.py:155] waiting for model...
INFO 05-12 13:17:04 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:17:04 metrics.py:276] Avg prompt throughput: 45.4 tokens/s, Avg generation throughput: 4.9 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54626 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:17:05 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:17:09 api_server.py:155] waiting for model...
INFO 05-12 13:17:09 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:17:09 metrics.py:276] Avg prompt throughput: 5.4 tokens/s, Avg generation throughput: 32.9 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54582 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:17:10 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:17:15 api_server.py:155] waiting for model...
INFO 05-12 13:17:15 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:17:15 metrics.py:276] Avg prompt throughput: 2.7 tokens/s, Avg generation throughput: 15.1 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:54532 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:17:16 api_server.py:155] waiting for model...
INFO 05-12 13:17:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54574 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:17:16 api_server.py:155] waiting for model...
INFO 05-12 13:17:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54610 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:17:16 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:17:21 api_server.py:155] waiting for model...
INFO 05-12 13:17:21 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:17:21 metrics.py:276] Avg prompt throughput: 107.8 tokens/s, Avg generation throughput: 141.3 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO:     ::1:54542 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:17:21 api_server.py:155] waiting for model...
INFO 05-12 13:17:21 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54540 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:17:22 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:17:26 api_server.py:155] waiting for model...
INFO 05-12 13:17:26 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:17:26 metrics.py:276] Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 58.5 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:54592 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:17:26 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:17:32 api_server.py:155] waiting for model...
INFO 05-12 13:17:32 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:17:32 metrics.py:276] Avg prompt throughput: 3.8 tokens/s, Avg generation throughput: 10.3 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:54614 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:17:32 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:17:37 api_server.py:155] waiting for model...
INFO 05-12 13:17:37 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:17:37 metrics.py:276] Avg prompt throughput: 5.1 tokens/s, Avg generation throughput: 16.1 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:54656 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:17:37 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:17:42 api_server.py:155] waiting for model...
INFO 05-12 13:17:42 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:17:42 metrics.py:276] Avg prompt throughput: 5.1 tokens/s, Avg generation throughput: 44.0 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO:     ::1:54654 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:17:42 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:17:47 api_server.py:155] waiting for model...
INFO 05-12 13:17:47 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:17:47 metrics.py:276] Avg prompt throughput: 10.0 tokens/s, Avg generation throughput: 26.4 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%
INFO:     ::1:54658 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:17:47 api_server.py:155] waiting for model...
INFO 05-12 13:17:47 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54520 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54650 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:17:48 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:17:53 api_server.py:155] waiting for model...
INFO 05-12 13:17:53 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:17:53 metrics.py:276] Avg prompt throughput: 8.6 tokens/s, Avg generation throughput: 10.2 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:54662 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:17:54 api_server.py:155] waiting for model...
INFO 05-12 13:17:54 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54438 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:17:54 api_server.py:155] waiting for model...
INFO 05-12 13:17:54 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54668 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:17:54 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:17:59 api_server.py:155] waiting for model...
INFO 05-12 13:17:59 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:17:59 metrics.py:276] Avg prompt throughput: 7.6 tokens/s, Avg generation throughput: 116.3 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%
INFO:     ::1:54600 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:17:59 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:18:04 api_server.py:155] waiting for model...
INFO 05-12 13:18:04 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:18:04 metrics.py:276] Avg prompt throughput: 2.9 tokens/s, Avg generation throughput: 5.1 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:54444 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:18:05 api_server.py:155] waiting for model...
INFO 05-12 13:18:05 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54428 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:18:05 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:18:10 api_server.py:155] waiting for model...
INFO 05-12 13:18:10 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:18:10 metrics.py:276] Avg prompt throughput: 6.9 tokens/s, Avg generation throughput: 77.8 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54620 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54674 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:18:10 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:18:16 api_server.py:155] waiting for model...
INFO 05-12 13:18:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:18:16 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54648 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:18:16 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:18:21 api_server.py:155] waiting for model...
INFO 05-12 13:18:21 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:18:21 metrics.py:276] Avg prompt throughput: 31.7 tokens/s, Avg generation throughput: 20.3 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54660 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54640 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:18:21 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:18:26 api_server.py:155] waiting for model...
INFO 05-12 13:18:26 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:18:26 metrics.py:276] Avg prompt throughput: 7.1 tokens/s, Avg generation throughput: 4.8 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:54594 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:18:27 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:18:32 api_server.py:155] waiting for model...
INFO 05-12 13:18:32 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:18:32 metrics.py:276] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 35.5 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54624 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:18:32 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:18:37 api_server.py:155] waiting for model...
INFO 05-12 13:18:37 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:18:37 metrics.py:276] Avg prompt throughput: 2.7 tokens/s, Avg generation throughput: 19.6 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:54580 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:18:37 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:18:42 api_server.py:155] waiting for model...
INFO 05-12 13:18:42 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54578 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54670 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54672 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:18:42 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:18:47 api_server.py:155] waiting for model...
INFO 05-12 13:18:47 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:18:47 metrics.py:276] Avg prompt throughput: 3.7 tokens/s, Avg generation throughput: 12.4 tokens/s, Running: 22 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:54608 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:18:48 api_server.py:155] waiting for model...
INFO 05-12 13:18:48 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:54450 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:18:48 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:18:53 api_server.py:155] waiting for model...
INFO 05-12 13:18:53 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:18:53 metrics.py:276] Avg prompt throughput: 7.9 tokens/s, Avg generation throughput: 75.7 tokens/s, Running: 22 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:54622 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:18:53 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:18:58 api_server.py:155] waiting for model...
INFO 05-12 13:18:58 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:18:58 metrics.py:276] Avg prompt throughput: 9.3 tokens/s, Avg generation throughput: 16.7 tokens/s, Running: 22 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:54634 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54680 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54678 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54642 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54562 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54690 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54576 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54696 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54566 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54652 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54666 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54686 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54688 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:19:03 metrics.py:276] Avg prompt throughput: 4.2 tokens/s, Avg generation throughput: 428.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54664 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54644 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54676 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54698 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:19:08 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 217.3 tokens/s, Running: 6 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:54636 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54692 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54646 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54682 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54700 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:54702 - "POST /v1/completions HTTP/1.1" 200 OK
Results written to .artifact/benchmarks/results/2df53e37-b256-4052-b31a-2ad94475fd0d.jsonl
Killing process 108936...
/var/spool/slurmd/job3671406/slurm_script: line 42: 108904 Killed                  apptainer run --nv --home /mnt/petrelfs/huqinghao/xzyao/:/home/xiayao --bind /mnt/petrelfs/huqinghao/xzyao/code/vllm:/vllm --bind /mnt/petrelfs/huqinghao/xzyao/code/triteia:/triteia --env PYTHONPATH=/vllm:/triteia --env CUDA_LAUNCH_BLOCKING=1 --env TVM_TARGET=nvidia/nvidia-a100 --env BITBLAS_TARGET=nvidia/nvidia-a100 --env RAY_DEDUP_LOGS=0 --workdir /vllm /mnt/petrelfs/huqinghao/xzyao/images/deltaserve.sif python3 /vllm/vllm/entrypoints/openai/api_server.py --model /vllm/.idea/full_models/Llama-2-13b-hf --disable-log-requests --gpu-memory-utilization 0.85 --swap-modules delta-1=/vllm/.idea/full_models/Llama-2-13b-hf-1 delta-2=/vllm/.idea/full_models/Llama-2-13b-hf-2 delta-3=/vllm/.idea/full_models/Llama-2-13b-hf-3 delta-4=/vllm/.idea/full_models/Llama-2-13b-hf-4 delta-5=/vllm/.idea/full_models/Llama-2-13b-hf-5 delta-6=/vllm/.idea/full_models/Llama-2-13b-hf-6 delta-7=/vllm/.idea/full_models/Llama-2-13b-hf-7 delta-8=/vllm/.idea/full_models/Llama-2-13b-hf-8 --tensor-parallel-size 4 --enforce-eager --max-deltas 0
