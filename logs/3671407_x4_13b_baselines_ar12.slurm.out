/mnt/hwfile/huqinghao/miniconda3/bin/python
2024-05-12 13:19:27,389 - INFO - Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2024-05-12 13:19:27,390 - INFO - Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-05-12 13:19:27,390 - INFO - NumExpr defaulting to 8 threads.
2024-05-12 13:19:28,298	INFO scripts.py:1182 -- Did not find any active Ray processes.
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49c8e0>: Failed to establish a new connection: [Errno 111] Connection refused'))

==========
== CUDA ==
==========

CUDA Version 12.1.0

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

*************************
** DEPRECATION NOTICE! **
*************************
THIS IMAGE IS DEPRECATED and is scheduled for DELETION.
    https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/support-policy.md

Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ce80>: Failed to establish a new connection: [Errno 111] Connection refused'))
/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:160: UserWarning: Field "model_name_or_path" has conflict with protected namespace "model_".

You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.
  warnings.warn(
INFO 05-12 13:19:46 api_server.py:219] vLLM API server version 0.3.3
INFO 05-12 13:19:46 api_server.py:220] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=[], delta_modules=[], swap_modules=[SwapModule(name='delta-1', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-1'), SwapModule(name='delta-2', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-2'), SwapModule(name='delta-3', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-3'), SwapModule(name='delta-4', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-4'), SwapModule(name='delta-5', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-5'), SwapModule(name='delta-6', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-6'), SwapModule(name='delta-7', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-7'), SwapModule(name='delta-8', local_path='/vllm/.idea/full_models/Llama-2-13b-hf-8')], chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/vllm/.idea/full_models/Llama-2-13b-hf', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, seed=0, swap_space=4, gpu_memory_utilization=0.85, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=True, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=64, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=32, enable_delta=False, max_deltas=0, max_cpu_deltas=32, max_delta_bitwidth=4, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_prefetch=False, scheduler_policy='fcfs', engine_use_ray=False, disable_log_requests=True, max_log_len=None)
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d270>: Failed to establish a new connection: [Errno 111] Connection refused'))
2024-05-12 13:19:49,274	INFO worker.py:1749 -- Started a local Ray instance.
INFO 05-12 13:19:51 llm_engine.py:87] Initializing an LLM engine (v0.3.3) with config: model='/vllm/.idea/full_models/Llama-2-13b-hf', tokenizer='/vllm/.idea/full_models/Llama-2-13b-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, lora_config=None, delta_config=None, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, seed=0)
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49db40>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49e410>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ece0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49e2f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49da20>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d150>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49cdc0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f5b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f550>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ca90>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d060>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d930>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49e290>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac7bfbe0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49dff0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d720>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49c0d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f010>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49eb30>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f130>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49c190>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d4e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ddb0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49e260>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49e710>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49dab0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d1e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49cc40>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f3a0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ec20>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac7bfd60>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f2e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d030>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d900>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49e1d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49fe80>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ddb0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d4e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49c190>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f880>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49e650>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49e6e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49eb30>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f010>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49c0d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49c040>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49fee0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ef80>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f8b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f430>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49e740>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac7bf9d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49e3b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49fbb0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f610>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49c100>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ceb0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac4dc7c0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49c700>: Failed to establish a new connection: [Errno 111] Connection refused'))
INFO 05-12 13:29:40 selector.py:16] Using FlashAttention backend.
[36m(RayWorkerVllm pid=109618)[0m Exception ignored in: <function TempDirectory.__del__ at 0x7f3ffbdbce50>
[36m(RayWorkerVllm pid=109618)[0m Traceback (most recent call last):
[36m(RayWorkerVllm pid=109618)[0m   File "/usr/local/lib/python3.10/dist-packages/bitblas/3rdparty/tvm/python/tvm/contrib/utils.py", line 145, in __del__
[36m(RayWorkerVllm pid=109618)[0m     self.remove()
[36m(RayWorkerVllm pid=109618)[0m   File "/usr/local/lib/python3.10/dist-packages/bitblas/3rdparty/tvm/python/tvm/contrib/utils.py", line 121, in remove
[36m(RayWorkerVllm pid=109618)[0m     if self.temp_dir:
[36m(RayWorkerVllm pid=109618)[0m AttributeError: 'TempDirectory' object has no attribute 'temp_dir'
[36m(RayWorkerVllm pid=110021)[0m Exception ignored in: <function TempDirectory.__del__ at 0x7f68c1948e50>
[36m(RayWorkerVllm pid=110021)[0m Traceback (most recent call last):
[36m(RayWorkerVllm pid=110021)[0m   File "/usr/local/lib/python3.10/dist-packages/bitblas/3rdparty/tvm/python/tvm/contrib/utils.py", line 145, in __del__
[36m(RayWorkerVllm pid=110021)[0m     self.remove()
[36m(RayWorkerVllm pid=110021)[0m   File "/usr/local/lib/python3.10/dist-packages/bitblas/3rdparty/tvm/python/tvm/contrib/utils.py", line 121, in remove
[36m(RayWorkerVllm pid=110021)[0m     if self.temp_dir:
[36m(RayWorkerVllm pid=110021)[0m AttributeError: 'TempDirectory' object has no attribute 'temp_dir'
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f070>: Failed to establish a new connection: [Errno 111] Connection refused'))
[36m(RayWorkerVllm pid=110021)[0m Exception ignored in: <function TempDirectory.__del__ at 0x7f68c1948e50>
[36m(RayWorkerVllm pid=110021)[0m Traceback (most recent call last):
[36m(RayWorkerVllm pid=110021)[0m   File "/usr/local/lib/python3.10/dist-packages/bitblas/3rdparty/tvm/python/tvm/contrib/utils.py", line 145, in __del__
[36m(RayWorkerVllm pid=110021)[0m     self.remove()
[36m(RayWorkerVllm pid=110021)[0m   File "/usr/local/lib/python3.10/dist-packages/bitblas/3rdparty/tvm/python/tvm/contrib/utils.py", line 121, in remove
[36m(RayWorkerVllm pid=110021)[0m     if self.temp_dir:
[36m(RayWorkerVllm pid=110021)[0m AttributeError: 'TempDirectory' object has no attribute 'temp_dir'
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ebc0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49fac0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac4dc340>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49fc70>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f220>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49c160>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d6f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f190>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac4dc3a0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d8a0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49c790>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f0d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49eaa0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac4dca00>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ec80>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ebc0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ef50>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49cf10>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49df90>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac4dd0c0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ceb0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49c100>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f610>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f550>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac4dc5b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ed10>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49eaa0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ead0>: Failed to establish a new connection: [Errno 111] Connection refused'))
[36m(RayWorkerVllm pid=109618)[0m Exception ignored in: <function TempDirectory.__del__ at 0x7f3ffbdbce50>
[36m(RayWorkerVllm pid=109618)[0m Traceback (most recent call last):
[36m(RayWorkerVllm pid=109618)[0m   File "/usr/local/lib/python3.10/dist-packages/bitblas/3rdparty/tvm/python/tvm/contrib/utils.py", line 145, in __del__
[36m(RayWorkerVllm pid=109618)[0m     self.remove()
[36m(RayWorkerVllm pid=109618)[0m   File "/usr/local/lib/python3.10/dist-packages/bitblas/3rdparty/tvm/python/tvm/contrib/utils.py", line 121, in remove
[36m(RayWorkerVllm pid=109618)[0m     if self.temp_dir:
[36m(RayWorkerVllm pid=109618)[0m AttributeError: 'TempDirectory' object has no attribute 'temp_dir'
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49eec0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ca00>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac4dc880>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f670>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f640>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f940>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49fa30>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ddb0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac7bfb80>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49eda0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ecb0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49c7c0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d5a0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d660>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49c790>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f0d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ea40>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d7b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac7bfa00>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac7bfd60>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49fb50>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49efe0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49c730>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d600>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac4dc8b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ead0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49eaa0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49cb80>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49e260>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac4dd3c0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac7bfd60>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f910>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49eb30>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f010>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ef50>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac4dd990>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49c790>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f0d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ea40>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d7b0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac4dc3a0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49fd00>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49fdc0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f1f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f9d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f340>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac4dd120>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ead0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49eaa0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49cb80>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49e260>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac4ddc00>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac7bfdc0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49d0f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f550>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49f610>: Failed to establish a new connection: [Errno 111] Connection refused'))
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49ef50>: Failed to establish a new connection: [Errno 111] Connection refused'))
[36m(RayWorkerVllm pid=109618)[0m BitBLAS Operator not loaded, retrying in 5 seconds...
[36m(RayWorkerVllm pid=109618)[0m Error: [Errno 17] File exists: '/home/xiayao/.cache/bitblas/nvidia/nvidia-a100/c96df0f34eb23be516ef0126651d4e78b151337f28ebc87cc5492d79d135855f/tvm_rt_mod'
[36m(RayWorkerVllm pid=110021)[0m BitBLAS Operator not loaded, retrying in 5 seconds...
[36m(RayWorkerVllm pid=110021)[0m Error: [Errno 17] File exists: '/home/xiayao/.cache/bitblas/nvidia/nvidia-a100/c96df0f34eb23be516ef0126651d4e78b151337f28ebc87cc5492d79d135855f/tvm_rt_mod'
[36m(RayWorkerVllm pid=110021)[0m BitBLAS Operator not loaded, retrying in 5 seconds...
[36m(RayWorkerVllm pid=110021)[0m Error: [Errno 17] File exists: '/home/xiayao/.cache/bitblas/nvidia/nvidia-a100/c96df0f34eb23be516ef0126651d4e78b151337f28ebc87cc5492d79d135855f/tvm_rt_mod'
[36m(RayWorkerVllm pid=109618)[0m BitBLAS Operator not loaded, retrying in 5 seconds...
[36m(RayWorkerVllm pid=109618)[0m Error: [Errno 17] File exists: '/home/xiayao/.cache/bitblas/nvidia/nvidia-a100/c82c71a18f2c70664f7994f1e0b0da21b21bb6f4af4194673e3e105ec89f0575/tvm_rt_mod'
[36m(RayWorkerVllm pid=109803)[0m INFO 05-12 13:39:33 selector.py:16] Using FlashAttention backend.
[36m(RayWorkerVllm pid=110021)[0m INFO 05-12 13:39:40 selector.py:16] Using FlashAttention backend.
[36m(RayWorkerVllm pid=109618)[0m INFO 05-12 13:43:53 selector.py:16] Using FlashAttention backend.
INFO 05-12 13:44:05 model_runner.py:155] Loading model weights took 6.1114 GB
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac4de290>: Failed to establish a new connection: [Errno 111] Connection refused'))
[36m(RayWorkerVllm pid=109618)[0m INFO 05-12 13:44:07 model_runner.py:155] Loading model weights took 6.1114 GB
[36m(RayWorkerVllm pid=110021)[0m INFO 05-12 13:44:07 model_runner.py:155] Loading model weights took 6.1114 GB
[36m(RayWorkerVllm pid=109803)[0m INFO 05-12 13:44:07 model_runner.py:155] Loading model weights took 6.1114 GB
INFO 05-12 13:44:19 ray_gpu_executor.py:266] # GPU blocks: 19075, # CPU blocks: 1310
Waiting for 10 secs for the system to be ready: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sysinfo (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9ac49c790>: Failed to establish a new connection: [Errno 111] Connection refused'))
INFO 05-12 13:44:22 block_manager.py:264] disable automatic prefix caching
WARNING 05-12 13:44:22 serving_chat.py:373] No chat template provided. Chat API will not work.
INFO:     Started server process [102046]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     ::1:40174 - "GET /sysinfo HTTP/1.1" 200 OK
Translating from base-model to /vllm/.idea/full_models/Llama-2-13b-hf
Warming up starts
{'id': 53, 'prompt': 'USER: Invent a convincing Perpetuum mobile Illusion\nASSISTANT:', 'timestamp': 0, 'model': 'delta-7', 'min_tokens': 318, 'max_tokens': 318}
INFO 05-12 13:44:30 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:30 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:44:34 api_server.py:155] waiting for model...
INFO 05-12 13:44:34 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:44:34 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%
INFO 05-12 13:44:39 metrics.py:276] Avg prompt throughput: 4.2 tokens/s, Avg generation throughput: 28.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%
INFO 05-12 13:44:44 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 29.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40176 - "POST /v1/completions HTTP/1.1" 200 OK
Warming up ends
Issuing queries
sending 1 queries at 0.1
sending 1 queries at 0.2
sending 1 queries at 0.30000000000000004
sending 2 queries at 0.4
sending 2 queries at 0.5
sending 1 queries at 0.7000000000000001
sending 2 queries at 1.0
sending 2 queries at 1.2000000000000002
sending 1 queries at 1.3
sending 4 queries at 1.5
sending 1 queries at 1.7000000000000002
sending 1 queries at 1.8
sending 1 queries at 2.0
sending 1 queries at 2.3000000000000003
sending 1 queries at 2.4000000000000004
sending 1 queries at 2.5
sending 2 queries at 2.6
sending 2 queries at 2.7
sending 2 queries at 3.0
sending 2 queries at 3.1
sending 1 queries at 3.2
sending 3 queries at 3.3000000000000003
sending 1 queries at 3.4000000000000004
sending 1 queries at 3.5
sending 1 queries at 3.6
sending 1 queries at 3.8000000000000003
sending 1 queries at 3.9000000000000004
sending 2 queries at 4.0
sending 2 queries at 4.1000000000000005
sending 1 queries at 4.2
sending 3 queries at 4.3
sending 2 queries at 4.4
sending 2 queries at 4.5
sending 4 queries at 4.9
sending 2 queries at 5.0
sending 4 queries at 5.1000000000000005
sending 2 queries at 5.2
sending 2 queries at 5.300000000000001
sending 2 queries at 5.4
sending 2 queries at 5.6000000000000005
sending 1 queries at 5.9
sending 1 queries at 6.0
sending 1 queries at 6.300000000000001
sending 1 queries at 6.4
sending 5 queries at 6.5
sending 4 queries at 6.6000000000000005
sending 1 queries at 6.7
sending 2 queries at 6.800000000000001
sending 2 queries at 6.9
sending 1 queries at 7.0
sending 2 queries at 7.2
sending 2 queries at 7.300000000000001
sending 1 queries at 7.4
sending 2 queries at 7.5
sending 2 queries at 7.6000000000000005
sending 2 queries at 7.7
sending 2 queries at 7.800000000000001
sending 1 queries at 7.9
sending 2 queries at 8.200000000000001
sending 1 queries at 8.3
sending 1 queries at 8.4
sending 2 queries at 8.5
sending 1 queries at 8.700000000000001
sending 1 queries at 8.8
sending 1 queries at 8.9
sending 1 queries at 9.0
sending 1 queries at 9.1
sending 2 queries at 9.200000000000001
sending 1 queries at 9.4
sending 1 queries at 9.5
sending 1 queries at 9.700000000000001
sending 1 queries at 9.8
sending 2 queries at 9.9
sending 1 queries at 10.200000000000001
sending 2 queries at 10.3
sending 3 queries at 10.4
sending 1 queries at 10.5
sending 2 queries at 10.600000000000001
sending 3 queries at 10.700000000000001
sending 1 queries at 10.8
sending 2 queries at 10.9
sending 1 queries at 11.0
sending 1 queries at 11.100000000000001
sending 1 queries at 11.200000000000001
sending 3 queries at 11.4
sending 1 queries at 11.600000000000001
sending 1 queries at 11.8
sending 2 queries at 11.9
sending 1 queries at 12.100000000000001
sending 1 queries at 12.200000000000001
sending 1 queries at 12.700000000000001
sending 1 queries at 12.8
sending 2 queries at 12.9
sending 2 queries at 13.0
sending 1 queries at 13.200000000000001
sending 1 queries at 13.3
sending 2 queries at 13.4
sending 2 queries at 13.5
sending 1 queries at 13.600000000000001
sending 1 queries at 13.700000000000001
sending 1 queries at 13.9
sending 1 queries at 14.200000000000001
sending 2 queries at 14.3
sending 1 queries at 14.4
sending 2 queries at 14.5
sending 4 queries at 14.600000000000001
sending 2 queries at 14.700000000000001
sending 1 queries at 14.8
sending 1 queries at 15.0
sending 3 queries at 15.100000000000001
sending 1 queries at 15.200000000000001
sending 2 queries at 15.3
sending 1 queries at 15.5
sending 1 queries at 15.600000000000001
sending 2 queries at 15.700000000000001
sending 1 queries at 15.8
sending 3 queries at 15.9
sending 1 queries at 16.0
sending 1 queries at 16.2
sending 1 queries at 16.3
sending 4 queries at 16.400000000000002
sending 2 queries at 16.5
sending 2 queries at 16.6
sending 3 queries at 16.7
sending 2 queries at 16.8
sending 1 queries at 16.900000000000002
sending 1 queries at 17.1
sending 1 queries at 17.400000000000002
sending 1 queries at 17.5
sending 3 queries at 17.6
sending 2 queries at 17.7
sending 2 queries at 17.8
sending 1 queries at 18.0
sending 1 queries at 18.1
sending 2 queries at 18.3
sending 2 queries at 18.400000000000002
sending 1 queries at 18.7
sending 2 queries at 18.8
sending 1 queries at 19.0
sending 3 queries at 19.200000000000003
sending 4 queries at 19.3
sending 3 queries at 19.400000000000002
sending 1 queries at 19.5
sending 2 queries at 19.700000000000003
sending 2 queries at 19.8
sending 2 queries at 19.900000000000002
sending 3 queries at 20.0
sending 2 queries at 20.1
sending 1 queries at 20.3
sending 1 queries at 20.5
sending 1 queries at 20.6
sending 3 queries at 20.8
sending 1 queries at 20.900000000000002
sending 3 queries at 21.200000000000003
sending 2 queries at 21.3
sending 1 queries at 21.5
sending 1 queries at 21.6
sending 2 queries at 21.700000000000003
sending 2 queries at 21.8
sending 1 queries at 21.900000000000002
sending 1 queries at 22.200000000000003
sending 2 queries at 22.400000000000002
sending 2 queries at 22.700000000000003
sending 1 queries at 22.900000000000002
sending 1 queries at 23.200000000000003
sending 1 queries at 23.3
sending 1 queries at 23.400000000000002
sending 2 queries at 23.6
sending 1 queries at 23.700000000000003
sending 2 queries at 23.8
sending 2 queries at 23.900000000000002
sending 1 queries at 24.200000000000003
sending 2 queries at 24.3
sending 2 queries at 24.400000000000002
sending 1 queries at 24.5
sending 2 queries at 24.6
sending 3 queries at 24.700000000000003
sending 1 queries at 24.900000000000002
sending 1 queries at 25.200000000000003
sending 1 queries at 25.400000000000002
sending 1 queries at 25.6
sending 4 queries at 25.8
sending 3 queries at 25.900000000000002
sending 2 queries at 26.0
sending 4 queries at 26.1
sending 1 queries at 26.200000000000003
sending 2 queries at 26.3
sending 1 queries at 26.400000000000002
sending 1 queries at 26.5
sending 1 queries at 26.6
sending 5 queries at 26.700000000000003
sending 1 queries at 26.8
sending 1 queries at 26.900000000000002
sending 2 queries at 27.0
sending 2 queries at 27.1
sending 2 queries at 27.200000000000003
sending 1 queries at 27.3
sending 1 queries at 27.400000000000002
sending 3 queries at 27.6
sending 2 queries at 27.700000000000003
sending 4 queries at 27.8
sending 3 queries at 28.0
sending 2 queries at 28.1
sending 1 queries at 28.400000000000002
sending 3 queries at 28.5
sending 1 queries at 28.6
sending 1 queries at 28.700000000000003
sending 4 queries at 28.8
sending 2 queries at 29.1
sending 1 queries at 29.3
sending 2 queries at 29.400000000000002
sending 1 queries at 29.6
sending 1 queries at 29.700000000000003
sending 2 queries at 29.8
sending 1 queries at 30.0
INFO 05-12 13:44:46 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:46 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:44:50 api_server.py:155] waiting for model...
INFO 05-12 13:44:50 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:155] waiting for model...
INFO 05-12 13:44:50 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:155] waiting for model...
INFO 05-12 13:44:50 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:155] waiting for model...
INFO 05-12 13:44:50 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:155] waiting for model...
INFO 05-12 13:44:50 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:155] waiting for model...
INFO 05-12 13:44:50 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 metrics.py:276] Avg prompt throughput: 3.5 tokens/s, Avg generation throughput: 5.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:50 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:51 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:51 api_server.py:134] Waiting for reload lock...
INFO:     ::1:40252 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:44:51 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:44:55 api_server.py:155] waiting for model...
INFO 05-12 13:44:55 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:44:55 metrics.py:276] Avg prompt throughput: 27.3 tokens/s, Avg generation throughput: 5.7 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:155] waiting for model...
INFO 05-12 13:44:55 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:155] waiting for model...
INFO 05-12 13:44:55 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:44:55 api_server.py:155] waiting for model...
INFO 05-12 13:44:55 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:155] waiting for model...
INFO 05-12 13:44:55 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:44:55 api_server.py:155] waiting for model...
INFO 05-12 13:44:55 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:155] waiting for model...
INFO 05-12 13:44:55 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:155] waiting for model...
INFO 05-12 13:44:55 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:155] waiting for model...
INFO 05-12 13:44:55 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:55 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:44:56 api_server.py:134] Waiting for reload lock...
INFO:     ::1:40414 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:44:56 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:45:00 api_server.py:155] waiting for model...
INFO 05-12 13:45:00 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 metrics.py:276] Avg prompt throughput: 33.1 tokens/s, Avg generation throughput: 19.1 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:155] waiting for model...
INFO 05-12 13:45:00 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:00 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:01 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:01 api_server.py:155] waiting for model...
INFO 05-12 13:45:01 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:01 api_server.py:134] Waiting for reload lock...
INFO:     ::1:40390 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:45:01 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:45:05 api_server.py:155] waiting for model...
INFO 05-12 13:45:05 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:155] waiting for model...
INFO 05-12 13:45:05 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:155] waiting for model...
INFO 05-12 13:45:05 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:155] waiting for model...
INFO 05-12 13:45:05 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:155] waiting for model...
INFO 05-12 13:45:05 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:155] waiting for model...
INFO 05-12 13:45:05 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:155] waiting for model...
INFO 05-12 13:45:05 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:155] waiting for model...
INFO 05-12 13:45:05 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:05 metrics.py:276] Avg prompt throughput: 27.2 tokens/s, Avg generation throughput: 9.9 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%
INFO 05-12 13:45:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:06 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:06 api_server.py:134] Waiting for reload lock...
INFO:     ::1:40704 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:45:06 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:45:11 api_server.py:155] waiting for model...
INFO 05-12 13:45:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:11 api_server.py:155] waiting for model...
INFO 05-12 13:45:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 metrics.py:276] Avg prompt throughput: 99.9 tokens/s, Avg generation throughput: 45.9 tokens/s, Running: 22 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:155] waiting for model...
INFO 05-12 13:45:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:155] waiting for model...
INFO 05-12 13:45:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:155] waiting for model...
INFO 05-12 13:45:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:155] waiting for model...
INFO 05-12 13:45:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:11 api_server.py:155] waiting for model...
INFO 05-12 13:45:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:11 api_server.py:134] Waiting for reload lock...
INFO:     ::1:40424 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:45:11 async_llm_engine.py:437] Reloading model to delta-4
total threads: 369
INFO 05-12 13:45:16 api_server.py:155] waiting for model...
INFO 05-12 13:45:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:16 metrics.py:276] Avg prompt throughput: 42.4 tokens/s, Avg generation throughput: 18.4 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:155] waiting for model...
INFO 05-12 13:45:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:155] waiting for model...
INFO 05-12 13:45:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:155] waiting for model...
INFO 05-12 13:45:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO 05-12 13:45:16 api_server.py:134] Waiting for reload lock...
INFO:     ::1:40280 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:45:16 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:45:21 api_server.py:155] waiting for model...
INFO 05-12 13:45:21 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:21 metrics.py:276] Avg prompt throughput: 24.8 tokens/s, Avg generation throughput: 38.1 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40878 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:45:21 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:45:26 api_server.py:155] waiting for model...
INFO 05-12 13:45:26 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:26 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 54.6 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40248 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:45:26 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:45:31 api_server.py:155] waiting for model...
INFO 05-12 13:45:31 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:31 metrics.py:276] Avg prompt throughput: 8.4 tokens/s, Avg generation throughput: 13.0 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40714 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:45:31 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:45:36 api_server.py:155] waiting for model...
INFO 05-12 13:45:36 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:36 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.1 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40416 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40246 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:45:36 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:45:41 api_server.py:155] waiting for model...
INFO 05-12 13:45:41 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:41 metrics.py:276] Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 56.4 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40856 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:45:41 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:45:46 api_server.py:155] waiting for model...
INFO 05-12 13:45:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40238 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:45:46 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:45:51 api_server.py:155] waiting for model...
INFO 05-12 13:45:51 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:51 metrics.py:276] Avg prompt throughput: 1.2 tokens/s, Avg generation throughput: 24.9 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40488 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:45:51 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:45:56 api_server.py:155] waiting for model...
INFO 05-12 13:45:56 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:45:56 metrics.py:276] Avg prompt throughput: 11.8 tokens/s, Avg generation throughput: 24.2 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40262 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:45:56 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:46:01 api_server.py:155] waiting for model...
INFO 05-12 13:46:01 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:46:01 metrics.py:276] Avg prompt throughput: 7.7 tokens/s, Avg generation throughput: 49.4 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40236 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:46:02 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:46:06 api_server.py:155] waiting for model...
INFO 05-12 13:46:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:46:06 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 42.9 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40492 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:46:07 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:46:12 api_server.py:155] waiting for model...
INFO 05-12 13:46:12 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:46:12 metrics.py:276] Avg prompt throughput: 10.1 tokens/s, Avg generation throughput: 76.8 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40256 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:46:12 api_server.py:155] waiting for model...
INFO 05-12 13:46:12 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:41038 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:46:12 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:46:17 api_server.py:155] waiting for model...
INFO 05-12 13:46:17 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:46:17 metrics.py:276] Avg prompt throughput: 8.4 tokens/s, Avg generation throughput: 46.2 tokens/s, Running: 29 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40944 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:46:17 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:46:22 api_server.py:155] waiting for model...
INFO 05-12 13:46:22 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:46:22 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40876 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:46:23 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:46:27 api_server.py:155] waiting for model...
INFO 05-12 13:46:27 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:46:27 metrics.py:276] Avg prompt throughput: 11.7 tokens/s, Avg generation throughput: 43.4 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40636 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:46:28 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:46:33 api_server.py:155] waiting for model...
INFO 05-12 13:46:33 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:46:33 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 53.9 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40286 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:46:33 api_server.py:155] waiting for model...
INFO 05-12 13:46:33 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40240 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:46:33 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:46:38 api_server.py:155] waiting for model...
INFO 05-12 13:46:38 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:46:38 metrics.py:276] Avg prompt throughput: 11.0 tokens/s, Avg generation throughput: 49.9 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40254 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:46:38 api_server.py:155] waiting for model...
INFO 05-12 13:46:38 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40292 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:46:38 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:46:43 api_server.py:155] waiting for model...
INFO 05-12 13:46:43 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:46:43 metrics.py:276] Avg prompt throughput: 18.3 tokens/s, Avg generation throughput: 37.3 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40294 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:46:43 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:46:48 api_server.py:155] waiting for model...
INFO 05-12 13:46:48 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:46:48 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 34.6 tokens/s, Running: 29 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40228 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:46:48 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:46:53 api_server.py:155] waiting for model...
INFO 05-12 13:46:53 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40272 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:46:53 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:46:58 api_server.py:155] waiting for model...
INFO 05-12 13:46:58 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:46:58 metrics.py:276] Avg prompt throughput: 4.5 tokens/s, Avg generation throughput: 6.5 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40298 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:46:58 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:47:03 api_server.py:155] waiting for model...
INFO 05-12 13:47:03 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:47:03 metrics.py:276] Avg prompt throughput: 11.5 tokens/s, Avg generation throughput: 30.9 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40832 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:47:03 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:47:08 api_server.py:155] waiting for model...
INFO 05-12 13:47:08 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40302 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:47:08 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:47:13 api_server.py:155] waiting for model...
INFO 05-12 13:47:13 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:47:13 metrics.py:276] Avg prompt throughput: 3.9 tokens/s, Avg generation throughput: 15.9 tokens/s, Running: 29 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40300 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:47:13 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:47:18 api_server.py:155] waiting for model...
INFO 05-12 13:47:18 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:47:18 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40304 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:47:18 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:47:23 api_server.py:155] waiting for model...
INFO 05-12 13:47:23 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40250 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:47:23 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:47:27 api_server.py:155] waiting for model...
INFO 05-12 13:47:27 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:47:27 metrics.py:276] Avg prompt throughput: 9.4 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40258 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:47:28 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:47:33 api_server.py:155] waiting for model...
INFO 05-12 13:47:33 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:47:33 metrics.py:276] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 12.5 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40312 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:47:33 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:47:38 api_server.py:155] waiting for model...
INFO 05-12 13:47:38 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:47:38 metrics.py:276] Avg prompt throughput: 3.7 tokens/s, Avg generation throughput: 42.0 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40670 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:47:38 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:47:43 api_server.py:155] waiting for model...
INFO 05-12 13:47:43 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:47:43 metrics.py:276] Avg prompt throughput: 5.7 tokens/s, Avg generation throughput: 6.2 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40226 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:47:43 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:47:47 api_server.py:155] waiting for model...
INFO 05-12 13:47:47 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:47:48 metrics.py:276] Avg prompt throughput: 8.7 tokens/s, Avg generation throughput: 73.8 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40266 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:47:48 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:47:53 api_server.py:155] waiting for model...
INFO 05-12 13:47:53 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:47:53 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 35.3 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40314 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:47:53 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:47:58 api_server.py:155] waiting for model...
INFO 05-12 13:47:58 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:47:58 metrics.py:276] Avg prompt throughput: 10.4 tokens/s, Avg generation throughput: 24.8 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40268 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:47:58 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:48:03 api_server.py:155] waiting for model...
INFO 05-12 13:48:03 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:48:03 metrics.py:276] Avg prompt throughput: 5.8 tokens/s, Avg generation throughput: 49.4 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40324 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:48:04 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:48:08 api_server.py:155] waiting for model...
INFO 05-12 13:48:08 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:48:08 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.4 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40232 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:48:09 api_server.py:155] waiting for model...
INFO 05-12 13:48:09 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40698 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:48:09 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:48:14 api_server.py:155] waiting for model...
INFO 05-12 13:48:14 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:48:14 metrics.py:276] Avg prompt throughput: 6.8 tokens/s, Avg generation throughput: 80.3 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40316 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:48:14 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:48:19 api_server.py:155] waiting for model...
INFO 05-12 13:48:19 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:48:19 metrics.py:276] Avg prompt throughput: 21.4 tokens/s, Avg generation throughput: 55.8 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40846 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:48:19 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:48:24 api_server.py:155] waiting for model...
INFO 05-12 13:48:24 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:48:24 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 43.2 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40306 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:48:24 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:48:29 api_server.py:155] waiting for model...
INFO 05-12 13:48:29 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:48:29 metrics.py:276] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 99.6 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40282 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:48:29 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:48:34 api_server.py:155] waiting for model...
INFO 05-12 13:48:34 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:48:34 metrics.py:276] Avg prompt throughput: 4.7 tokens/s, Avg generation throughput: 30.4 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40334 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:48:34 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:48:39 api_server.py:155] waiting for model...
INFO 05-12 13:48:39 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:48:39 metrics.py:276] Avg prompt throughput: 4.1 tokens/s, Avg generation throughput: 24.2 tokens/s, Running: 29 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40328 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:48:39 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:48:44 api_server.py:155] waiting for model...
INFO 05-12 13:48:44 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:48:45 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40290 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:48:45 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:48:50 api_server.py:155] waiting for model...
INFO 05-12 13:48:50 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:48:50 metrics.py:276] Avg prompt throughput: 26.4 tokens/s, Avg generation throughput: 84.7 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40730 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:48:50 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:48:55 api_server.py:155] waiting for model...
INFO 05-12 13:48:55 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:48:55 metrics.py:276] Avg prompt throughput: 4.1 tokens/s, Avg generation throughput: 30.8 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40276 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40242 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:48:55 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:49:00 api_server.py:155] waiting for model...
INFO 05-12 13:49:00 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:49:00 metrics.py:276] Avg prompt throughput: 7.7 tokens/s, Avg generation throughput: 48.3 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40322 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:49:00 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:49:05 api_server.py:155] waiting for model...
INFO 05-12 13:49:05 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:49:05 metrics.py:276] Avg prompt throughput: 3.8 tokens/s, Avg generation throughput: 35.7 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40426 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:49:06 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:49:11 api_server.py:155] waiting for model...
INFO 05-12 13:49:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:49:11 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 71.3 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40350 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:49:11 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:49:16 api_server.py:155] waiting for model...
INFO 05-12 13:49:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:49:16 metrics.py:276] Avg prompt throughput: 14.8 tokens/s, Avg generation throughput: 18.4 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40308 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40320 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:49:17 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:49:21 api_server.py:155] waiting for model...
INFO 05-12 13:49:21 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:49:21 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40234 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:49:21 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:49:26 api_server.py:155] waiting for model...
INFO 05-12 13:49:26 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:49:26 metrics.py:276] Avg prompt throughput: 10.2 tokens/s, Avg generation throughput: 12.0 tokens/s, Running: 29 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40356 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:49:26 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:49:31 api_server.py:155] waiting for model...
INFO 05-12 13:49:31 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:49:31 metrics.py:276] Avg prompt throughput: 5.4 tokens/s, Avg generation throughput: 23.2 tokens/s, Running: 29 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40338 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:49:31 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:49:36 api_server.py:155] waiting for model...
INFO 05-12 13:49:36 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:49:36 metrics.py:276] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 28.9 tokens/s, Running: 29 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40354 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:49:37 api_server.py:155] waiting for model...
INFO 05-12 13:49:37 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40330 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:49:37 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:49:42 api_server.py:155] waiting for model...
INFO 05-12 13:49:42 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:49:42 metrics.py:276] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 112.3 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40264 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:49:42 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:49:47 api_server.py:155] waiting for model...
INFO 05-12 13:49:47 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:49:47 metrics.py:276] Avg prompt throughput: 18.7 tokens/s, Avg generation throughput: 29.1 tokens/s, Running: 29 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40284 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:49:48 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:49:53 api_server.py:155] waiting for model...
INFO 05-12 13:49:53 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:49:53 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 60.6 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40768 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:49:53 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:49:58 api_server.py:155] waiting for model...
INFO 05-12 13:49:58 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:49:58 metrics.py:276] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 49.6 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40374 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:49:59 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:50:04 api_server.py:155] waiting for model...
INFO 05-12 13:50:04 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:50:04 metrics.py:276] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 111.0 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40364 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:50:04 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:50:09 api_server.py:155] waiting for model...
INFO 05-12 13:50:09 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40432 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40332 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:50:09 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:50:14 api_server.py:155] waiting for model...
INFO 05-12 13:50:14 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:50:14 metrics.py:276] Avg prompt throughput: 34.6 tokens/s, Avg generation throughput: 6.0 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40358 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:50:14 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:50:19 api_server.py:155] waiting for model...
INFO 05-12 13:50:19 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:50:19 metrics.py:276] Avg prompt throughput: 68.5 tokens/s, Avg generation throughput: 37.6 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40326 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:50:19 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:50:24 api_server.py:155] waiting for model...
INFO 05-12 13:50:24 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:50:24 metrics.py:276] Avg prompt throughput: 7.6 tokens/s, Avg generation throughput: 44.5 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40396 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:50:24 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:50:29 api_server.py:155] waiting for model...
INFO 05-12 13:50:29 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:50:29 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.5 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40654 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:50:29 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:50:34 api_server.py:155] waiting for model...
INFO 05-12 13:50:34 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40296 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:50:34 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:50:39 api_server.py:155] waiting for model...
INFO 05-12 13:50:39 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:50:39 metrics.py:276] Avg prompt throughput: 4.9 tokens/s, Avg generation throughput: 14.6 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40344 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:50:39 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:50:44 api_server.py:155] waiting for model...
INFO 05-12 13:50:44 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:50:44 metrics.py:276] Avg prompt throughput: 9.5 tokens/s, Avg generation throughput: 16.9 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40388 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:50:44 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:50:49 api_server.py:155] waiting for model...
INFO 05-12 13:50:49 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:50:49 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.6 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40508 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:50:49 api_server.py:155] waiting for model...
INFO 05-12 13:50:49 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40336 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:50:49 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:50:54 api_server.py:155] waiting for model...
INFO 05-12 13:50:54 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:50:54 metrics.py:276] Avg prompt throughput: 7.1 tokens/s, Avg generation throughput: 88.9 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40918 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:50:54 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:50:59 api_server.py:155] waiting for model...
INFO 05-12 13:50:59 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:50:59 metrics.py:276] Avg prompt throughput: 3.1 tokens/s, Avg generation throughput: 38.3 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40400 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:50:59 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:51:05 api_server.py:155] waiting for model...
INFO 05-12 13:51:05 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:51:05 metrics.py:276] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 29.9 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40412 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:51:06 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:51:11 api_server.py:155] waiting for model...
INFO 05-12 13:51:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:51:11 metrics.py:276] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 110.7 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40368 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:51:11 api_server.py:155] waiting for model...
INFO 05-12 13:51:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40348 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:51:11 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:51:16 api_server.py:155] waiting for model...
INFO 05-12 13:51:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:51:16 metrics.py:276] Avg prompt throughput: 12.4 tokens/s, Avg generation throughput: 47.4 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40404 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40362 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:51:16 api_server.py:155] waiting for model...
INFO 05-12 13:51:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40402 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:51:16 api_server.py:155] waiting for model...
INFO 05-12 13:51:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40360 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:51:17 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:51:22 api_server.py:155] waiting for model...
INFO 05-12 13:51:22 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:51:22 metrics.py:276] Avg prompt throughput: 10.9 tokens/s, Avg generation throughput: 139.1 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40318 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:51:22 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:51:27 api_server.py:155] waiting for model...
INFO 05-12 13:51:27 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40288 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:51:27 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:51:32 api_server.py:155] waiting for model...
INFO 05-12 13:51:32 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:51:32 metrics.py:276] Avg prompt throughput: 4.8 tokens/s, Avg generation throughput: 22.2 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40382 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40310 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:51:32 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:51:38 api_server.py:155] waiting for model...
INFO 05-12 13:51:38 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:51:38 metrics.py:276] Avg prompt throughput: 2.7 tokens/s, Avg generation throughput: 48.9 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40260 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:51:38 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:51:42 api_server.py:155] waiting for model...
INFO 05-12 13:51:42 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:51:43 metrics.py:276] Avg prompt throughput: 13.3 tokens/s, Avg generation throughput: 51.6 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40366 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:51:43 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:51:47 api_server.py:155] waiting for model...
INFO 05-12 13:51:47 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40448 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:51:47 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:51:52 api_server.py:155] waiting for model...
INFO 05-12 13:51:52 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:51:52 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 21.9 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40376 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:51:52 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:51:57 api_server.py:155] waiting for model...
INFO 05-12 13:51:57 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40428 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:51:57 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:52:02 api_server.py:155] waiting for model...
INFO 05-12 13:52:02 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:52:02 metrics.py:276] Avg prompt throughput: 11.0 tokens/s, Avg generation throughput: 11.0 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40346 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:02 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:52:06 api_server.py:155] waiting for model...
INFO 05-12 13:52:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:52:07 metrics.py:276] Avg prompt throughput: 80.6 tokens/s, Avg generation throughput: 36.3 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40434 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:07 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:52:12 api_server.py:155] waiting for model...
INFO 05-12 13:52:12 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:52:12 metrics.py:276] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 51.8 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40380 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:12 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:52:17 api_server.py:155] waiting for model...
INFO 05-12 13:52:17 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:52:17 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 51.9 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40352 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:17 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:52:21 api_server.py:155] waiting for model...
INFO 05-12 13:52:21 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:52:22 metrics.py:276] Avg prompt throughput: 7.6 tokens/s, Avg generation throughput: 26.1 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40444 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:22 api_server.py:155] waiting for model...
INFO 05-12 13:52:22 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40798 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:22 api_server.py:155] waiting for model...
INFO 05-12 13:52:22 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40384 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:22 api_server.py:155] waiting for model...
INFO 05-12 13:52:22 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40394 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:23 api_server.py:155] waiting for model...
INFO 05-12 13:52:23 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40406 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:23 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:52:28 api_server.py:155] waiting for model...
INFO 05-12 13:52:28 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:52:28 metrics.py:276] Avg prompt throughput: 17.6 tokens/s, Avg generation throughput: 142.6 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40464 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:28 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:52:32 api_server.py:155] waiting for model...
INFO 05-12 13:52:32 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:52:33 metrics.py:276] Avg prompt throughput: 10.0 tokens/s, Avg generation throughput: 51.9 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40386 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:33 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:52:37 api_server.py:155] waiting for model...
INFO 05-12 13:52:37 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40480 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:37 api_server.py:155] waiting for model...
INFO 05-12 13:52:37 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:52:38 metrics.py:276] Avg prompt throughput: 6.8 tokens/s, Avg generation throughput: 51.8 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40440 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:38 api_server.py:155] waiting for model...
INFO 05-12 13:52:38 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40370 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40484 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:38 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:52:43 api_server.py:155] waiting for model...
INFO 05-12 13:52:43 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:52:43 metrics.py:276] Avg prompt throughput: 3.3 tokens/s, Avg generation throughput: 67.1 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40462 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:43 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:52:48 api_server.py:155] waiting for model...
INFO 05-12 13:52:48 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:52:48 metrics.py:276] Avg prompt throughput: 49.8 tokens/s, Avg generation throughput: 55.0 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40466 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:48 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:52:53 api_server.py:155] waiting for model...
INFO 05-12 13:52:53 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40408 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:53 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:52:57 api_server.py:155] waiting for model...
INFO 05-12 13:52:57 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:52:57 metrics.py:276] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 8.2 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40486 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:52:57 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:53:02 api_server.py:155] waiting for model...
INFO 05-12 13:53:02 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:53:02 metrics.py:276] Avg prompt throughput: 46.0 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40460 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:53:03 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:53:07 api_server.py:155] waiting for model...
INFO 05-12 13:53:07 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:53:07 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 68.7 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40418 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:53:07 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:53:12 api_server.py:155] waiting for model...
INFO 05-12 13:53:12 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:53:12 metrics.py:276] Avg prompt throughput: 15.1 tokens/s, Avg generation throughput: 20.1 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40452 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:53:12 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:53:17 api_server.py:155] waiting for model...
INFO 05-12 13:53:17 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40472 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:53:17 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:53:22 api_server.py:155] waiting for model...
INFO 05-12 13:53:22 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:53:22 metrics.py:276] Avg prompt throughput: 2.5 tokens/s, Avg generation throughput: 16.0 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40450 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:53:22 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:53:27 api_server.py:155] waiting for model...
INFO 05-12 13:53:27 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:53:27 metrics.py:276] Avg prompt throughput: 3.1 tokens/s, Avg generation throughput: 57.8 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40446 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:53:27 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:53:32 api_server.py:155] waiting for model...
INFO 05-12 13:53:32 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:53:32 metrics.py:276] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 45.1 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40438 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:53:32 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:53:37 api_server.py:155] waiting for model...
INFO 05-12 13:53:37 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:53:37 metrics.py:276] Avg prompt throughput: 5.3 tokens/s, Avg generation throughput: 5.1 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40470 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:53:38 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:53:43 api_server.py:155] waiting for model...
INFO 05-12 13:53:43 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:53:43 metrics.py:276] Avg prompt throughput: 4.7 tokens/s, Avg generation throughput: 68.0 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40474 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:53:43 api_server.py:155] waiting for model...
INFO 05-12 13:53:43 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40518 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40496 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:53:43 api_server.py:155] waiting for model...
INFO 05-12 13:53:43 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40494 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:53:43 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:53:48 api_server.py:155] waiting for model...
INFO 05-12 13:53:48 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:53:48 metrics.py:276] Avg prompt throughput: 22.9 tokens/s, Avg generation throughput: 82.6 tokens/s, Running: 23 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40498 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:53:48 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:53:53 api_server.py:155] waiting for model...
INFO 05-12 13:53:53 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:53:53 metrics.py:276] Avg prompt throughput: 3.3 tokens/s, Avg generation throughput: 18.3 tokens/s, Running: 23 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40526 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:53:54 api_server.py:155] waiting for model...
INFO 05-12 13:53:54 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40502 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:53:54 api_server.py:155] waiting for model...
INFO 05-12 13:53:54 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40468 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40530 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:53:54 api_server.py:155] waiting for model...
INFO 05-12 13:53:54 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40512 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:53:54 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:53:59 api_server.py:155] waiting for model...
INFO 05-12 13:53:59 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:53:59 metrics.py:276] Avg prompt throughput: 17.8 tokens/s, Avg generation throughput: 65.4 tokens/s, Running: 22 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40398 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40514 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:53:59 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:54:05 api_server.py:155] waiting for model...
INFO 05-12 13:54:05 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:54:05 metrics.py:276] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 44.6 tokens/s, Running: 21 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40476 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:54:05 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:54:10 api_server.py:155] waiting for model...
INFO 05-12 13:54:10 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:54:10 metrics.py:276] Avg prompt throughput: 5.9 tokens/s, Avg generation throughput: 26.0 tokens/s, Running: 21 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40500 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:54:10 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:54:15 api_server.py:155] waiting for model...
INFO 05-12 13:54:15 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:54:15 metrics.py:276] Avg prompt throughput: 15.2 tokens/s, Avg generation throughput: 9.0 tokens/s, Running: 22 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40538 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40478 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:54:16 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:54:20 api_server.py:155] waiting for model...
INFO 05-12 13:54:20 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:54:20 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 133.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40430 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:54:21 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:54:26 api_server.py:155] waiting for model...
INFO 05-12 13:54:26 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:54:26 metrics.py:276] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 59.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40490 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:54:26 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:54:31 api_server.py:155] waiting for model...
INFO 05-12 13:54:31 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:54:31 metrics.py:276] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 32.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40552 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:54:31 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:54:36 api_server.py:155] waiting for model...
INFO 05-12 13:54:36 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40522 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:54:36 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:54:40 api_server.py:155] waiting for model...
INFO 05-12 13:54:40 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:54:40 metrics.py:276] Avg prompt throughput: 5.2 tokens/s, Avg generation throughput: 13.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40410 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40548 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:54:41 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:54:45 api_server.py:155] waiting for model...
INFO 05-12 13:54:45 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:54:45 metrics.py:276] Avg prompt throughput: 13.6 tokens/s, Avg generation throughput: 41.6 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40520 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:54:46 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:54:50 api_server.py:155] waiting for model...
INFO 05-12 13:54:50 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:54:50 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 45.8 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40544 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:54:51 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:54:55 api_server.py:155] waiting for model...
INFO 05-12 13:54:55 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:54:56 metrics.py:276] Avg prompt throughput: 6.6 tokens/s, Avg generation throughput: 20.1 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40504 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:54:56 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:55:00 api_server.py:155] waiting for model...
INFO 05-12 13:55:00 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40436 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:55:00 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:55:05 api_server.py:155] waiting for model...
INFO 05-12 13:55:05 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:55:05 metrics.py:276] Avg prompt throughput: 1.1 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40566 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:55:05 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:55:10 api_server.py:155] waiting for model...
INFO 05-12 13:55:10 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:55:10 metrics.py:276] Avg prompt throughput: 13.3 tokens/s, Avg generation throughput: 4.1 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40572 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40576 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:55:11 api_server.py:155] waiting for model...
INFO 05-12 13:55:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:55:11 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:55:15 api_server.py:155] waiting for model...
INFO 05-12 13:55:15 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:55:15 metrics.py:276] Avg prompt throughput: 13.4 tokens/s, Avg generation throughput: 32.0 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40563 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:55:15 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 13:55:20 api_server.py:155] waiting for model...
INFO 05-12 13:55:20 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40581 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:55:20 api_server.py:155] waiting for model...
INFO 05-12 13:55:20 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:55:20 metrics.py:276] Avg prompt throughput: 10.2 tokens/s, Avg generation throughput: 19.1 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40524 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:55:20 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:55:25 api_server.py:155] waiting for model...
INFO 05-12 13:55:25 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:55:25 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.6 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40482 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40420 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:55:26 api_server.py:155] waiting for model...
INFO 05-12 13:55:26 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40590 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:55:26 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:55:30 api_server.py:155] waiting for model...
INFO 05-12 13:55:30 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:55:30 metrics.py:276] Avg prompt throughput: 12.1 tokens/s, Avg generation throughput: 47.9 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40586 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:55:31 api_server.py:155] waiting for model...
INFO 05-12 13:55:31 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40532 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:55:31 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:55:36 api_server.py:155] waiting for model...
INFO 05-12 13:55:36 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:55:36 metrics.py:276] Avg prompt throughput: 76.1 tokens/s, Avg generation throughput: 84.3 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40442 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:55:36 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:55:41 api_server.py:155] waiting for model...
INFO 05-12 13:55:41 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:55:41 metrics.py:276] Avg prompt throughput: 17.5 tokens/s, Avg generation throughput: 4.0 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40568 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:55:42 api_server.py:155] waiting for model...
INFO 05-12 13:55:42 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40546 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:55:42 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:55:47 api_server.py:155] waiting for model...
INFO 05-12 13:55:47 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:55:47 metrics.py:276] Avg prompt throughput: 5.5 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40582 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:55:48 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:55:53 api_server.py:155] waiting for model...
INFO 05-12 13:55:53 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:55:53 metrics.py:276] Avg prompt throughput: 6.5 tokens/s, Avg generation throughput: 78.2 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40584 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:55:53 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:55:58 api_server.py:155] waiting for model...
INFO 05-12 13:55:58 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:55:58 metrics.py:276] Avg prompt throughput: 19.8 tokens/s, Avg generation throughput: 35.9 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40534 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:55:59 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:56:03 api_server.py:155] waiting for model...
INFO 05-12 13:56:03 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:56:03 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 79.8 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40554 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:56:04 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:56:09 api_server.py:155] waiting for model...
INFO 05-12 13:56:09 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:56:09 metrics.py:276] Avg prompt throughput: 3.0 tokens/s, Avg generation throughput: 105.9 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40614 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:56:09 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:56:14 api_server.py:155] waiting for model...
INFO 05-12 13:56:14 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:56:14 metrics.py:276] Avg prompt throughput: 2.1 tokens/s, Avg generation throughput: 27.5 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40604 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:56:14 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:56:20 api_server.py:155] waiting for model...
INFO 05-12 13:56:20 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:56:20 metrics.py:276] Avg prompt throughput: 8.5 tokens/s, Avg generation throughput: 3.5 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40600 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:56:20 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:56:24 api_server.py:155] waiting for model...
INFO 05-12 13:56:24 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:56:25 metrics.py:276] Avg prompt throughput: 134.6 tokens/s, Avg generation throughput: 14.6 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40588 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:56:25 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:56:29 api_server.py:155] waiting for model...
INFO 05-12 13:56:29 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:56:30 metrics.py:276] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40598 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:56:30 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:56:35 api_server.py:155] waiting for model...
INFO 05-12 13:56:35 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:56:35 metrics.py:276] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40540 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:56:35 api_server.py:155] waiting for model...
INFO 05-12 13:56:35 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40626 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:56:35 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:56:40 api_server.py:155] waiting for model...
INFO 05-12 13:56:40 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:56:40 metrics.py:276] Avg prompt throughput: 8.0 tokens/s, Avg generation throughput: 37.6 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40602 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:56:40 api_server.py:155] waiting for model...
INFO 05-12 13:56:40 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40536 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:56:41 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:56:46 api_server.py:155] waiting for model...
INFO 05-12 13:56:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:56:46 metrics.py:276] Avg prompt throughput: 6.8 tokens/s, Avg generation throughput: 86.1 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40632 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:56:47 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:56:51 api_server.py:155] waiting for model...
INFO 05-12 13:56:51 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40542 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:56:51 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:56:56 api_server.py:155] waiting for model...
INFO 05-12 13:56:56 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:56:56 metrics.py:276] Avg prompt throughput: 12.7 tokens/s, Avg generation throughput: 14.5 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40516 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:56:57 api_server.py:155] waiting for model...
INFO 05-12 13:56:57 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40510 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40616 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:56:58 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 13:57:02 api_server.py:155] waiting for model...
INFO 05-12 13:57:02 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:57:02 metrics.py:276] Avg prompt throughput: 10.2 tokens/s, Avg generation throughput: 101.8 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40592 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:57:03 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:57:07 api_server.py:155] waiting for model...
INFO 05-12 13:57:07 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:57:07 metrics.py:276] Avg prompt throughput: 4.2 tokens/s, Avg generation throughput: 7.0 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40646 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:57:08 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:57:12 api_server.py:155] waiting for model...
INFO 05-12 13:57:12 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40652 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:57:12 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:57:17 api_server.py:155] waiting for model...
INFO 05-12 13:57:17 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:57:17 metrics.py:276] Avg prompt throughput: 4.5 tokens/s, Avg generation throughput: 15.9 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40606 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:57:18 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:57:23 api_server.py:155] waiting for model...
INFO 05-12 13:57:23 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:57:23 metrics.py:276] Avg prompt throughput: 95.2 tokens/s, Avg generation throughput: 50.8 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40594 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:57:23 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:57:28 api_server.py:155] waiting for model...
INFO 05-12 13:57:28 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:57:28 metrics.py:276] Avg prompt throughput: 4.1 tokens/s, Avg generation throughput: 37.9 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40640 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:57:28 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:57:33 api_server.py:155] waiting for model...
INFO 05-12 13:57:33 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:57:33 metrics.py:276] Avg prompt throughput: 12.5 tokens/s, Avg generation throughput: 33.8 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40560 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:57:33 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:57:38 api_server.py:155] waiting for model...
INFO 05-12 13:57:38 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:57:38 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.2 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40660 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:57:38 api_server.py:155] waiting for model...
INFO 05-12 13:57:38 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40644 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:57:39 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:57:44 api_server.py:155] waiting for model...
INFO 05-12 13:57:44 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:57:44 metrics.py:276] Avg prompt throughput: 24.9 tokens/s, Avg generation throughput: 51.5 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40630 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:57:44 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:57:49 api_server.py:155] waiting for model...
INFO 05-12 13:57:49 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:57:49 metrics.py:276] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 13.1 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40623 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:57:49 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:57:54 api_server.py:155] waiting for model...
INFO 05-12 13:57:54 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:57:54 metrics.py:276] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 27.4 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40648 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:57:55 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:58:00 api_server.py:155] waiting for model...
INFO 05-12 13:58:00 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:58:00 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.2 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40656 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:58:01 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:58:06 api_server.py:155] waiting for model...
INFO 05-12 13:58:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:58:06 metrics.py:276] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 82.9 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40612 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:58:07 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:58:11 api_server.py:155] waiting for model...
INFO 05-12 13:58:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:58:11 metrics.py:276] Avg prompt throughput: 5.8 tokens/s, Avg generation throughput: 74.5 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40608 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40672 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:58:12 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:58:16 api_server.py:155] waiting for model...
INFO 05-12 13:58:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:58:16 metrics.py:276] Avg prompt throughput: 3.3 tokens/s, Avg generation throughput: 36.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40634 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:58:16 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:58:22 api_server.py:155] waiting for model...
INFO 05-12 13:58:22 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:58:22 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40676 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:58:22 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:58:27 api_server.py:155] waiting for model...
INFO 05-12 13:58:27 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:58:27 metrics.py:276] Avg prompt throughput: 8.6 tokens/s, Avg generation throughput: 12.7 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40666 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:58:28 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:58:33 api_server.py:155] waiting for model...
INFO 05-12 13:58:33 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:58:33 metrics.py:276] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40658 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:58:34 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:58:39 api_server.py:155] waiting for model...
INFO 05-12 13:58:39 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:58:39 metrics.py:276] Avg prompt throughput: 74.8 tokens/s, Avg generation throughput: 50.5 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40650 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:58:39 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:58:44 api_server.py:155] waiting for model...
INFO 05-12 13:58:44 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:58:44 metrics.py:276] Avg prompt throughput: 9.1 tokens/s, Avg generation throughput: 19.3 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40682 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:58:44 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 13:58:49 api_server.py:155] waiting for model...
INFO 05-12 13:58:49 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:58:49 metrics.py:276] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 31.8 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40668 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:58:49 api_server.py:155] waiting for model...
INFO 05-12 13:58:49 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40674 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:58:49 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 13:58:54 api_server.py:155] waiting for model...
INFO 05-12 13:58:54 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:58:54 metrics.py:276] Avg prompt throughput: 3.5 tokens/s, Avg generation throughput: 40.7 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40702 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:58:55 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:58:59 api_server.py:155] waiting for model...
INFO 05-12 13:58:59 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:58:59 metrics.py:276] Avg prompt throughput: 8.4 tokens/s, Avg generation throughput: 28.6 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40662 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:59:00 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:59:04 api_server.py:155] waiting for model...
INFO 05-12 13:59:04 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:59:04 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.7 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40574 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:59:05 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:59:10 api_server.py:155] waiting for model...
INFO 05-12 13:59:10 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:59:10 metrics.py:276] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 62.3 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40694 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:59:10 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:59:15 api_server.py:155] waiting for model...
INFO 05-12 13:59:15 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:59:15 metrics.py:276] Avg prompt throughput: 10.3 tokens/s, Avg generation throughput: 3.6 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40692 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:59:15 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:59:20 api_server.py:155] waiting for model...
INFO 05-12 13:59:20 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:59:20 metrics.py:276] Avg prompt throughput: 60.9 tokens/s, Avg generation throughput: 31.9 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40690 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:59:21 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 13:59:26 api_server.py:155] waiting for model...
INFO 05-12 13:59:26 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:59:26 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 101.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40624 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:59:26 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:59:31 api_server.py:155] waiting for model...
INFO 05-12 13:59:31 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:59:31 metrics.py:276] Avg prompt throughput: 13.0 tokens/s, Avg generation throughput: 9.6 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%
INFO:     ::1:40680 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:59:31 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 13:59:36 api_server.py:155] waiting for model...
INFO 05-12 13:59:36 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:59:36 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40718 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:59:36 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:59:41 api_server.py:155] waiting for model...
INFO 05-12 13:59:41 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40710 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40686 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:59:41 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 13:59:46 api_server.py:155] waiting for model...
INFO 05-12 13:59:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:59:46 metrics.py:276] Avg prompt throughput: 4.9 tokens/s, Avg generation throughput: 11.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40712 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:59:46 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 13:59:51 api_server.py:155] waiting for model...
INFO 05-12 13:59:51 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:59:51 metrics.py:276] Avg prompt throughput: 48.1 tokens/s, Avg generation throughput: 24.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40679 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:59:51 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 13:59:56 api_server.py:155] waiting for model...
INFO 05-12 13:59:56 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 13:59:56 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 33.3 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40618 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 13:59:56 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 14:00:01 api_server.py:155] waiting for model...
INFO 05-12 14:00:01 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:00:01 metrics.py:276] Avg prompt throughput: 5.7 tokens/s, Avg generation throughput: 8.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40720 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:00:02 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 14:00:07 api_server.py:155] waiting for model...
INFO 05-12 14:00:07 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:00:07 metrics.py:276] Avg prompt throughput: 2.5 tokens/s, Avg generation throughput: 61.5 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40729 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:00:07 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:00:12 api_server.py:155] waiting for model...
INFO 05-12 14:00:12 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:00:12 metrics.py:276] Avg prompt throughput: 15.7 tokens/s, Avg generation throughput: 24.0 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40732 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:00:12 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 14:00:17 api_server.py:155] waiting for model...
INFO 05-12 14:00:17 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:00:17 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.3 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40706 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:00:18 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 14:00:23 api_server.py:155] waiting for model...
INFO 05-12 14:00:23 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:00:23 metrics.py:276] Avg prompt throughput: 4.3 tokens/s, Avg generation throughput: 22.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40740 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:00:23 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 14:00:28 api_server.py:155] waiting for model...
INFO 05-12 14:00:28 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:00:28 metrics.py:276] Avg prompt throughput: 3.1 tokens/s, Avg generation throughput: 2.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40642 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:00:28 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 14:00:33 api_server.py:155] waiting for model...
INFO 05-12 14:00:33 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:00:33 metrics.py:276] Avg prompt throughput: 11.1 tokens/s, Avg generation throughput: 12.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40734 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:00:33 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:00:38 api_server.py:155] waiting for model...
INFO 05-12 14:00:38 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:00:38 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 48.2 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40726 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:00:38 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 14:00:43 api_server.py:155] waiting for model...
INFO 05-12 14:00:43 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:00:43 metrics.py:276] Avg prompt throughput: 7.8 tokens/s, Avg generation throughput: 30.2 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40760 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:00:44 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:00:48 api_server.py:155] waiting for model...
INFO 05-12 14:00:48 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:00:48 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 46.0 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40758 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:00:49 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 14:00:54 api_server.py:155] waiting for model...
INFO 05-12 14:00:54 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:00:54 metrics.py:276] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 16.7 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40764 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:00:54 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 14:00:59 api_server.py:155] waiting for model...
INFO 05-12 14:00:59 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:00:59 metrics.py:276] Avg prompt throughput: 11.5 tokens/s, Avg generation throughput: 11.9 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40762 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:00:59 api_server.py:155] waiting for model...
INFO 05-12 14:00:59 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40752 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:00:59 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 14:01:04 api_server.py:155] waiting for model...
INFO 05-12 14:01:04 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:01:04 metrics.py:276] Avg prompt throughput: 71.0 tokens/s, Avg generation throughput: 12.1 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40744 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40754 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:01:04 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 14:01:09 api_server.py:155] waiting for model...
INFO 05-12 14:01:09 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:01:09 metrics.py:276] Avg prompt throughput: 18.0 tokens/s, Avg generation throughput: 20.0 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40774 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:01:09 api_server.py:155] waiting for model...
INFO 05-12 14:01:09 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40780 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:01:09 api_server.py:155] waiting for model...
INFO 05-12 14:01:09 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40684 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:01:10 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 14:01:15 api_server.py:155] waiting for model...
INFO 05-12 14:01:15 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:01:15 metrics.py:276] Avg prompt throughput: 4.5 tokens/s, Avg generation throughput: 48.6 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40750 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:01:15 api_server.py:155] waiting for model...
INFO 05-12 14:01:15 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40776 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:01:15 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 14:01:20 api_server.py:155] waiting for model...
INFO 05-12 14:01:20 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:01:20 metrics.py:276] Avg prompt throughput: 7.5 tokens/s, Avg generation throughput: 22.2 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40708 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:01:20 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 14:01:25 api_server.py:155] waiting for model...
INFO 05-12 14:01:25 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:01:25 metrics.py:276] Avg prompt throughput: 26.1 tokens/s, Avg generation throughput: 22.6 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40790 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:01:25 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 14:01:30 api_server.py:155] waiting for model...
INFO 05-12 14:01:30 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:01:30 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 32.4 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40756 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:01:31 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 14:01:35 api_server.py:155] waiting for model...
INFO 05-12 14:01:35 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:01:35 metrics.py:276] Avg prompt throughput: 4.0 tokens/s, Avg generation throughput: 45.0 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40688 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:01:35 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:01:40 api_server.py:155] waiting for model...
INFO 05-12 14:01:40 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:01:40 metrics.py:276] Avg prompt throughput: 18.7 tokens/s, Avg generation throughput: 16.9 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40782 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:01:41 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 14:01:46 api_server.py:155] waiting for model...
INFO 05-12 14:01:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:01:46 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.8 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40766 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:01:46 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 14:01:51 api_server.py:155] waiting for model...
INFO 05-12 14:01:51 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:01:51 metrics.py:276] Avg prompt throughput: 5.1 tokens/s, Avg generation throughput: 48.0 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40700 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:01:52 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:01:57 api_server.py:155] waiting for model...
INFO 05-12 14:01:57 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:01:57 metrics.py:276] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 31.6 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40786 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:01:57 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 14:02:02 api_server.py:155] waiting for model...
INFO 05-12 14:02:02 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:02:02 metrics.py:276] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 60.0 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40806 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:02:03 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 14:02:07 api_server.py:155] waiting for model...
INFO 05-12 14:02:07 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:02:07 metrics.py:276] Avg prompt throughput: 2.2 tokens/s, Avg generation throughput: 22.6 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:02:08 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:02:13 api_server.py:155] waiting for model...
INFO 05-12 14:02:13 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:02:13 metrics.py:276] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 63.7 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40796 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:02:13 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 14:02:18 api_server.py:155] waiting for model...
INFO 05-12 14:02:18 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:02:18 metrics.py:276] Avg prompt throughput: 3.5 tokens/s, Avg generation throughput: 22.1 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40814 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:02:19 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 14:02:23 api_server.py:155] waiting for model...
INFO 05-12 14:02:23 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:02:23 metrics.py:276] Avg prompt throughput: 5.0 tokens/s, Avg generation throughput: 40.0 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40792 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:02:24 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:02:28 api_server.py:155] waiting for model...
INFO 05-12 14:02:28 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:02:28 metrics.py:276] Avg prompt throughput: 5.7 tokens/s, Avg generation throughput: 29.5 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40738 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:02:29 api_server.py:155] waiting for model...
INFO 05-12 14:02:29 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40816 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:02:29 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 14:02:34 api_server.py:155] waiting for model...
INFO 05-12 14:02:34 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:02:34 metrics.py:276] Avg prompt throughput: 11.9 tokens/s, Avg generation throughput: 18.9 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40820 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:02:34 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 14:02:39 api_server.py:155] waiting for model...
INFO 05-12 14:02:39 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:02:39 metrics.py:276] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 11.3 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40748 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40805 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:02:39 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 14:02:44 api_server.py:155] waiting for model...
INFO 05-12 14:02:44 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:02:44 metrics.py:276] Avg prompt throughput: 31.8 tokens/s, Avg generation throughput: 27.6 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40822 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:02:44 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 14:02:49 api_server.py:155] waiting for model...
INFO 05-12 14:02:49 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:02:49 metrics.py:276] Avg prompt throughput: 12.2 tokens/s, Avg generation throughput: 3.0 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40784 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:02:49 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 14:02:54 api_server.py:155] waiting for model...
INFO 05-12 14:02:54 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:02:54 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 22.8 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40830 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:02:56 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 14:03:01 api_server.py:155] waiting for model...
INFO 05-12 14:03:01 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:03:01 metrics.py:276] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 81.0 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40802 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:03:01 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 14:03:06 api_server.py:155] waiting for model...
INFO 05-12 14:03:06 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40834 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:03:06 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 14:03:11 api_server.py:155] waiting for model...
INFO 05-12 14:03:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:03:11 metrics.py:276] Avg prompt throughput: 4.7 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40812 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:03:12 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 14:03:16 api_server.py:155] waiting for model...
INFO 05-12 14:03:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:03:16 metrics.py:276] Avg prompt throughput: 9.9 tokens/s, Avg generation throughput: 62.0 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40826 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:03:17 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 14:03:22 api_server.py:155] waiting for model...
INFO 05-12 14:03:22 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:03:22 metrics.py:276] Avg prompt throughput: 3.5 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40844 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:03:22 api_server.py:155] waiting for model...
INFO 05-12 14:03:22 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40742 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:03:25 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 14:03:29 api_server.py:155] waiting for model...
INFO 05-12 14:03:29 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:03:29 metrics.py:276] Avg prompt throughput: 6.0 tokens/s, Avg generation throughput: 133.7 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40842 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:03:30 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 14:03:34 api_server.py:155] waiting for model...
INFO 05-12 14:03:34 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:03:34 metrics.py:276] Avg prompt throughput: 3.9 tokens/s, Avg generation throughput: 20.7 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40800 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:03:34 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 14:03:39 api_server.py:155] waiting for model...
INFO 05-12 14:03:39 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:03:39 metrics.py:276] Avg prompt throughput: 7.0 tokens/s, Avg generation throughput: 2.8 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40840 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:03:40 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:03:45 api_server.py:155] waiting for model...
INFO 05-12 14:03:45 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:03:45 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 47.4 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40850 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40808 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:03:46 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 14:03:50 api_server.py:155] waiting for model...
INFO 05-12 14:03:50 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:03:50 metrics.py:276] Avg prompt throughput: 4.2 tokens/s, Avg generation throughput: 29.6 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40828 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:03:51 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:03:55 api_server.py:155] waiting for model...
INFO 05-12 14:03:55 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:03:56 metrics.py:276] Avg prompt throughput: 8.8 tokens/s, Avg generation throughput: 11.7 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40854 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:03:56 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 14:04:00 api_server.py:155] waiting for model...
INFO 05-12 14:04:00 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:04:01 metrics.py:276] Avg prompt throughput: 8.2 tokens/s, Avg generation throughput: 16.7 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40839 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:04:02 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:04:07 api_server.py:155] waiting for model...
INFO 05-12 14:04:07 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:04:07 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 73.4 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40858 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:04:08 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 14:04:12 api_server.py:155] waiting for model...
INFO 05-12 14:04:12 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:04:12 metrics.py:276] Avg prompt throughput: 2.9 tokens/s, Avg generation throughput: 43.5 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.0%, CPU KV cache usage: 0.0%
INFO:     ::1:40862 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:04:15 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 14:04:20 api_server.py:155] waiting for model...
INFO 05-12 14:04:20 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:04:20 metrics.py:276] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 100.4 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO:     ::1:40778 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:04:20 async_llm_engine.py:437] Reloading model to delta-4
INFO 05-12 14:04:25 api_server.py:155] waiting for model...
INFO 05-12 14:04:25 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:04:25 metrics.py:276] Avg prompt throughput: 11.0 tokens/s, Avg generation throughput: 16.7 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%
INFO:     ::1:40788 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:04:25 api_server.py:155] waiting for model...
INFO 05-12 14:04:25 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40868 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:04:25 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 14:04:30 api_server.py:155] waiting for model...
INFO 05-12 14:04:30 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:04:30 metrics.py:276] Avg prompt throughput: 8.7 tokens/s, Avg generation throughput: 14.3 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40872 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:04:30 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 14:04:35 api_server.py:155] waiting for model...
INFO 05-12 14:04:35 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:04:35 metrics.py:276] Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 16.9 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40874 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:04:36 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 14:04:41 api_server.py:155] waiting for model...
INFO 05-12 14:04:41 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:04:41 metrics.py:276] Avg prompt throughput: 9.2 tokens/s, Avg generation throughput: 22.8 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40794 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:04:42 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 14:04:46 api_server.py:155] waiting for model...
INFO 05-12 14:04:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:04:46 metrics.py:276] Avg prompt throughput: 4.7 tokens/s, Avg generation throughput: 49.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40824 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:04:46 api_server.py:155] waiting for model...
INFO 05-12 14:04:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40864 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:04:46 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 14:04:52 api_server.py:155] waiting for model...
INFO 05-12 14:04:52 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:04:52 metrics.py:276] Avg prompt throughput: 8.9 tokens/s, Avg generation throughput: 9.5 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40848 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:04:52 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 14:04:57 api_server.py:155] waiting for model...
INFO 05-12 14:04:57 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:04:57 metrics.py:276] Avg prompt throughput: 6.2 tokens/s, Avg generation throughput: 46.4 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40852 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:04:58 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 14:05:03 api_server.py:155] waiting for model...
INFO 05-12 14:05:03 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:05:03 metrics.py:276] Avg prompt throughput: 2.3 tokens/s, Avg generation throughput: 56.1 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40819 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:05:03 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 14:05:08 api_server.py:155] waiting for model...
INFO 05-12 14:05:08 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:05:08 metrics.py:276] Avg prompt throughput: 6.7 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40866 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:05:09 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 14:05:14 api_server.py:155] waiting for model...
INFO 05-12 14:05:14 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:05:14 metrics.py:276] Avg prompt throughput: 3.7 tokens/s, Avg generation throughput: 21.1 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40892 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:05:14 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 14:05:19 api_server.py:155] waiting for model...
INFO 05-12 14:05:19 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:05:19 metrics.py:276] Avg prompt throughput: 1.9 tokens/s, Avg generation throughput: 28.9 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40901 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:05:20 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 14:05:25 api_server.py:155] waiting for model...
INFO 05-12 14:05:25 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:05:25 metrics.py:276] Avg prompt throughput: 5.6 tokens/s, Avg generation throughput: 35.9 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40902 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:05:25 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 14:05:30 api_server.py:155] waiting for model...
INFO 05-12 14:05:30 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:05:30 metrics.py:276] Avg prompt throughput: 7.7 tokens/s, Avg generation throughput: 27.5 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40898 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40882 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:05:30 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:05:35 api_server.py:155] waiting for model...
INFO 05-12 14:05:35 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:05:35 metrics.py:276] Avg prompt throughput: 5.7 tokens/s, Avg generation throughput: 9.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40888 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:05:36 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 14:05:40 api_server.py:155] waiting for model...
INFO 05-12 14:05:40 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:05:40 metrics.py:276] Avg prompt throughput: 4.9 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40860 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:05:41 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 14:05:46 api_server.py:155] waiting for model...
INFO 05-12 14:05:46 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:05:46 metrics.py:276] Avg prompt throughput: 6.3 tokens/s, Avg generation throughput: 40.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40910 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:05:48 api_server.py:155] waiting for model...
INFO 05-12 14:05:48 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40916 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:05:48 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 14:05:53 api_server.py:155] waiting for model...
INFO 05-12 14:05:53 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:05:53 metrics.py:276] Avg prompt throughput: 12.9 tokens/s, Avg generation throughput: 84.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40884 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:05:53 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 14:05:58 api_server.py:155] waiting for model...
INFO 05-12 14:05:58 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:05:58 metrics.py:276] Avg prompt throughput: 3.7 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40922 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:05:59 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 14:06:04 api_server.py:155] waiting for model...
INFO 05-12 14:06:04 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:06:04 metrics.py:276] Avg prompt throughput: 6.5 tokens/s, Avg generation throughput: 67.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40904 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:06:04 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 14:06:09 api_server.py:155] waiting for model...
INFO 05-12 14:06:09 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:06:09 metrics.py:276] Avg prompt throughput: 7.6 tokens/s, Avg generation throughput: 21.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40880 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:06:09 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 14:06:14 api_server.py:155] waiting for model...
INFO 05-12 14:06:14 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:06:14 metrics.py:276] Avg prompt throughput: 12.4 tokens/s, Avg generation throughput: 8.8 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40920 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:06:15 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 14:06:20 api_server.py:155] waiting for model...
INFO 05-12 14:06:20 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:06:20 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 35.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40870 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:06:21 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 14:06:26 api_server.py:155] waiting for model...
INFO 05-12 14:06:26 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:06:26 metrics.py:276] Avg prompt throughput: 3.1 tokens/s, Avg generation throughput: 50.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40913 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:06:26 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 14:06:31 api_server.py:155] waiting for model...
INFO 05-12 14:06:31 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40890 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:06:31 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 14:06:36 api_server.py:155] waiting for model...
INFO 05-12 14:06:36 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:06:36 metrics.py:276] Avg prompt throughput: 6.8 tokens/s, Avg generation throughput: 9.9 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40886 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:06:36 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 14:06:41 api_server.py:155] waiting for model...
INFO 05-12 14:06:41 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:06:41 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%
INFO:     ::1:40914 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:06:41 api_server.py:155] waiting for model...
INFO 05-12 14:06:41 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40942 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:06:44 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:06:49 api_server.py:155] waiting for model...
INFO 05-12 14:06:49 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:06:49 metrics.py:276] Avg prompt throughput: 15.7 tokens/s, Avg generation throughput: 109.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40934 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:06:49 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 14:06:54 api_server.py:155] waiting for model...
INFO 05-12 14:06:54 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:06:54 metrics.py:276] Avg prompt throughput: 3.5 tokens/s, Avg generation throughput: 8.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40946 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:06:54 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 14:06:59 api_server.py:155] waiting for model...
INFO 05-12 14:06:59 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:06:59 metrics.py:276] Avg prompt throughput: 3.7 tokens/s, Avg generation throughput: 27.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40932 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:06:59 api_server.py:155] waiting for model...
INFO 05-12 14:06:59 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40952 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:07:01 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 14:07:05 api_server.py:155] waiting for model...
INFO 05-12 14:07:05 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:07:05 metrics.py:276] Avg prompt throughput: 7.0 tokens/s, Avg generation throughput: 75.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40928 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:07:06 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 14:07:10 api_server.py:155] waiting for model...
INFO 05-12 14:07:11 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:07:11 metrics.py:276] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 18.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40938 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:07:11 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 14:07:16 api_server.py:155] waiting for model...
INFO 05-12 14:07:16 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:07:16 metrics.py:276] Avg prompt throughput: 8.6 tokens/s, Avg generation throughput: 44.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40948 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:07:17 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:07:21 api_server.py:155] waiting for model...
INFO 05-12 14:07:21 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:07:21 metrics.py:276] Avg prompt throughput: 3.1 tokens/s, Avg generation throughput: 20.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40960 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:07:22 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 14:07:28 api_server.py:155] waiting for model...
INFO 05-12 14:07:28 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:07:28 metrics.py:276] Avg prompt throughput: 4.8 tokens/s, Avg generation throughput: 27.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40908 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:07:28 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 14:07:33 api_server.py:155] waiting for model...
INFO 05-12 14:07:33 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:07:33 metrics.py:276] Avg prompt throughput: 1.9 tokens/s, Avg generation throughput: 10.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40962 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:07:33 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 14:07:38 api_server.py:155] waiting for model...
INFO 05-12 14:07:38 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:07:38 metrics.py:276] Avg prompt throughput: 13.1 tokens/s, Avg generation throughput: 11.1 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40926 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:07:38 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 14:07:43 api_server.py:155] waiting for model...
INFO 05-12 14:07:43 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:07:43 metrics.py:276] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 17.5 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40930 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:07:43 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:07:48 api_server.py:155] waiting for model...
INFO 05-12 14:07:48 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:07:48 metrics.py:276] Avg prompt throughput: 4.2 tokens/s, Avg generation throughput: 8.8 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40956 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:07:48 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 14:07:54 api_server.py:155] waiting for model...
INFO 05-12 14:07:54 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:07:54 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40972 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:07:54 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:07:59 api_server.py:155] waiting for model...
INFO 05-12 14:07:59 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:07:59 metrics.py:276] Avg prompt throughput: 1.9 tokens/s, Avg generation throughput: 6.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40976 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:07:59 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 14:08:04 api_server.py:155] waiting for model...
INFO 05-12 14:08:04 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:08:04 metrics.py:276] Avg prompt throughput: 4.1 tokens/s, Avg generation throughput: 6.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.5%, CPU KV cache usage: 0.0%
INFO:     ::1:40974 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:08:04 api_server.py:155] waiting for model...
INFO 05-12 14:08:04 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40941 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:08:04 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:08:09 api_server.py:155] waiting for model...
INFO 05-12 14:08:09 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:08:09 metrics.py:276] Avg prompt throughput: 133.5 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40958 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:08:10 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 14:08:15 api_server.py:155] waiting for model...
INFO 05-12 14:08:15 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:08:15 metrics.py:276] Avg prompt throughput: 5.1 tokens/s, Avg generation throughput: 30.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40936 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:08:15 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 14:08:20 api_server.py:155] waiting for model...
INFO 05-12 14:08:20 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:08:20 metrics.py:276] Avg prompt throughput: 9.2 tokens/s, Avg generation throughput: 4.6 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40924 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:08:20 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 14:08:24 api_server.py:155] waiting for model...
INFO 05-12 14:08:24 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:08:25 metrics.py:276] Avg prompt throughput: 60.2 tokens/s, Avg generation throughput: 17.6 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40950 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:08:25 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 14:08:30 api_server.py:155] waiting for model...
INFO 05-12 14:08:30 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:08:30 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 42.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:40954 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:08:32 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 14:08:37 api_server.py:155] waiting for model...
INFO 05-12 14:08:37 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:08:37 metrics.py:276] Avg prompt throughput: 2.4 tokens/s, Avg generation throughput: 61.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40964 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:08:37 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:08:42 api_server.py:155] waiting for model...
INFO 05-12 14:08:42 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:08:42 metrics.py:276] Avg prompt throughput: 65.2 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40966 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:08:42 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 14:08:47 api_server.py:155] waiting for model...
INFO 05-12 14:08:47 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:08:47 metrics.py:276] Avg prompt throughput: 2.5 tokens/s, Avg generation throughput: 34.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40996 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:08:48 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:08:53 api_server.py:155] waiting for model...
INFO 05-12 14:08:53 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:08:53 metrics.py:276] Avg prompt throughput: 2.9 tokens/s, Avg generation throughput: 12.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40984 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:08:53 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 14:08:58 api_server.py:155] waiting for model...
INFO 05-12 14:08:58 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:08:58 metrics.py:276] Avg prompt throughput: 4.2 tokens/s, Avg generation throughput: 12.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:41000 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:08:58 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 14:09:03 api_server.py:155] waiting for model...
INFO 05-12 14:09:03 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:09:03 metrics.py:276] Avg prompt throughput: 4.9 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:41002 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:09:03 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 14:09:08 api_server.py:155] waiting for model...
INFO 05-12 14:09:08 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:09:08 metrics.py:276] Avg prompt throughput: 15.7 tokens/s, Avg generation throughput: 8.9 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:41004 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:09:08 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 14:09:13 api_server.py:155] waiting for model...
INFO 05-12 14:09:13 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:40968 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:09:13 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 14:09:18 api_server.py:155] waiting for model...
INFO 05-12 14:09:18 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:09:18 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.4 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:41012 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:09:19 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 14:09:24 api_server.py:155] waiting for model...
INFO 05-12 14:09:24 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:09:24 metrics.py:276] Avg prompt throughput: 9.2 tokens/s, Avg generation throughput: 37.5 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:40994 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:09:24 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 14:09:29 api_server.py:155] waiting for model...
INFO 05-12 14:09:29 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:09:29 metrics.py:276] Avg prompt throughput: 3.3 tokens/s, Avg generation throughput: 4.2 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40978 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:09:29 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 14:09:34 api_server.py:155] waiting for model...
INFO 05-12 14:09:34 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:09:34 metrics.py:276] Avg prompt throughput: 5.2 tokens/s, Avg generation throughput: 17.7 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:41018 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:09:34 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 14:09:39 api_server.py:155] waiting for model...
INFO 05-12 14:09:39 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:09:39 metrics.py:276] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 8.8 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:41010 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:09:40 async_llm_engine.py:437] Reloading model to delta-1
INFO 05-12 14:09:45 api_server.py:155] waiting for model...
INFO 05-12 14:09:45 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:09:45 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 23.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:41024 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:09:45 async_llm_engine.py:437] Reloading model to delta-8
INFO 05-12 14:09:50 api_server.py:155] waiting for model...
INFO 05-12 14:09:50 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:09:50 metrics.py:276] Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 29.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.6%, CPU KV cache usage: 0.0%
INFO:     ::1:41020 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:09:50 api_server.py:155] waiting for model...
INFO 05-12 14:09:50 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO:     ::1:41022 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:09:51 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 14:09:55 api_server.py:155] waiting for model...
INFO 05-12 14:09:55 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:09:55 metrics.py:276] Avg prompt throughput: 30.6 tokens/s, Avg generation throughput: 24.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:40992 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:09:56 async_llm_engine.py:437] Reloading model to delta-2
INFO 05-12 14:10:01 api_server.py:155] waiting for model...
INFO 05-12 14:10:01 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:10:01 metrics.py:276] Avg prompt throughput: 4.1 tokens/s, Avg generation throughput: 46.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:41032 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:10:03 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 14:10:08 api_server.py:155] waiting for model...
INFO 05-12 14:10:08 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:10:08 metrics.py:276] Avg prompt throughput: 5.7 tokens/s, Avg generation throughput: 65.3 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%
INFO:     ::1:41028 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:10:08 async_llm_engine.py:437] Reloading model to delta-7
INFO 05-12 14:10:13 api_server.py:155] waiting for model...
INFO 05-12 14:10:13 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:10:13 metrics.py:276] Avg prompt throughput: 3.3 tokens/s, Avg generation throughput: 8.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40987 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:10:14 async_llm_engine.py:437] Reloading model to delta-6
INFO 05-12 14:10:19 api_server.py:155] waiting for model...
INFO 05-12 14:10:19 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:10:19 metrics.py:276] Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 36.0 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40998 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:10:19 async_llm_engine.py:437] Reloading model to /vllm/.idea/full_models/Llama-2-13b-hf
INFO 05-12 14:10:25 api_server.py:155] waiting for model...
INFO 05-12 14:10:25 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:10:25 metrics.py:276] Avg prompt throughput: 2.5 tokens/s, Avg generation throughput: 23.8 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:41016 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:10:26 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 14:10:31 api_server.py:155] waiting for model...
INFO 05-12 14:10:31 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:10:31 metrics.py:276] Avg prompt throughput: 3.7 tokens/s, Avg generation throughput: 61.6 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:40990 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:10:31 async_llm_engine.py:437] Reloading model to delta-5
INFO 05-12 14:10:36 api_server.py:155] waiting for model...
INFO 05-12 14:10:36 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:10:36 metrics.py:276] Avg prompt throughput: 10.1 tokens/s, Avg generation throughput: 13.1 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%
INFO:     ::1:41014 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:10:38 async_llm_engine.py:437] Reloading model to delta-3
INFO 05-12 14:10:43 api_server.py:155] waiting for model...
INFO 05-12 14:10:43 api_server.py:163] requested model is loaded --> continuing...
Prefetching is disabled
INFO 05-12 14:10:43 metrics.py:276] Avg prompt throughput: 3.1 tokens/s, Avg generation throughput: 73.9 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%
INFO:     ::1:41044 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40980 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:41036 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:41040 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:40988 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:41026 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:10:48 metrics.py:276] Avg prompt throughput: 4.2 tokens/s, Avg generation throughput: 190.8 tokens/s, Running: 5 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.4%, CPU KV cache usage: 0.0%
INFO:     ::1:41050 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:41042 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:41034 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 05-12 14:10:53 metrics.py:276] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.2 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%
INFO:     ::1:41048 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     ::1:41052 - "POST /v1/completions HTTP/1.1" 200 OK
Results written to .artifact/benchmarks/results/a4925670-7d77-49ae-a3a4-5559ddcf72f0.jsonl
Killing process 102046...
/var/spool/slurmd/job3671407/slurm_script: line 42: 102012 Killed                  apptainer run --nv --home /mnt/petrelfs/huqinghao/xzyao/:/home/xiayao --bind /mnt/petrelfs/huqinghao/xzyao/code/vllm:/vllm --bind /mnt/petrelfs/huqinghao/xzyao/code/triteia:/triteia --env PYTHONPATH=/vllm:/triteia --env CUDA_LAUNCH_BLOCKING=1 --env TVM_TARGET=nvidia/nvidia-a100 --env BITBLAS_TARGET=nvidia/nvidia-a100 --env RAY_DEDUP_LOGS=0 --workdir /vllm /mnt/petrelfs/huqinghao/xzyao/images/deltaserve.sif python3 /vllm/vllm/entrypoints/openai/api_server.py --model /vllm/.idea/full_models/Llama-2-13b-hf --disable-log-requests --gpu-memory-utilization 0.85 --swap-modules delta-1=/vllm/.idea/full_models/Llama-2-13b-hf-1 delta-2=/vllm/.idea/full_models/Llama-2-13b-hf-2 delta-3=/vllm/.idea/full_models/Llama-2-13b-hf-3 delta-4=/vllm/.idea/full_models/Llama-2-13b-hf-4 delta-5=/vllm/.idea/full_models/Llama-2-13b-hf-5 delta-6=/vllm/.idea/full_models/Llama-2-13b-hf-6 delta-7=/vllm/.idea/full_models/Llama-2-13b-hf-7 delta-8=/vllm/.idea/full_models/Llama-2-13b-hf-8 --tensor-parallel-size 4 --enforce-eager --max-deltas 0
