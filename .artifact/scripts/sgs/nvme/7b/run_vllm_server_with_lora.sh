python -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-2-7b-hf --host 0.0.0.0 --enable-lora --disable-log-requests --gpu-memory-utilization 0.85 \
--lora-modules lora-1=/mnt/scratch/xiayao/cache/models/lora/llama-7b-hf-lora-r16-1 lora-2=/mnt/scratch/xiayao/cache/models/lora/llama-7b-hf-lora-r16-2 lora-3=/mnt/scratch/xiayao/cache/models/lora/llama-7b-hf-lora-r16-3 lora-4=/mnt/scratch/xiayao/cache/models/lora/llama-7b-hf-lora-r16-4 lora-5=/mnt/scratch/xiayao/cache/models/lora/llama-7b-hf-lora-r16-5 lora-6=/mnt/scratch/xiayao/cache/models/lora/llama-7b-hf-lora-r16-6 lora-7=/mnt/scratch/xiayao/cache/models/lora/llama-7b-hf-lora-r16-7 lora-8=/mnt/scratch/xiayao/cache/models/lora/llama-7b-hf-lora-r16-8 lora-9=/mnt/scratch/xiayao/cache/models/lora/llama-7b-hf-lora-r16-9 lora-10=/mnt/scratch/xiayao/cache/models/lora/llama-7b-hf-lora-r16-10 lora-11=/mnt/scratch/xiayao/cache/models/lora/llama-7b-hf-lora-r16-11 lora-12=/mnt/scratch/xiayao/cache/models/lora/llama-7b-hf-lora-r16-12 lora-13=/mnt/scratch/xiayao/cache/models/lora/llama-7b-hf-lora-r16-13 lora-14=/mnt/scratch/xiayao/cache/models/lora/llama-7b-hf-lora-r16-14 lora-15=/mnt/scratch/xiayao/cache/models/lora/llama-7b-hf-lora-r16-15 lora-16=/mnt/scratch/xiayao/cache/models/lora/llama-7b-hf-lora-r16-16 \
--max-loras 4 --max-cpu-loras 32 --tensor-parallel-size 1 \
--enforce-eager --port 8081