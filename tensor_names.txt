Key: lm_head.weight, shape: torch.Size([32000, 4096]), dtype: torch.float16, range: [-0.02386474609375, 0.022796630859375]
Key: model.embed_tokens.weight, shape: torch.Size([32000, 4096]), dtype: torch.float16, range: [-0.00650787353515625, 0.00640869140625]
Key: model.layers.0.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0052490234375, 0.0030670166015625]
Key: model.layers.0.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.0.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2147448616, 2146936695]
Key: model.layers.0.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109466, 2021090935]
Key: model.layers.0.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00040721893310546875, 0.0008950233459472656]
Key: model.layers.0.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.0.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146925688, 2147121015]
Key: model.layers.0.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109465, 2021095286]
Key: model.layers.0.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00028133392333984375, 0.0007700920104980469]
Key: model.layers.0.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.0.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146940792, 2146940791]
Key: model.layers.0.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2056820889, 2022008439]
Key: model.layers.0.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00031876564025878906, 0.0011491775512695312]
Key: model.layers.0.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00482177734375, 0.003204345703125]
Key: model.layers.0.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.0.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147448696, 2147448695]
Key: model.layers.0.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2056878217, 2021160535]
Key: model.layers.0.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00010973215103149414, 0.0008544921875]
Key: model.layers.0.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.0.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147473272, 2147469231]
Key: model.layers.0.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105353, 2021029478]
Key: model.layers.0.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00029397010803222656, 0.0005574226379394531]
Key: model.layers.0.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.0.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147186680, 2146506615]
Key: model.layers.0.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2072615050, 2020042615]
Key: model.layers.0.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00015401840209960938, 0.0008058547973632812]
Key: model.layers.0.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.0.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924408, 2146994232]
Key: model.layers.0.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2055768457, 2021226087]
Key: model.layers.0.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00025010108947753906, 0.000652313232421875]
Key: model.layers.1.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.005859375, 0.00250244140625]
Key: model.layers.1.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.1.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2145875835, 2146924407]
Key: model.layers.1.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043674, 2021029734]
Key: model.layers.1.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00041294097900390625, 0.0009765625]
Key: model.layers.1.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.1.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147186555, 2147252087]
Key: model.layers.1.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2041088154, 2021156470]
Key: model.layers.1.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003173351287841797, 0.0006175041198730469]
Key: model.layers.1.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.1.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147186552, 2147100615]
Key: model.layers.1.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109465, 2021099383]
Key: model.layers.1.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0002868175506591797, 0.0010175704956054688]
Key: model.layers.1.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.003662109375, 0.003173828125]
Key: model.layers.1.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.1.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147018889, 2147234953]
Key: model.layers.1.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2057804170, 2021095287]
Key: model.layers.1.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00018966197967529297, 0.0007848739624023438]
Key: model.layers.1.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.1.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147448701, 2147450743]
Key: model.layers.1.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2055837850, 2019977078]
Key: model.layers.1.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003275871276855469, 0.0006461143493652344]
Key: model.layers.1.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.1.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147384952, 2147438919]
Key: model.layers.1.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2072492985, 2037675926]
Key: model.layers.1.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00026798248291015625, 0.0008244514465332031]
Key: model.layers.1.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.1.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147009417, 2147160178]
Key: model.layers.1.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040048009, 2021095030]
Key: model.layers.1.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00028896331787109375, 0.00083160400390625]
Key: model.layers.10.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00390625, 0.003173828125]
Key: model.layers.10.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.10.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2146016366, 2145875835]
Key: model.layers.10.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109194, 2021029750]
Key: model.layers.10.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00041413307189941406, 0.0006847381591796875]
Key: model.layers.10.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.10.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146137976, 2146948983]
Key: model.layers.10.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109465, 2021099110]
Key: model.layers.10.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003590583801269531, 0.0007424354553222656]
Key: model.layers.10.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.10.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146924413, 2147317623]
Key: model.layers.10.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2041158042, 2021091174]
Key: model.layers.10.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003609657287597656, 0.0007443428039550781]
Key: model.layers.10.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0029296875, 0.00439453125]
Key: model.layers.10.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.10.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147252093, 2146924407]
Key: model.layers.10.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105369, 2021095271]
Key: model.layers.10.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00031638145446777344, 0.000743865966796875]
Key: model.layers.10.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.10.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146936696, 2146924407]
Key: model.layers.10.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2055768458, 2036754551]
Key: model.layers.10.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003552436828613281, 0.0006041526794433594]
Key: model.layers.10.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.10.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2145876600, 2145888119]
Key: model.layers.10.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043673, 2021095015]
Key: model.layers.10.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00030422210693359375, 0.0007138252258300781]
Key: model.layers.10.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.10.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146925944, 2144828743]
Key: model.layers.10.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2056886426, 2021029735]
Key: model.layers.10.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003113746643066406, 0.0005826950073242188]
Key: model.layers.11.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0029296875, 0.003662109375]
Key: model.layers.11.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.11.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2146923320, 2144967070]
Key: model.layers.11.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105370, 2021095287]
Key: model.layers.11.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00043964385986328125, 0.000766754150390625]
Key: model.layers.11.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.11.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147252088, 2147121015]
Key: model.layers.11.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040174986, 2021090679]
Key: model.layers.11.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00034999847412109375, 0.0009870529174804688]
Key: model.layers.11.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.11.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146236344, 2146924407]
Key: model.layers.11.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040105368, 2021095030]
Key: model.layers.11.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00034999847412109375, 0.0009226799011230469]
Key: model.layers.11.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.003173828125, 0.0040283203125]
Key: model.layers.11.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.11.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147317624, 2146924487]
Key: model.layers.11.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109193, 2021029751]
Key: model.layers.11.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003509521484375, 0.0007205009460449219]
Key: model.layers.11.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.11.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147315768, 2147448647]
Key: model.layers.11.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109194, 2021090935]
Key: model.layers.11.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00036334991455078125, 0.0007100105285644531]
Key: model.layers.11.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.11.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147251064, 2146912884]
Key: model.layers.11.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040039562, 2021025639]
Key: model.layers.11.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003325939178466797, 0.0007491111755371094]
Key: model.layers.11.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.11.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2145875827, 2146903927]
Key: model.layers.11.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2055833738, 2020042342]
Key: model.layers.11.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003323554992675781, 0.0006661415100097656]
Key: model.layers.12.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.003173828125, 0.00341796875]
Key: model.layers.12.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.12.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2144827256, 2144946585]
Key: model.layers.12.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105098, 2020046710]
Key: model.layers.12.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0004382133483886719, 0.0007548332214355469]
Key: model.layers.12.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.12.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146924488, 2146940791]
Key: model.layers.12.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040105114, 2021095270]
Key: model.layers.12.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003426074981689453, 0.0008640289306640625]
Key: model.layers.12.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.12.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147317624, 2146925171]
Key: model.layers.12.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2055837834, 2022205046]
Key: model.layers.12.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003712177276611328, 0.0010614395141601562]
Key: model.layers.12.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0032958984375, 0.004150390625]
Key: model.layers.12.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.12.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2145482696, 2146924407]
Key: model.layers.12.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043641, 2021095270]
Key: model.layers.12.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00032210350036621094, 0.0007538795471191406]
Key: model.layers.12.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.12.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147166168, 2146924400]
Key: model.layers.12.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040113306, 2022139766]
Key: model.layers.12.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003647804260253906, 0.0006513595581054688]
Key: model.layers.12.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.12.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146925112, 2146924402]
Key: model.layers.12.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040040074, 2021029734]
Key: model.layers.12.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003249645233154297, 0.0007376670837402344]
Key: model.layers.12.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.12.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146950008, 2147132030]
Key: model.layers.12.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039056777, 2021090919]
Key: model.layers.12.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.000324249267578125, 0.0006389617919921875]
Key: model.layers.13.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00341796875, 0.003662109375]
Key: model.layers.13.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.13.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2144827256, 2145163641]
Key: model.layers.13.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2023328137, 2021095287]
Key: model.layers.13.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00044655799865722656, 0.0009765625]
Key: model.layers.13.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.13.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147198840, 2147121020]
Key: model.layers.13.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2055833753, 2036819830]
Key: model.layers.13.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003707408905029297, 0.0009579658508300781]
Key: model.layers.13.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.13.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147186637, 2146994232]
Key: model.layers.13.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040105354, 2021099127]
Key: model.layers.13.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003402233123779297, 0.0009102821350097656]
Key: model.layers.13.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00244140625, 0.004150390625]
Key: model.layers.13.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.13.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146925944, 2146924343]
Key: model.layers.13.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105097, 2021091174]
Key: model.layers.13.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00030612945556640625, 0.0008778572082519531]
Key: model.layers.13.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.13.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147432568, 2147448695]
Key: model.layers.13.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105371, 2021091174]
Key: model.layers.13.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003719329833984375, 0.00101470947265625]
Key: model.layers.13.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.13.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147006713, 2146924401]
Key: model.layers.13.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2056882330, 2021025399]
Key: model.layers.13.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00031280517578125, 0.0008730888366699219]
Key: model.layers.13.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.13.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2144897161, 2146924407]
Key: model.layers.13.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109466, 2021091174]
Key: model.layers.13.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003294944763183594, 0.0007276535034179688]
Key: model.layers.14.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.003173828125, 0.00390625]
Key: model.layers.14.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.14.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2144827256, 2144827207]
Key: model.layers.14.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105354, 2021095269]
Key: model.layers.14.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00043582916259765625, 0.0006933212280273438]
Key: model.layers.14.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.14.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146939000, 2146924407]
Key: model.layers.14.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109466, 2021090934]
Key: model.layers.14.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00033020973205566406, 0.0007505416870117188]
Key: model.layers.14.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.14.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146940796, 2146924487]
Key: model.layers.14.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109466, 2021091190]
Key: model.layers.14.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00035309791564941406, 0.0007829666137695312]
Key: model.layers.14.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.002197265625, 0.00439453125]
Key: model.layers.14.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.14.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924412, 2146925943]
Key: model.layers.14.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105098, 2020042598]
Key: model.layers.14.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003285408020019531, 0.0007328987121582031]
Key: model.layers.14.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.14.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147338104, 2146269111]
Key: model.layers.14.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2056816778, 2020042630]
Key: model.layers.14.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003638267517089844, 0.0007090568542480469]
Key: model.layers.14.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.14.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146940792, 2146947639]
Key: model.layers.14.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109449, 2021091191]
Key: model.layers.14.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00032639503479003906, 0.0008797645568847656]
Key: model.layers.14.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.14.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2145876604, 2145897335]
Key: model.layers.14.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2055772570, 2021099383]
Key: model.layers.14.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003218650817871094, 0.0006084442138671875]
Key: model.layers.15.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.002685546875, 0.004150390625]
Key: model.layers.15.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.15.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2144827256, 2145875831]
Key: model.layers.15.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040039577, 2020046438]
Key: model.layers.15.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00043773651123046875, 0.0007238388061523438]
Key: model.layers.15.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.15.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147080538, 2147252007]
Key: model.layers.15.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2056816761, 2022078054]
Key: model.layers.15.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003802776336669922, 0.0008168220520019531]
Key: model.layers.15.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.15.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146924408, 2147383159]
Key: model.layers.15.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2041157770, 2021095287]
Key: model.layers.15.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003476142883300781, 0.0007157325744628906]
Key: model.layers.15.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.002197265625, 0.005126953125]
Key: model.layers.15.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.15.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147185272, 2146925682]
Key: model.layers.15.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039060618, 2021095287]
Key: model.layers.15.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00030732154846191406, 0.0008230209350585938]
Key: model.layers.15.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.15.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146925436, 2146973915]
Key: model.layers.15.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105114, 2021025399]
Key: model.layers.15.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003809928894042969, 0.0008115768432617188]
Key: model.layers.15.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.15.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147251064, 2146924410]
Key: model.layers.15.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105113, 2021095014]
Key: model.layers.15.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003001689910888672, 0.0007810592651367188]
Key: model.layers.15.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.15.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146951800, 2146925383]
Key: model.layers.15.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109210, 2019976822]
Key: model.layers.15.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00032448768615722656, 0.0006184577941894531]
Key: model.layers.16.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0029296875, 0.0048828125]
Key: model.layers.16.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.16.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2146530136, 2146015641]
Key: model.layers.16.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040039561, 2021029734]
Key: model.layers.16.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00043702125549316406, 0.0007104873657226562]
Key: model.layers.16.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.16.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147186552, 2147104631]
Key: model.layers.16.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109466, 2022143606]
Key: model.layers.16.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00036835670471191406, 0.0008788108825683594]
Key: model.layers.16.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.16.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147448771, 2146946871]
Key: model.layers.16.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109466, 2021095031]
Key: model.layers.16.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00035262107849121094, 0.0008301734924316406]
Key: model.layers.16.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0028076171875, 0.007080078125]
Key: model.layers.16.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.16.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924408, 2146203511]
Key: model.layers.16.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109193, 2020046711]
Key: model.layers.16.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003101825714111328, 0.0007596015930175781]
Key: model.layers.16.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.16.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147239800, 2147253879]
Key: model.layers.16.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2056755610, 2020042360]
Key: model.layers.16.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003981590270996094, 0.0008587837219238281]
Key: model.layers.16.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.16.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924408, 2146924407]
Key: model.layers.16.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039060889, 2021095286]
Key: model.layers.16.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0002903938293457031, 0.0007691383361816406]
Key: model.layers.16.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.16.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146269042, 2144827362]
Key: model.layers.16.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105369, 2021095271]
Key: model.layers.16.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00032901763916015625, 0.00064849853515625]
Key: model.layers.17.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.002685546875, 0.00341796875]
Key: model.layers.17.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.17.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2144967098, 2145875831]
Key: model.layers.17.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109210, 2021029478]
Key: model.layers.17.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00043082237243652344, 0.00075531005859375]
Key: model.layers.17.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.17.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146936696, 2146924407]
Key: model.layers.17.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2056821129, 2021091175]
Key: model.layers.17.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.000377655029296875, 0.0007772445678710938]
Key: model.layers.17.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.17.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146908024, 2146925431]
Key: model.layers.17.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040043914, 2021090935]
Key: model.layers.17.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00034689903259277344, 0.0008444786071777344]
Key: model.layers.17.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0029296875, 0.004150390625]
Key: model.layers.17.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.17.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146785141, 2147202935]
Key: model.layers.17.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105370, 2019977078]
Key: model.layers.17.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003020763397216797, 0.0007534027099609375]
Key: model.layers.17.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.17.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146925688, 2146993032]
Key: model.layers.17.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105114, 2021025639]
Key: model.layers.17.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003993511199951172, 0.0006489753723144531]
Key: model.layers.17.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.17.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924408, 2146686834]
Key: model.layers.17.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109210, 2021090918]
Key: model.layers.17.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003113746643066406, 0.0007901191711425781]
Key: model.layers.17.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.17.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924408, 2145875831]
Key: model.layers.17.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105114, 2021090935]
Key: model.layers.17.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003361701965332031, 0.0006432533264160156]
Key: model.layers.18.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0029296875, 0.004150390625]
Key: model.layers.18.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.18.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2143790968, 2146015129]
Key: model.layers.18.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109465, 2020046695]
Key: model.layers.18.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0004470348358154297, 0.000762939453125]
Key: model.layers.18.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.18.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147186552, 2146937719]
Key: model.layers.18.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2055702937, 2021095015]
Key: model.layers.18.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003662109375, 0.0007281303405761719]
Key: model.layers.18.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.18.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147198843, 2147252087]
Key: model.layers.18.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109450, 2021095286]
Key: model.layers.18.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00036334991455078125, 0.0006976127624511719]
Key: model.layers.18.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00244140625, 0.00439453125]
Key: model.layers.18.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.18.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924408, 2146925687]
Key: model.layers.18.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109209, 2022143591]
Key: model.layers.18.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00030517578125, 0.0007486343383789062]
Key: model.layers.18.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.18.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924408, 2147382391]
Key: model.layers.18.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039060634, 2021090918]
Key: model.layers.18.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0004146099090576172, 0.0006937980651855469]
Key: model.layers.18.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.18.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147186600, 2146913143]
Key: model.layers.18.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2023336090, 2021090935]
Key: model.layers.18.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003101825714111328, 0.0007071495056152344]
Key: model.layers.18.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.18.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924520, 2145888119]
Key: model.layers.18.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040113306, 2021095287]
Key: model.layers.18.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003294944763183594, 0.0006227493286132812]
Key: model.layers.19.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.002197265625, 0.0048828125]
Key: model.layers.19.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.19.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2145156216, 2145863591]
Key: model.layers.19.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043657, 2005362263]
Key: model.layers.19.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00045013427734375, 0.0006794929504394531]
Key: model.layers.19.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.19.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147153784, 2147121079]
Key: model.layers.19.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109194, 2021091174]
Key: model.layers.19.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003173351287841797, 0.0011072158813476562]
Key: model.layers.19.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.19.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147252088, 2147338039]
Key: model.layers.19.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109465, 2021095031]
Key: model.layers.19.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00033164024353027344, 0.0010986328125]
Key: model.layers.19.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00244140625, 0.008544921875]
Key: model.layers.19.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.19.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924408, 2146465578]
Key: model.layers.19.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039060618, 2019981174]
Key: model.layers.19.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00030517578125, 0.0007877349853515625]
Key: model.layers.19.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.19.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146925368, 2146940791]
Key: model.layers.19.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105369, 2021095271]
Key: model.layers.19.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00041604042053222656, 0.0006670951843261719]
Key: model.layers.19.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.19.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146948984, 2146940794]
Key: model.layers.19.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043658, 2021029479]
Key: model.layers.19.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0002942085266113281, 0.0007443428039550781]
Key: model.layers.19.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.19.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146529656, 2145876855]
Key: model.layers.19.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043658, 2021029751]
Key: model.layers.19.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0002961158752441406, 0.0006437301635742188]
Key: model.layers.2.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00439453125, 0.0025634765625]
Key: model.layers.2.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.2.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2146923643, 2143778683]
Key: model.layers.2.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105098, 2021091190]
Key: model.layers.2.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00039267539978027344, 0.0007414817810058594]
Key: model.layers.2.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.2.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147187576, 2146924407]
Key: model.layers.2.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040105369, 2021095030]
Key: model.layers.2.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003266334533691406, 0.0006327629089355469]
Key: model.layers.2.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.2.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147186664, 2147317543]
Key: model.layers.2.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040113289, 2021091190]
Key: model.layers.2.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003008842468261719, 0.0007038116455078125]
Key: model.layers.2.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0029296875, 0.0030517578125]
Key: model.layers.2.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.2.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147121012, 2147186631]
Key: model.layers.2.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105370, 2021160325]
Key: model.layers.2.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00024819374084472656, 0.0007004737854003906]
Key: model.layers.2.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.2.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147428344, 2147250551]
Key: model.layers.2.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109449, 2021095271]
Key: model.layers.2.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003685951232910156, 0.0007166862487792969]
Key: model.layers.2.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.2.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146908024, 2146924411]
Key: model.layers.2.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105114, 2021099126]
Key: model.layers.2.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0002892017364501953, 0.0009317398071289062]
Key: model.layers.2.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.2.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147252088, 2147448695]
Key: model.layers.2.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109450, 2021029494]
Key: model.layers.2.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003371238708496094, 0.0006427764892578125]
Key: model.layers.20.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0029296875, 0.0048828125]
Key: model.layers.20.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.20.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2146150264, 2146924405]
Key: model.layers.20.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043913, 2021091174]
Key: model.layers.20.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00044274330139160156, 0.0006856918334960938]
Key: model.layers.20.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.20.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146924408, 2147317623]
Key: model.layers.20.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109465, 2021091175]
Key: model.layers.20.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003540515899658203, 0.00069427490234375]
Key: model.layers.20.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.20.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146924408, 2146924407]
Key: model.layers.20.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040108938, 2021095014]
Key: model.layers.20.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003478527069091797, 0.0006766319274902344]
Key: model.layers.20.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.002685546875, 0.004638671875]
Key: model.layers.20.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.20.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147235704, 2147186554]
Key: model.layers.20.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040039578, 2021091175]
Key: model.layers.20.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003178119659423828, 0.0007948875427246094]
Key: model.layers.20.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.20.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146903928, 2147186551]
Key: model.layers.20.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2041158040, 2021091159]
Key: model.layers.20.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00041103363037109375, 0.0007195472717285156]
Key: model.layers.20.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.20.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146994308, 2146924535]
Key: model.layers.20.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2055834010, 2021095031]
Key: model.layers.20.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003197193145751953, 0.0008058547973632812]
Key: model.layers.20.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.20.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2145351544, 2147122039]
Key: model.layers.20.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2055772554, 2021090919]
Key: model.layers.20.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003688335418701172, 0.0006499290466308594]
Key: model.layers.21.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.002685546875, 0.003662109375]
Key: model.layers.21.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.21.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2143909752, 2145875767]
Key: model.layers.21.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040039833, 2021029751]
Key: model.layers.21.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0004603862762451172, 0.0007281303405761719]
Key: model.layers.21.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.21.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146940856, 2147186619]
Key: model.layers.21.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040105369, 2021095271]
Key: model.layers.21.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00036263465881347656, 0.0007085800170898438]
Key: model.layers.21.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.21.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146924408, 2146137975]
Key: model.layers.21.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109467, 2021099367]
Key: model.layers.21.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00032258033752441406, 0.00070953369140625]
Key: model.layers.21.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0030517578125, 0.004150390625]
Key: model.layers.21.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.21.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147383160, 2146204599]
Key: model.layers.21.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040113289, 2021095271]
Key: model.layers.21.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003490447998046875, 0.0007395744323730469]
Key: model.layers.21.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.21.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924406, 2145863546]
Key: model.layers.21.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105353, 2021095270]
Key: model.layers.21.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0004241466522216797, 0.0006823539733886719]
Key: model.layers.21.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.21.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924408, 2146924415]
Key: model.layers.21.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043930, 2005431926]
Key: model.layers.21.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00033926963806152344, 0.0007696151733398438]
Key: model.layers.21.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.21.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146993033, 2146924407]
Key: model.layers.21.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040039578, 2021095271]
Key: model.layers.21.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00033473968505859375, 0.00064849853515625]
Key: model.layers.22.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0029296875, 0.00341796875]
Key: model.layers.22.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.22.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2144840312, 2145089397]
Key: model.layers.22.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039056794, 2021029479]
Key: model.layers.22.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00047087669372558594, 0.0007138252258300781]
Key: model.layers.22.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.22.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147264376, 2146924359]
Key: model.layers.22.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2039060891, 2021095030]
Key: model.layers.22.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003612041473388672, 0.0007090568542480469]
Key: model.layers.22.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.22.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146662264, 2147231607]
Key: model.layers.22.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040043914, 2022078327]
Key: model.layers.22.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00033283233642578125, 0.0011205673217773438]
Key: model.layers.22.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.003173828125, 0.0048828125]
Key: model.layers.22.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.22.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2145875832, 2146660983]
Key: model.layers.22.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105610, 2021095286]
Key: model.layers.22.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003371238708496094, 0.0007472038269042969]
Key: model.layers.22.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.22.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2145994616, 2146793335]
Key: model.layers.22.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105114, 2021156726]
Key: model.layers.22.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0004303455352783203, 0.0007143020629882812]
Key: model.layers.22.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.22.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146994313, 2145877111]
Key: model.layers.22.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105353, 2021029734]
Key: model.layers.22.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00033020973205566406, 0.0007963180541992188]
Key: model.layers.22.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.22.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147252088, 2145859447]
Key: model.layers.22.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2056882314, 2021095014]
Key: model.layers.22.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00037932395935058594, 0.0006647109985351562]
Key: model.layers.23.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0029296875, 0.0048828125]
Key: model.layers.23.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.23.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2143847817, 2144826487]
Key: model.layers.23.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043657, 2005366632]
Key: model.layers.23.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00045871734619140625, 0.000701904296875]
Key: model.layers.23.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.23.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146924408, 2146924404]
Key: model.layers.23.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109209, 2022143863]
Key: model.layers.23.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00034689903259277344, 0.0007691383361816406]
Key: model.layers.23.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.23.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146924488, 2146936695]
Key: model.layers.23.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109466, 2021090935]
Key: model.layers.23.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00032830238342285156, 0.0009746551513671875]
Key: model.layers.23.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.002685546875, 0.005859375]
Key: model.layers.23.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.23.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147120952, 2147316855]
Key: model.layers.23.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2055772553, 2021094759]
Key: model.layers.23.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003371238708496094, 0.0007824897766113281]
Key: model.layers.23.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.23.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146268984, 2146936695]
Key: model.layers.23.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039060618, 2022074230]
Key: model.layers.23.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0004317760467529297, 0.0007672309875488281]
Key: model.layers.23.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.23.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146596648, 2145875831]
Key: model.layers.23.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043930, 2021095288]
Key: model.layers.23.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00035262107849121094, 0.0007839202880859375]
Key: model.layers.23.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.23.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147121016, 2146924311]
Key: model.layers.23.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2023332249, 2020042599]
Key: model.layers.23.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00038051605224609375, 0.0006504058837890625]
Key: model.layers.24.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00341796875, 0.00390625]
Key: model.layers.24.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.24.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2143922808, 2145875879]
Key: model.layers.24.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043657, 2021090919]
Key: model.layers.24.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0004487037658691406, 0.0006947517395019531]
Key: model.layers.24.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.24.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2145875838, 2146649979]
Key: model.layers.24.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040043913, 2021095286]
Key: model.layers.24.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003173351287841797, 0.0007205009460449219]
Key: model.layers.24.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.24.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146925944, 2146924471]
Key: model.layers.24.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109450, 2021091175]
Key: model.layers.24.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003292560577392578, 0.0007462501525878906]
Key: model.layers.24.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.002685546875, 0.004638671875]
Key: model.layers.24.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.24.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146925176, 2145875915]
Key: model.layers.24.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039056777, 2020042598]
Key: model.layers.24.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003311634063720703, 0.0007448196411132812]
Key: model.layers.24.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.24.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146137976, 2147055479]
Key: model.layers.24.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043930, 2021025655]
Key: model.layers.24.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00042939186096191406, 0.0007042884826660156]
Key: model.layers.24.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.24.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924408, 2146925431]
Key: model.layers.24.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039056537, 2005362534]
Key: model.layers.24.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00034880638122558594, 0.0007872581481933594]
Key: model.layers.24.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.24.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146923124, 2146924404]
Key: model.layers.24.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109450, 2020046694]
Key: model.layers.24.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003631114959716797, 0.0006747245788574219]
Key: model.layers.25.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0029296875, 0.00439453125]
Key: model.layers.25.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.25.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2143975336, 2144967065]
Key: model.layers.25.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043642, 2021029494]
Key: model.layers.25.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00046753883361816406, 0.0007772445678710938]
Key: model.layers.25.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.25.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146925944, 2146924407]
Key: model.layers.25.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040043674, 2021160807]
Key: model.layers.25.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003173351287841797, 0.0007061958312988281]
Key: model.layers.25.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.25.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146925176, 2146924407]
Key: model.layers.25.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040174746, 2021160823]
Key: model.layers.25.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003495216369628906, 0.0007109642028808594]
Key: model.layers.25.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00341796875, 0.004150390625]
Key: model.layers.25.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.25.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146137980, 2146532215]
Key: model.layers.25.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040039833, 2021095030]
Key: model.layers.25.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003509521484375, 0.0007529258728027344]
Key: model.layers.25.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.25.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146899832, 2146924407]
Key: model.layers.25.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2041088153, 2021025398]
Key: model.layers.25.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00041866302490234375, 0.0006914138793945312]
Key: model.layers.25.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.25.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147383160, 2145877367]
Key: model.layers.25.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109449, 2021095271]
Key: model.layers.25.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003402233123779297, 0.0007414817810058594]
Key: model.layers.25.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.25.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146333816, 2146923639]
Key: model.layers.25.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043913, 2021095031]
Key: model.layers.25.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003609657287597656, 0.0007100105285644531]
Key: model.layers.26.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0029296875, 0.00439453125]
Key: model.layers.26.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.26.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2144840565, 2145749126]
Key: model.layers.26.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109193, 2021025383]
Key: model.layers.26.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00045990943908691406, 0.0007243156433105469]
Key: model.layers.26.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.26.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147252024, 2146924471]
Key: model.layers.26.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2041153673, 2022078071]
Key: model.layers.26.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003814697265625, 0.0007152557373046875]
Key: model.layers.26.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.26.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146940792, 2146727799]
Key: model.layers.26.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2056821130, 2022143863]
Key: model.layers.26.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.000308990478515625, 0.0007042884826660156]
Key: model.layers.26.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0029296875, 0.004638671875]
Key: model.layers.26.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.26.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2144843640, 2144828023]
Key: model.layers.26.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105097, 2021099350]
Key: model.layers.26.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00033974647521972656, 0.0007719993591308594]
Key: model.layers.26.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.26.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924408, 2146924407]
Key: model.layers.26.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043658, 2021095288]
Key: model.layers.26.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0004181861877441406, 0.0006685256958007812]
Key: model.layers.26.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.26.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146925365, 2146924407]
Key: model.layers.26.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043913, 2021091174]
Key: model.layers.26.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00034928321838378906, 0.0007419586181640625]
Key: model.layers.26.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.26.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2145171320, 2146924404]
Key: model.layers.26.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109193, 2021095031]
Key: model.layers.26.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003654956817626953, 0.0006556510925292969]
Key: model.layers.27.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00341796875, 0.00439453125]
Key: model.layers.27.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.27.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2145879928, 2144827259]
Key: model.layers.27.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043913, 2020042598]
Key: model.layers.27.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0004608631134033203, 0.0006871223449707031]
Key: model.layers.27.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.27.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146923640, 2146923639]
Key: model.layers.27.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2041158041, 2022139510]
Key: model.layers.27.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003554821014404297, 0.001285552978515625]
Key: model.layers.27.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.27.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146925182, 2146072442]
Key: model.layers.27.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040170874, 2021095287]
Key: model.layers.27.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003285408020019531, 0.0007653236389160156]
Key: model.layers.27.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0029296875, 0.0040283203125]
Key: model.layers.27.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.27.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924412, 2146924407]
Key: model.layers.27.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109194, 2021091175]
Key: model.layers.27.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003209114074707031, 0.0007371902465820312]
Key: model.layers.27.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.27.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146941560, 2147186551]
Key: model.layers.27.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105098, 2021095015]
Key: model.layers.27.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0004200935363769531, 0.0006589889526367188]
Key: model.layers.27.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.27.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146932600, 2147121079]
Key: model.layers.27.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039057049, 2021029479]
Key: model.layers.27.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0002944469451904297, 0.0007510185241699219]
Key: model.layers.27.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.27.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924344, 2146924407]
Key: model.layers.27.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040039818, 2037802599]
Key: model.layers.27.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003447532653808594, 0.0006761550903320312]
Key: model.layers.28.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00341796875, 0.00439453125]
Key: model.layers.28.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.28.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2146072440, 2143828107]
Key: model.layers.28.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109450, 2021095015]
Key: model.layers.28.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00045371055603027344, 0.0006818771362304688]
Key: model.layers.28.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.28.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146924404, 2146936695]
Key: model.layers.28.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2055833994, 2021029750]
Key: model.layers.28.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003113746643066406, 0.0007352828979492188]
Key: model.layers.28.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.28.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146940792, 2147121011]
Key: model.layers.28.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2055768201, 2021095015]
Key: model.layers.28.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00034117698669433594, 0.0007009506225585938]
Key: model.layers.28.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.002685546875, 0.003662109375]
Key: model.layers.28.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.28.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2145695512, 2145876599]
Key: model.layers.28.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039056793, 2021095015]
Key: model.layers.28.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003325939178466797, 0.0007410049438476562]
Key: model.layers.28.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.28.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146930888, 2146924411]
Key: model.layers.28.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105354, 2021095030]
Key: model.layers.28.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00042057037353515625, 0.0007004737854003906]
Key: model.layers.28.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.28.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924412, 2146924407]
Key: model.layers.28.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109466, 2021095286]
Key: model.layers.28.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00030303001403808594, 0.0007505416870117188]
Key: model.layers.28.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.28.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2145875912, 2145691511]
Key: model.layers.28.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105370, 2021095014]
Key: model.layers.28.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00033283233642578125, 0.0007853507995605469]
Key: model.layers.29.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00341796875, 0.00390625]
Key: model.layers.29.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.29.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2145023864, 2143795063]
Key: model.layers.29.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2038995593, 2020042615]
Key: model.layers.29.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00044727325439453125, 0.0006642341613769531]
Key: model.layers.29.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.29.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146203512, 2147317617]
Key: model.layers.29.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040105370, 2021095015]
Key: model.layers.29.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003342628479003906, 0.0008912086486816406]
Key: model.layers.29.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.29.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147253112, 2146924401]
Key: model.layers.29.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109465, 2021095270]
Key: model.layers.29.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00029921531677246094, 0.0009684562683105469]
Key: model.layers.29.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00341796875, 0.00341796875]
Key: model.layers.29.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.29.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146625464, 2146137975]
Key: model.layers.29.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039060874, 2020042343]
Key: model.layers.29.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003571510314941406, 0.0009031295776367188]
Key: model.layers.29.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.29.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146940768, 2145876647]
Key: model.layers.29.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109465, 2020046438]
Key: model.layers.29.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0004169940948486328, 0.0010738372802734375]
Key: model.layers.29.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.29.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147448690, 2145896311]
Key: model.layers.29.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040108954, 2021095030]
Key: model.layers.29.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003299713134765625, 0.0008463859558105469]
Key: model.layers.29.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.29.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924296, 2147227506]
Key: model.layers.29.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039056536, 2021029494]
Key: model.layers.29.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00033664703369140625, 0.0006885528564453125]
Key: model.layers.3.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00341796875, 0.00341796875]
Key: model.layers.3.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.3.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2146088824, 2145024631]
Key: model.layers.3.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2056886665, 2021025654]
Key: model.layers.3.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.000431060791015625, 0.0007581710815429688]
Key: model.layers.3.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.3.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146924413, 2147268467]
Key: model.layers.3.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2056817034, 2021095286]
Key: model.layers.3.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003616809844970703, 0.0006918907165527344]
Key: model.layers.3.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.3.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146923288, 2146923319]
Key: model.layers.3.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040105114, 2021095287]
Key: model.layers.3.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003383159637451172, 0.0007081031799316406]
Key: model.layers.3.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0028076171875, 0.003662109375]
Key: model.layers.3.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.3.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146948984, 2147252091]
Key: model.layers.3.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109450, 2022139751]
Key: model.layers.3.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00029397010803222656, 0.000675201416015625]
Key: model.layers.3.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.3.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147250808, 2146940791]
Key: model.layers.3.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040170906, 2021095286]
Key: model.layers.3.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003571510314941406, 0.0006480216979980469]
Key: model.layers.3.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.3.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147186456, 2146924455]
Key: model.layers.3.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039056282, 2021025638]
Key: model.layers.3.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0002989768981933594, 0.0008015632629394531]
Key: model.layers.3.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.3.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2145875832, 2146105271]
Key: model.layers.3.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109467, 2021095030]
Key: model.layers.3.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003235340118408203, 0.0006241798400878906]
Key: model.layers.30.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0035400390625, 0.00390625]
Key: model.layers.30.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.30.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2146743416, 2145875834]
Key: model.layers.30.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040039818, 2021029494]
Key: model.layers.30.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0004353523254394531, 0.0007309913635253906]
Key: model.layers.30.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.30.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146924408, 2146924407]
Key: model.layers.30.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2056886681, 2022143606]
Key: model.layers.30.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0002815723419189453, 0.0013141632080078125]
Key: model.layers.30.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.30.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146072440, 2145876599]
Key: model.layers.30.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2041092250, 2022143590]
Key: model.layers.30.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003032684326171875, 0.0013303756713867188]
Key: model.layers.30.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.004638671875, 0.003173828125]
Key: model.layers.30.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.30.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147109752, 2146924407]
Key: model.layers.30.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2056816778, 2021095014]
Key: model.layers.30.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003237724304199219, 0.0007977485656738281]
Key: model.layers.30.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.30.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147317624, 2146998402]
Key: model.layers.30.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2056751498, 2022078054]
Key: model.layers.30.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00042057037353515625, 0.000701904296875]
Key: model.layers.30.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.30.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146662269, 2146923895]
Key: model.layers.30.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2056886441, 2021090918]
Key: model.layers.30.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00031495094299316406, 0.0007939338684082031]
Key: model.layers.30.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.30.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2145875832, 2146137975]
Key: model.layers.30.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109449, 2020046438]
Key: model.layers.30.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00033164024353027344, 0.0006899833679199219]
Key: model.layers.31.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00244140625, 0.004150390625]
Key: model.layers.31.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.31.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2146994313, 2146924455]
Key: model.layers.31.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040039817, 2021095014]
Key: model.layers.31.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003066062927246094, 0.0012731552124023438]
Key: model.layers.31.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.31.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147448696, 2147187575]
Key: model.layers.31.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109210, 2021095527]
Key: model.layers.31.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00018966197967529297, 0.0010080337524414062]
Key: model.layers.31.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.31.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146940792, 2147186551]
Key: model.layers.31.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109465, 2021091446]
Key: model.layers.31.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00017750263214111328, 0.0010356903076171875]
Key: model.layers.31.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0029296875, 0.0029296875]
Key: model.layers.31.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.31.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147170120, 2147252087]
Key: model.layers.31.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109209, 2021091174]
Key: model.layers.31.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.000331878662109375, 0.0008854866027832031]
Key: model.layers.31.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.31.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147014523, 2146670503]
Key: model.layers.31.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2041092234, 2020964215]
Key: model.layers.31.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003876686096191406, 0.0009255409240722656]
Key: model.layers.31.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.31.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146940792, 2146924407]
Key: model.layers.31.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039060874, 2021095270]
Key: model.layers.31.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003204345703125, 0.0012578964233398438]
Key: model.layers.31.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.31.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2145875840, 2146924407]
Key: model.layers.31.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109450, 2019981175]
Key: model.layers.31.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00032258033752441406, 0.0006108283996582031]
Key: model.layers.4.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.003173828125, 0.00341796875]
Key: model.layers.4.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.4.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2145875832, 2145892215]
Key: model.layers.4.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109449, 2021029479]
Key: model.layers.4.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0004429817199707031, 0.0006780624389648438]
Key: model.layers.4.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.4.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146948988, 2146924407]
Key: model.layers.4.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109466, 2021095270]
Key: model.layers.4.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003256797790527344, 0.0006999969482421875]
Key: model.layers.4.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.4.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146924408, 2147252087]
Key: model.layers.4.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109193, 2021095015]
Key: model.layers.4.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003199577331542969, 0.0006442070007324219]
Key: model.layers.4.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0032958984375, 0.0045166015625]
Key: model.layers.4.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.4.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2145999258, 2146925879]
Key: model.layers.4.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039056794, 2021095286]
Key: model.layers.4.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00029754638671875, 0.0007314682006835938]
Key: model.layers.4.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.4.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146994313, 2146924407]
Key: model.layers.4.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109450, 2020046695]
Key: model.layers.4.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003521442413330078, 0.0006413459777832031]
Key: model.layers.4.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.4.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146908020, 2146924407]
Key: model.layers.4.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109722, 2021095270]
Key: model.layers.4.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00030231475830078125, 0.0006346702575683594]
Key: model.layers.4.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.4.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146926011, 2146727719]
Key: model.layers.4.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2041092490, 2021029494]
Key: model.layers.4.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003483295440673828, 0.0006155967712402344]
Key: model.layers.5.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.003662109375, 0.003173828125]
Key: model.layers.5.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.5.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2144847739, 2145025404]
Key: model.layers.5.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039060890, 2021095015]
Key: model.layers.5.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00043487548828125, 0.0007014274597167969]
Key: model.layers.5.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.5.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146944952, 2146531191]
Key: model.layers.5.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040105369, 2021095014]
Key: model.layers.5.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00033664703369140625, 0.0008363723754882812]
Key: model.layers.5.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.5.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146924411, 2146924407]
Key: model.layers.5.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109449, 2021095287]
Key: model.layers.5.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00035834312438964844, 0.0006842613220214844]
Key: model.layers.5.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0029296875, 0.00341796875]
Key: model.layers.5.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.5.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146906920, 2145945832]
Key: model.layers.5.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109449, 2021091174]
Key: model.layers.5.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00028586387634277344, 0.0009360313415527344]
Key: model.layers.5.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.5.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147252152, 2147305335]
Key: model.layers.5.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040174729, 2021095031]
Key: model.layers.5.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003829002380371094, 0.0006170272827148438]
Key: model.layers.5.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.5.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146940018, 2146924407]
Key: model.layers.5.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105098, 2021029751]
Key: model.layers.5.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003266334533691406, 0.0009360313415527344]
Key: model.layers.5.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.5.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147448696, 2145945812]
Key: model.layers.5.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043914, 2021091175]
Key: model.layers.5.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003097057342529297, 0.0005841255187988281]
Key: model.layers.6.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0029296875, 0.003662109375]
Key: model.layers.6.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.6.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2146924344, 2146048375]
Key: model.layers.6.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039056521, 2020111975]
Key: model.layers.6.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0004260540008544922, 0.0006661415100097656]
Key: model.layers.6.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.6.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146994361, 2146924407]
Key: model.layers.6.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109466, 2021160566]
Key: model.layers.6.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00031185150146484375, 0.0007081031799316406]
Key: model.layers.6.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.6.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146936696, 2147252087]
Key: model.layers.6.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2055841929, 2021095286]
Key: model.layers.6.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.000354766845703125, 0.0006666183471679688]
Key: model.layers.6.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.003173828125, 0.003662109375]
Key: model.layers.6.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.6.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924412, 2146940791]
Key: model.layers.6.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2056816794, 2021029479]
Key: model.layers.6.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00031638145446777344, 0.0006866455078125]
Key: model.layers.6.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.6.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924296, 2146944983]
Key: model.layers.6.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109449, 2021095015]
Key: model.layers.6.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003809928894042969, 0.0006251335144042969]
Key: model.layers.6.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.6.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147251976, 2147250551]
Key: model.layers.6.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2056886410, 2021095286]
Key: model.layers.6.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003325939178466797, 0.0008244514465332031]
Key: model.layers.6.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.6.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2145904508, 2145875751]
Key: model.layers.6.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039056777, 2005296743]
Key: model.layers.6.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00029349327087402344, 0.0006151199340820312]
Key: model.layers.7.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.0029296875, 0.00341796875]
Key: model.layers.7.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.7.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2144843128, 2145088375]
Key: model.layers.7.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043913, 2021095030]
Key: model.layers.7.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0004284381866455078, 0.0006771087646484375]
Key: model.layers.7.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.7.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147186552, 2147157879]
Key: model.layers.7.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040109194, 2021095287]
Key: model.layers.7.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003447532653808594, 0.0007567405700683594]
Key: model.layers.7.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.7.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147317619, 2147186551]
Key: model.layers.7.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2055833737, 2021099127]
Key: model.layers.7.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.00033974647521972656, 0.0006871223449707031]
Key: model.layers.7.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.002685546875, 0.0035400390625]
Key: model.layers.7.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.7.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924408, 2146940023]
Key: model.layers.7.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105354, 2021029735]
Key: model.layers.7.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003237724304199219, 0.0007634162902832031]
Key: model.layers.7.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.7.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147186632, 2146924412]
Key: model.layers.7.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040174746, 2021029735]
Key: model.layers.7.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00038886070251464844, 0.0006060600280761719]
Key: model.layers.7.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.7.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147187768, 2147186551]
Key: model.layers.7.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2041154201, 2021025398]
Key: model.layers.7.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00030422210693359375, 0.0008897781372070312]
Key: model.layers.7.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.7.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146924403, 2147317618]
Key: model.layers.7.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040105114, 2021025399]
Key: model.layers.7.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003376007080078125, 0.0006532669067382812]
Key: model.layers.8.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.002685546875, 0.00390625]
Key: model.layers.8.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.8.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2145874808, 2144827332]
Key: model.layers.8.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040039850, 2021029479]
Key: model.layers.8.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00043463706970214844, 0.000701904296875]
Key: model.layers.8.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.8.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147065242, 2147064153]
Key: model.layers.8.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040043914, 2022078311]
Key: model.layers.8.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003504753112792969, 0.0008401870727539062]
Key: model.layers.8.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.8.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147186552, 2146925436]
Key: model.layers.8.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2056821145, 2021095015]
Key: model.layers.8.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003216266632080078, 0.0006875991821289062]
Key: model.layers.8.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00341796875, 0.003662109375]
Key: model.layers.8.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.8.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2145877368, 2146924407]
Key: model.layers.8.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043929, 2021025398]
Key: model.layers.8.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003025531768798828, 0.0008406639099121094]
Key: model.layers.8.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.8.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147252088, 2146995340]
Key: model.layers.8.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039056521, 2019915383]
Key: model.layers.8.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00037789344787597656, 0.0006074905395507812]
Key: model.layers.8.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.8.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146944882, 2147186551]
Key: model.layers.8.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043929, 2021095287]
Key: model.layers.8.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00032258033752441406, 0.0007805824279785156]
Key: model.layers.8.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.8.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147317628, 2146333819]
Key: model.layers.8.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109450, 2021029751]
Key: model.layers.8.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0002684593200683594, 0.0006361007690429688]
Key: model.layers.9.input_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00341796875, 0.00390625]
Key: model.layers.9.mlp.down_proj.g_idx, shape: torch.Size([11008]), dtype: torch.int32, range: [0, 0]
Key: model.layers.9.mlp.down_proj.qweight, shape: torch.Size([1376, 4096]), dtype: torch.int32, range: [-2144839544, 2145875831]
Key: model.layers.9.mlp.down_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040043930, 2020042359]
Key: model.layers.9.mlp.down_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0004382133483886719, 0.0006976127624511719]
Key: model.layers.9.mlp.gate_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.9.mlp.gate_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2146925432, 2147252087]
Key: model.layers.9.mlp.gate_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2056882569, 2021156710]
Key: model.layers.9.mlp.gate_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003445148468017578, 0.0008325576782226562]
Key: model.layers.9.mlp.up_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.9.mlp.up_proj.qweight, shape: torch.Size([512, 11008]), dtype: torch.int32, range: [-2147383160, 2146924407]
Key: model.layers.9.mlp.up_proj.qzeros, shape: torch.Size([1, 1376]), dtype: torch.int32, range: [-2040174746, 2021095270]
Key: model.layers.9.mlp.up_proj.scales, shape: torch.Size([1, 11008]), dtype: torch.float16, range: [0.0003399848937988281, 0.0007066726684570312]
Key: model.layers.9.post_attention_layernorm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.002685546875, 0.00439453125]
Key: model.layers.9.self_attn.k_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.9.self_attn.k_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146940728, 2147317511]
Key: model.layers.9.self_attn.k_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040109465, 2021095271]
Key: model.layers.9.self_attn.k_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00031280517578125, 0.0008473396301269531]
Key: model.layers.9.self_attn.o_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.9.self_attn.o_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2147383160, 2147449751]
Key: model.layers.9.self_attn.o_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2040170890, 2021095286]
Key: model.layers.9.self_attn.o_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003485679626464844, 0.0006184577941894531]
Key: model.layers.9.self_attn.q_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.9.self_attn.q_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2146925467, 2147252092]
Key: model.layers.9.self_attn.q_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2055837850, 2022074215]
Key: model.layers.9.self_attn.q_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.0003151893615722656, 0.000705718994140625]
Key: model.layers.9.self_attn.v_proj.g_idx, shape: torch.Size([4096]), dtype: torch.int32, range: [0, 0]
Key: model.layers.9.self_attn.v_proj.qweight, shape: torch.Size([512, 4096]), dtype: torch.int32, range: [-2145864312, 2146924365]
Key: model.layers.9.self_attn.v_proj.qzeros, shape: torch.Size([1, 512]), dtype: torch.int32, range: [-2039060633, 2019981159]
Key: model.layers.9.self_attn.v_proj.scales, shape: torch.Size([1, 4096]), dtype: torch.float16, range: [0.00031375885009765625, 0.0005979537963867188]
Key: model.norm.weight, shape: torch.Size([4096]), dtype: torch.float16, range: [-0.00018978118896484375, 0.009765625]
